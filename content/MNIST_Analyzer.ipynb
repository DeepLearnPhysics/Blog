{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Output of a Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous tutorial, I showed how to train a network with minibatching, batch norm, and writing to file to save the network weights.  Here, I'll show you how to restore that network to do analysis.  The network model itself is copy/pasted from the previous tutorial, though there are ways to load the network without knowing how it was constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the convolutional network, copied from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_step(x, training):\n",
    "    \n",
    "    n_filters = 2*x.get_shape().as_list()[-1]\n",
    " \n",
    "\n",
    "    x = tf.layers.conv2d(x, filters=n_filters,\n",
    "                        kernel_size=[3,3],\n",
    "                        strides=[1,1],\n",
    "                        padding='same',\n",
    "                        use_bias=False,\n",
    "                        reuse=False,\n",
    "                        trainable=training)\n",
    "    \n",
    "\n",
    "    # Here's an important gotcha: I set the decay to 0.9, and updates_collection to None\n",
    "    # This forces the update to happen \"in place\" and makes sure the batch norm parameters\n",
    "    # Get saved to files.\n",
    "    x = tf.contrib.layers.batch_norm(x,\n",
    "                                     updates_collections=None,\n",
    "                                     decay=0.9,\n",
    "                                     is_training=training,\n",
    "                                     trainable=training,\n",
    "                                     # name=\"BatchNorm\",\n",
    "                                     reuse=False)\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "\n",
    "def build_network(x, training):\n",
    "    \n",
    "    print(\"Building network, initial shape: {0}\".format(x.get_shape()))\n",
    "        \n",
    "    # Initial convolutions:\n",
    "    x = convolutional_step(x, training)\n",
    "    x = convolutional_step(x, training)\n",
    "    \n",
    "    # Downsample to 14x14:\n",
    "    x = tf.layers.max_pooling2d(x,\n",
    "                                 pool_size=2,\n",
    "                                 strides=2,\n",
    "                                 padding='valid')\n",
    "    \n",
    "    print(\"After first downsample shape: {0}\".format(x.get_shape()))\n",
    "    \n",
    "    # More convolutions:\n",
    "    x = convolutional_step(x, training)\n",
    "    x = convolutional_step(x, training)\n",
    "    \n",
    "    # Downsample to 7x7:\n",
    "    x = tf.layers.max_pooling2d(x,\n",
    "                                 pool_size=2,\n",
    "                                 strides=2,\n",
    "                                 padding='valid')\n",
    "    \n",
    "    print(\"After first downsample shape: {0}\".format(x.get_shape()))\n",
    "    \n",
    "    # More convolutions:\n",
    "    x = convolutional_step(x, training)\n",
    "    x = convolutional_step(x, training)\n",
    "    \n",
    "    # Downsample to 3x3:\n",
    "    x = tf.layers.max_pooling2d(x,\n",
    "                                 pool_size=2,\n",
    "                                 strides=2,\n",
    "                                 padding='valid')\n",
    "    \n",
    "    print(\"After last downsample shape: {0}\".format(x.get_shape()))\n",
    "    \n",
    "    # Do a bottle neck step to merge into just 10 filters:\n",
    "    \n",
    "    x = tf.layers.conv2d(x,filters=10,\n",
    "                        kernel_size=[1,1],\n",
    "                        strides=[1,1],\n",
    "                        padding='same',\n",
    "                        use_bias=False,\n",
    "                        reuse=False,\n",
    "                        trainable=training\n",
    "                        )\n",
    "    \n",
    "    \n",
    "    # Do global average pooling to make 10 output logits:\n",
    "    shape = (x.shape[1], x.shape[2])\n",
    "\n",
    "    x = tf.nn.pool(x,\n",
    "                   window_shape=shape,\n",
    "                   pooling_type=\"AVG\",\n",
    "                   padding=\"VALID\",\n",
    "                   dilation_rate=None,\n",
    "                   strides=None,\n",
    "                   name=\"GlobalAveragePool\",\n",
    "                   data_format=None)\n",
    "\n",
    "    # Reshape to remove empty dimensions:\n",
    "    x = tf.reshape(x, [tf.shape(x)[0], 10],\n",
    "                   name=\"global_pooling_reshape\")\n",
    "\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also set up the placeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network, initial shape: (?, 28, 28, 1)\n",
      "After first downsample shape: (?, 14, 14, 4)\n",
      "After first downsample shape: (?, 7, 7, 16)\n",
      "After last downsample shape: (?, 3, 3, 64)\n",
      "Final output shape: (?, 10)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Define the input placeholders, as defined in the tensorflow mnist tutorial:\n",
    "x  = tf.placeholder(tf.float32, shape=[None, 784], name='x')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10], name='y_')\n",
    "\n",
    "reshaped_x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "y = build_network(reshaped_x, training=True)\n",
    "print \"Final output shape: {0}\".format(y.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore the network using a saver.  If you point it to the write folder, tensorflow keeps track of what the most recent weights file is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest Checkpoint is: /home/cadams/DeepLearnPhysics/mnist-train-and-analyze/log_mnist_classifier/checkpoints/save-1000\n"
     ]
    }
   ],
   "source": [
    "writer = tf.train.Saver()\n",
    "save_dir     = '/home/cadams/DeepLearnPhysics/mnist-train-and-analyze' + \"/log_mnist_classifier/checkpoints\"\n",
    "latest_checkpoint = tf.train.latest_checkpoint(save_dir)\n",
    "print (\"Latest Checkpoint is: {0}\".format(latest_checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/cadams/DeepLearnPhysics/mnist-train-and-analyze/log_mnist_classifier/checkpoints/save-1000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "writer.restore(sess, latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we're ready to go.  Let's loop through the validation data set and get the predictions for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = mnist.validation.images\n",
    "labels = mnist.validation.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many are we looking at?  It's possible they can all be processed at once, but I'll do it in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images.shape: (5000, 784)\n"
     ]
    }
   ],
   "source": [
    "print \"Images.shape: {0}\".format(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, 5000 images. Let's do 50 batches of 100 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = numpy.zeros((5000,10))\n",
    "for batch in range(50):\n",
    "    batch_images = images[batch*100:(batch+1)*100]\n",
    "    batch_labels = labels[batch*100:(batch+1)*100]\n",
    "    batch_predictions = sess.run(y, feed_dict={x:batch_images, y_:batch_labels})\n",
    "    predicted_labels[batch*100:(batch+1)*100] = batch_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label: [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "Prediction: [-3.77722335 -4.80439615 -1.45840096  4.89647245 -5.67046452  7.00556707\n",
      " -2.71908355 -4.13560677  2.23484254 -3.28994966]\n"
     ]
    }
   ],
   "source": [
    "print \"True Label: {0}\".format(labels[0])\n",
    "print \"Prediction: {0}\".format(predicted_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks funky, right?  It's because the softmax function is only applied at the loss calculation, so we need to apply it ourselves here.  It's easy to do with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1)\n"
     ]
    }
   ],
   "source": [
    "_exp = numpy.exp(predicted_labels)\n",
    "_sum = numpy.expand_dims(numpy.sum(_exp, axis=1), axis=1)\n",
    "print _sum.shape\n",
    "softmax_labels = _exp / _sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.83632246e-05   6.57435975e-06   1.86638275e-04   1.07371043e-01\n",
      "   2.76518857e-06   8.84820804e-01   5.29045741e-05   1.28323148e-05\n",
      "   7.49818213e-03   2.98929807e-05]\n"
     ]
    }
   ],
   "source": [
    "print softmax_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much more reasonable.  It claims an 88% probability that the label is the 5th index, just like the truth.  Let's find the predicted label for all entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_index = numpy.argmax(softmax_labels, axis=1) \n",
    "true_index       = numpy.argmax(labels, axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly compute the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation data: 0.9834\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy on validation data: {0}\".format(\n",
    "    numpy.mean(classified_index==true_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's good accuracy, on par with the training and testing sets.  Let's generate a matrix to show it's classification results as a function of input label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = numpy.zeros((10,10))\n",
    "for true_label in range(10):\n",
    "    indexes = numpy.where(true_index == true_label)\n",
    "    count = len(indexes[0])\n",
    "    for predicted_label in range(10):\n",
    "        this_category_count = numpy.sum(classified_index[indexes] == predicted_label)\n",
    "        confusion_matrix[true_label,predicted_label] = 1.0*this_category_count/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's display the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9de151b210>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkYAAAJKCAYAAADJH4oXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnkoIabL1VlAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9de155a050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import colors\n",
    "figure = plt.figure(figsize=(10,10))\n",
    "plt.imshow(confusion_matrix,vmin=0,vmax=1.0, cmap='Blues', interpolation='none',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know there are entries off the diagonal, but they're pretty small.  You can play around with the color map to highlight these, but the results look pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
