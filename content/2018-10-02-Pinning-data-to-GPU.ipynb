{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traditional and recommended data pipeline for deep learning involves pre-processing the data on CPU (data augmentation, cropping, etc), then loading small batches of pre-processed data on the GPU. There are several good reasons for this:\n",
    "\n",
    "* The datasets are often huge and cannot fit on the GPU memory.\n",
    "* The networks are big and the memory transfer overhead is negligible compared to the network computations.\n",
    "\n",
    "However this does not always apply. If the dataset is small enough to fit on the GPU memory or the network computation time is of the same order as the memory transfer overhead, we start to think about doing the pre-processing directly on GPU.\n",
    "\n",
    "![CPU / GPU pipeline](https://www.tensorflow.org/images/datasets_without_pipelining.png)\n",
    "*Pre-processing on CPU, training on GPU and idle times (figure from [Tensorflow documentation](https://www.tensorflow.org/performance/datasets_performance))*\n",
    "\n",
    "**Some context on our use case**: We want to train a network on 3D images that are too big to be fed directly to the network. Our current pipeline is to crop our big images on CPU before feeding the crops one by one to the network training on GPU. First, the extraction of crops turns out to be expensive on CPU (of same order of magnitude as our network computations) and easily parallelizable on a GPU. Second, this scheme involves many small CPU-GPU memory transfers (one per crop) which we would like to avoid, as it costs a lot of time. Instead we want to transfer a handful of big images on the GPU in one shot, crop them on the GPU and feed them to the network *without going back to the CPU*. \n",
    "\n",
    "The cropping part involves writing our own custom CUDA kernel and integrating it in Tensorflow or PyTorch. We won't talk about this here. Let's focus on the data movement part.\n",
    "\n",
    "**To summarize what we want to achieve without the context details:**\n",
    "1. Load a batch of data on CPU\n",
    "2. Transfer the batch to GPU\n",
    "3. For each image in that batch:\n",
    "    1. Do some pre-processing on GPU, which outputs a batch of possibly unknown length (e.g. the number of crops might not be deterministic).\n",
    "    2. **Pin the data to the GPU** (i.e. prevent it from going back to CPU).\n",
    "    3. Use the pre-processed batch to do further computations on **minibatches** (such as training a network).\n",
    "\n",
    "We will go over toy example for this pipeline using both [Tensorflow](#In-Tensorflow) and [PyTorch](#In-PyTorch).\n",
    "\n",
    "\n",
    "*__Important warning__ If you work with more traditional 2D images you might want to use the recent [DALI](https://github.com/NVIDIA/DALI) library from NVIDIA. It solves exactly this issue: pre-processing the data on GPU before feeding it to a deep learning framework. They have bindings to [TensorFlow](https://docs.nvidia.com/deeplearning/sdk/dali-archived/dali_01_beta/docs/examples/tensorflow/tensorflow-resnet50.html) and PyTorch, too.\n",
    "In our case 3D images are not (yet) supported by DALI and no short-term implementation is planned, which explains why we have to tackle this 'by hand'.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Tensorflow\n",
    "It turns out to be surprisingly hard in Tensorflow. First, it is hard to determine whether a Tensorflow operation implementation is available for GPU. The only way is to check the Github repository and look for a CUDA kernel corresponding to the operation. Second, it turns out most pre-processing operations such as `tf.train.batch` are implemented on CPU. (Random sidenote: random operations such as `tf.random_uniform` or `tf.random_crop` also seem to be only available on CPU.) Of course, Tensorflow recommends that pre-processing takes place on CPU... What it means for us: we might do our pre-processing on GPU, but as soon as we try to batch it for the actual computation it will be sent back to CPU.\n",
    "\n",
    "**The only way to pin data to the GPU in Tensorflow is to declare it as a `tf.constant`.** It gives rise to a convoluted but working pipeline: load a batch of data on GPU as a `tf.constant`, do the preprocessing on GPU, then use a placeholder for the index that defines a minibatch. This approach suggested in [this blog post](https://eklitzke.org/pinning-gpu-memory-in-tensorflow) works well, but one detail that is left out is how to change our batch of data: once it has been consumed by the network, how do we proceed to the next batch of data and declare it as another `tf.constant`? How do we run the network on that new constant? As you may know, once the graph has been defined, Tensorflow freezes it and runs always the same graph.\n",
    "\n",
    "The answer is to do some surgery with the Tensorflow computation graph: for each batch of data, remove the node for the `tf.constant` and replace it with the new batch.\n",
    "\n",
    "Let's demonstrate with a **toy example** how to do it in practice. First let us define our data: an array of shape (100, 3). We want to transfer it to GPU in batches of 20, do some pre-processing and then feed it to the network one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# The size of each initial batch.\n",
    "BATCH_SIZE = 20\n",
    "# The size of minibatch size which we want to pre-process.\n",
    "MINIBATCH_SIZE = 1\n",
    "# Initial number of images/data.\n",
    "N = 100\n",
    "\n",
    "# Create the dataset in CPU memory\n",
    "np_data = np.array(range(N*4*4), dtype=np.float32).reshape(N, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    # Load a batch of data on GPU\n",
    "    tf_data = tf.constant(np_data[0:BATCH_SIZE], dtype=tf.float32, name='data')\n",
    "    # Index of the minibatch inside the current batch\n",
    "    ix = tf.placeholder(shape=(), dtype=tf.int32, name='ix')\n",
    "    # ix = tf.constant(0, dtype=tf.int32, name='ix')\n",
    "    # Select a single image from that batch = shape (1, 3, 3)\n",
    "    batch = tf.slice(tf_data, [MINIBATCH_SIZE * ix, 0, 0], [MINIBATCH_SIZE, -1, -1], name='batch')\n",
    "    # ...\n",
    "    # Do some pre-processing here on the batch, which outputs a minibatch of size (4, 2, 2)\n",
    "    # ...\n",
    "    minibatch = tf.reshape(batch, (-1, 2, 2))[:4]\n",
    "    # Do something with the minibatch - here dummy computation\n",
    "    # If we wanted to work on the minibatch slice by slice, we \n",
    "    # could have another index placeholder\n",
    "    outp = tf.reduce_sum(tf.square(minibatch), name='outp')\n",
    "    # Save graph definition\n",
    "    gdef = g.as_graph_def()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ix` is a placeholder for the index inside the current batch. The batch data is defined as a `tf.constant` to force it to stay on GPU once it has been moved there. We use `tf.slice` to extract the data corresponding to our index `ix` from our initial batch, for the pre-processing step. After pre-processing we end up with a `minibatch` which is made of several images. `outp` performs some dummy computation on this minibatch. We save the graph definition in `gdef` variable for our later surgery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Set tf.AUTO_REUSE to be allowed to re-import the graph at each batch\n",
    "    with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "        # Loop over batches of data of size BATCH_SIZE\n",
    "        for idx in range(N/BATCH_SIZE):\n",
    "            new_data = tf.constant(np_data[BATCH_SIZE*idx:BATCH_SIZE*(idx+1)], dtype=tf.float32, name='data%d' % idx)\n",
    "            tf.import_graph_def(gdef, input_map={'data:0': new_data}, name='')\n",
    "            # If we wanted to train a network we should save/restore weights\n",
    "            # at this level.\n",
    "            # sess.run(tf.global_variables_initializer())\n",
    "            # For each batch, we are going to run the computation graph on a MINIBATCH_SIZE sample\n",
    "            for i in range(BATCH_SIZE/MINIBATCH_SIZE):\n",
    "                o_tensor = tf.get_default_graph().get_tensor_by_name('outp:0' if idx == 0 else 'outp_%d:0' % idx)\n",
    "                o = sess.run([o_tensor], feed_dict={tf.get_default_graph().get_tensor_by_name('ix:0' if idx == 0 else 'ix_%d:0' % idx): i})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key to the surgery on TF computation graph lies in `tf.import_graph_def`. We use the keyword argument `input_map` to map the `data:0` constant node to a new constant node which holds the next batch of data. Note that the `name` argument should be set to an empty string, or all the variables will have an additional name scope appended to their names.\n",
    "\n",
    "*__Warning__: `tf.import_graph_def` only restores the graph, it does not restore variables values. If we wanted to train a real network, we should store all the weights for each batch of data and restore them after we do our surgery on the graph. For the sake of simplicity we leave this out to the reader. Please note that it can be yet another downside of this method, since storing/restoring weights involves additional memory transfers between CPU/GPU.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Profiling** If we time it using `nvprof` profiler, we can see that there are only 5 host to device transfers (i.e. CPU to GPU) as expected. There are however still 100 transfers from device to host (GPU to CPU): every time we call `sess.run` in Tensorflow, after the computation graph is executed all the tensors that were requested are brought back to CPU (and each tensor brought back to CPU takes 1 call to `CUDA memcpy DtoH`, in our case we only asked for the output tensor).\n",
    "\n",
    "```bash\n",
    "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    "                   29.61%  113.89us       100  1.1380us  1.0880us  1.5040us  [CUDA memcpy DtoH]\n",
    "                    1.59%  6.1120us         5  1.2220us  1.1200us  1.4080us  [CUDA memcpy HtoD]\n",
    "```\n",
    "\n",
    "As you can see any data transfer will take at least 1 microsecond, no matter how small the data is. Let us increase the dataset to a batch size of 200 and 1000 entries, keeping the same ratio 1:5 between the batch size and the dataset size. Now we can clearly see the difference:\n",
    "\n",
    "```bash\n",
    "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    "                   30.19%  1.1380ms      1000  1.1370us  1.0870us  4.3200us  [CUDA memcpy DtoH]\n",
    "                    0.30%  11.296us         5  2.2590us  2.2400us  2.2720us  [CUDA memcpy HtoD]\n",
    "```\n",
    "Despite the data size being 10 times bigger in HtoD transfers, the average time for each call is only twice bigger. If we had kept our 'naive' scheme, sending the minibatch data one by one to the GPU, it would have increased similarly to the current DtoH transfers, by a factor of 10. So using this strategy already cuts by almost half the memory transfer time needed to achieve our goal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In PyTorch\n",
    "PyTorch is meant to be more flexible and DIY spirit than Tensorflow, so it is not surprising if this pipeline is much easier to achieve in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# The size of each initial batch.\n",
    "BATCH_SIZE = 20\n",
    "# The size of minibatch size which we want to pre-process.\n",
    "MINIBATCH_SIZE = 1\n",
    "# Initial number of images/data.\n",
    "N = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset on CPU\n",
    "np_data = np.array(range(N*4*4), dtype=np.float32).reshape(N, 4, 4)\n",
    "# Load to Torch tensor\n",
    "data = torch.from_numpy(np_data)\n",
    "dataset = torch.utils.data.TensorDataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating batches from the dataset is simple and we can specify that it should be pinned to the device memory with `pin_memory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batches\n",
    "batch = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate over the batches and do our pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over batches\n",
    "for i, data in enumerate(batch):\n",
    "    image, = data\n",
    "    # Load the batch to GPU\n",
    "    image = image.cuda()\n",
    "    # Slice into chunks\n",
    "    chunks = torch.chunk(image, BATCH_SIZE/MINIBATCH_SIZE, dim=0)\n",
    "    for c in chunks:\n",
    "        # ...\n",
    "        # Do some pre-processing and output a minibatch\n",
    "        # ...\n",
    "        minibatch = c.view((-1, 2, 2))[:4]\n",
    "        # If we wanted to work on the minibatch images one by one we could use\n",
    "        # torch.chunk again.\n",
    "        output = torch.sum(torch.sqrt(minibatch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Profiling** Running `nvprof` gives us:\n",
    "```bash\n",
    "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    "                   17.43%  113.44us       100  1.1340us  1.0880us  1.6000us  [CUDA memcpy DtoH]\n",
    "                   13.04%  84.896us        13  6.5300us     992ns  67.968us  [CUDA memcpy HtoD]\n",
    "```\n",
    "We have 8 unexpected calls to `CUDA memcpy HtoD` which come from the `torch.utils.data.DataLoader` call. They are independent of the batch size and the dataset size. The rest of the calls is as expected, and if you look back on the code it looks much cleaner than the TF equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Tensorflow guide on input pipeline](https://www.tensorflow.org/performance/datasets_performance)\n",
    "\n",
    "* [Pinning GPU memory in Tensorflow](https://eklitzke.org/pinning-gpu-memory-in-tensorflow)\n",
    "\n",
    "* [Official announcement on NVIDIA DALI](https://news.developer.nvidia.com/announcing-nvidia-dali-and-nvidia-nvjpeg/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
