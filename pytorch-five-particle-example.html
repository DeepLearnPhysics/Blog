
<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="http://deeplearnphysics.org/Blog/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="http://deeplearnphysics.org/Blog/theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="http://deeplearnphysics.org/Blog/theme/font-awesome/css/font-awesome.min.css">




  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="index, follow" />

    <!-- Chrome, Firefox OS and Opera -->
    <meta name="theme-color" content="#333">
    <!-- Windows Phone -->
    <meta name="msapplication-navbutton-color" content="#333">
    <!-- iOS Safari -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

<meta name="author" content="Taritree" />
<meta name="description" content="An example of training a classification network on the 5-particle LArCV training data using pytorch." />
<meta name="keywords" content="resnet, pytorch, classification, example">
<meta property="og:site_name" content="DeepLearnPhysics Blog"/>
<meta property="og:title" content="PyTorch/LArCV Classification Example with Data Set (v0.1.0)"/>
<meta property="og:description" content="An example of training a classification network on the 5-particle LArCV training data using pytorch."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="http://deeplearnphysics.org/Blog/pytorch-five-particle-example.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-01-09 00:00:00-06:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="http://deeplearnphysics.org/Blog/author/taritree.html">
<meta property="article:section" content="tutorial"/>
<meta property="article:tag" content="resnet"/>
<meta property="article:tag" content="pytorch"/>
<meta property="article:tag" content="classification"/>
<meta property="article:tag" content="example"/>
<meta property="og:image" content="profile.png">


<!-- Default meta cards for twitter -->
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@dlphysics">
<meta name="twitter:creator" content="@dlphysics">
<meta name="twitter:title" content="PyTorch/LArCV Classification Example with Data Set (v0.1.0)">
<meta name="twitter:description" content="<p>An example of training a classification network on the 5-particle LArCV training data using pytorch.</p>">
<meta name="twitter:image" content="http://deeplearnphysics.org/Blog/theme/img/profile_small.png" />


  <title>DeepLearnPhysics Blog &ndash; PyTorch/LArCV Classification Example with Data Set (v0.1.0)</title>
</head>
<body>
  <aside>
    <div>
      <a href="http://deeplearnphysics.org/Blog">
        <img src="http://deeplearnphysics.org/Blog/theme/img/profile.png" alt="Blog" title="Blog">
      </a>
      <h1><a href="http://deeplearnphysics.org/Blog">Blog</a></h1>

<p>DeepLearnPhysics Group</p>

      <ul class="social">
        <li><a class="sc-home" href="http://deeplearnphysics.org" target="_blank"><i class="fa fa-home"></i></a></li>
        <li><a class="sc-twitter" href="https://twitter.com/dlphysics" target="_blank"><i class="fa fa-twitter"></i></a></li>
        <li><a class="sc-github" href="http://github.com/DeepLearnPhysics" target="_blank"><i class="fa fa-github"></i></a></li>
      </ul>
    </div>

  </aside>
  <main>
    <nav>


      <a href="http://deeplearnphysics.org/Blog/index.html">Home</a>
      <a href="http://deeplearnphysics.org/Blog/categories.html">Category</a>
      <a href="http://deeplearnphysics.org/Blog/archives.html">Archives</a>
      <a href="http://deeplearnphysics.org/Blog/tags.html">Tags</a>
      <a href="http://deeplearnphysics.org/Blog/authors.html">Authors</a>


    </nav>

<article class="single">
  <header>
    <h1 id="pytorch-five-particle-example">PyTorch/LArCV Classification Example with Data Set (v0.1.0)</h1>
    <p>
          Posted on Tue 09 January 2018 in <a href="http://deeplearnphysics.org/Blog/category/tutorial.html">tutorial</a>

            by

              <a href="http://deeplearnphysics.org/Blog/author/taritree.html">Taritree</a>    </p>
  </header>

  <!-- script is a local library -->
  <link rel="stylesheet" type="text/css" href="http://deeplearnphysics.org/Blog/theme/stylesheet/kazunotebook.css">

  <div>
    <style type="text/css">/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}


.rendered_html pre,



.rendered_html tr,
.rendered_html th,

.rendered_html td,


.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,

div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
</style>
<style type="text/css">.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style>
<style type="text/css">
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }
</style>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<meta content="https://avatars0.githubusercontent.com/u/21003710?s=200&amp;v=4" name="twitter:image" />
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="PyTorch-Classification-Example">PyTorch Classification Example<a class="anchor-link" href="#PyTorch-Classification-Example">¶</a></h1><p>In this notebook, we're going to use ResNet-18 implemented in pyTorch to classify the 5-particle example training data.</p>
<p>This tutorial is meant to walk through some of the necessary steps to load images stored in LArCV files and train a network.  For more details on how to use pytorch, refer to the official pytorch tutorials.</p>
<p>This notebook will try to be self-contained in terms of code. 
However, you can find the code separated into different files in the following repositories</p>
<ul>
<li><a href="https://github.com/DeepLearnPhysics/larcvdataset">LArCVDataset</a>: concrete instance of pytorch Dataset class written for LArCV2 IO</li>
<li><a href="https://github.com/DeepLearnPhysics/pytorch-resnet-example">pytorch-classification-example</a>: many of the files and scripts found in this tutorial</li>
<li><a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py">pytorch resnet implementation</a>: is where we get our implementation with some slight modifications</li>
<li><a href="https://github.com/pytorch/examples/blob/master/imagenet/main.py">pytorch ImageNet training example</a>: is where we get the methods for training, again, with some slight modifications</li>
</ul>
<p>You will also need the training data. Go to the <a href="http://deeplearnphysics.org/DataChallenge/">open data page</a> and download the either the 5k or 50k training/validation samples.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [1]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># Import our modules</span>

<span class="c1"># python</span>
<span class="kn">import</span> <span class="nn">os</span><span class="o">,</span><span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">traceback</span>

<span class="c1"># numpy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># torch</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.parallel</span>
<span class="kn">import</span> <span class="nn">torch.backends.cudnn</span> <span class="kn">as</span> <span class="nn">cudnn</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="kn">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.optim</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span>
<span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="kn">as</span> <span class="nn">transforms</span>
<span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="kn">as</span> <span class="nn">datasets</span>
<span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="kn">as</span> <span class="nn">models</span>

<span class="c1"># ROOT/LArCV</span>
<span class="kn">import</span> <span class="nn">ROOT</span>
<span class="kn">from</span> <span class="nn">larcv</span> <span class="kn">import</span> <span class="n">larcv</span>

<span class="o">%</span><span class="k">matplotlib</span> notebook
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Welcome to JupyROOT 6.12/04
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Set-the-GPU-to-use">Set the GPU to use<a class="anchor-link" href="#Set-the-GPU-to-use">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [2]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span> <span class="mi">1</span> <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[2]:</div>
<div class="output_text output_subarea output_execute_result">
<pre><torch.cuda.device at 0x7f1788e94410></pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Setup-Data-IO">Setup Data IO<a class="anchor-link" href="#Setup-Data-IO">¶</a></h1><h2 id="Location-of-data-on-your-local-machine">Location of data on your local machine<a class="anchor-link" href="#Location-of-data-on-your-local-machine">¶</a></h2><p>Set the path to the data files in this block.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [3]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">path_to_train_data</span><span class="o">=</span><span class="s2">"/home/taritree/working/dlphysics/testset/train_50k.root"</span>
<span class="n">path_to_test_data</span><span class="o">=</span><span class="s2">"/home/taritree/working/dlphysics/testset/test_40k.root"</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path_to_train_data</span><span class="p">):</span>
    <span class="k">print</span> <span class="s2">"Could not find the training data file."</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path_to_test_data</span><span class="p">):</span>
    <span class="k">print</span> <span class="s2">"Could not find the validation data file."</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Define-LArCVDataset">Define LArCVDataset<a class="anchor-link" href="#Define-LArCVDataset">¶</a></h2><p>First, we define a class that will load our data. There are many ways to do this. We create a concrete instance of pytorch's <code>Dataset</code> class, which can be used in the <code>DataLoader</code> class (which we do not use).</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [4]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># from: https://github.com/deeplearnphysics/larcvdataset</span>

<span class="n">larcv</span><span class="o">.</span><span class="n">PSet</span> <span class="c1"># touch this to force libBase to load, which has CreatePSetFromFile</span>
<span class="kn">from</span> <span class="nn">larcv.dataloader2</span> <span class="kn">import</span> <span class="n">larcv_threadio</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span> <span class="nc">LArCVDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="sd">""" LArCV data set interface for PyTorch"""</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">,</span> <span class="n">fillername</span><span class="p">,</span> <span class="n">verbosity</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">loadallinmem</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">randomize_inmem_data</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_inmem_events</span><span class="o">=-</span><span class="mi">1</span> <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbosity</span> <span class="o">=</span> <span class="n">verbosity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batchsize</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">randomize_inmem_data</span> <span class="o">=</span> <span class="n">randomize_inmem_data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_inmem_events</span> <span class="o">=</span> <span class="n">max_inmem_events</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loadallinmem</span> <span class="o">=</span> <span class="n">loadallinmem</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>  

        <span class="c1"># we setup the larcv threadfiller class, which handles io from larcv files</span>
        <span class="c1"># this follows steps from larcv tutorials</span>
        
        <span class="c1"># setup cfg dictionary needed for larcv_threadio      </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filler_cfg</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filler_cfg</span><span class="p">[</span><span class="s2">"filler_name"</span><span class="p">]</span> <span class="o">=</span> <span class="n">fillername</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filler_cfg</span><span class="p">[</span><span class="s2">"verbosity"</span><span class="p">]</span>   <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbosity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filler_cfg</span><span class="p">[</span><span class="s2">"filler_cfg"</span><span class="p">]</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Could not find filler configuration file: </span><span class="si">%s</span><span class="s2">"</span><span class="o">%</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">))</span>

        <span class="c1"># we read the first line of the config file, which should have name of config parameter set</span>
        <span class="n">linepset</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">,</span><span class="s1">'r'</span><span class="p">)</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cfgname</span> <span class="o">=</span> <span class="n">linepset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">":"</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        
        <span class="c1"># we load the pset ourselves, as we want access to values in 'ProcessName' list</span>
        <span class="c1"># will use these as the names of the data products loaded. store in self.datalist</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pset</span> <span class="o">=</span> <span class="n">larcv</span><span class="o">.</span><span class="n">CreatePSetFromFile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">cfgname</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"larcv::PSet"</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfgname</span><span class="p">)</span>
        <span class="n">datastr_v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pset</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"std::vector<std::string>"</span><span class="p">)(</span><span class="s2">"ProcessName"</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">datalist</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">datastr_v</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">datalist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">datastr_v</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        
        <span class="c1"># finally, configure io</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">io</span> <span class="o">=</span> <span class="n">larcv_threadio</span><span class="p">()</span>        
        <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">configure</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filler_cfg</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loadallinmem</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_loadinmem</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">loadallinmem</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">fetch_n_entries</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alldata</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">datalist</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">loadallinmem</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datalist</span><span class="p">:</span>
                <span class="n">out</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">fetch_data</span><span class="p">(</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span><span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batchsize</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datalist</span><span class="p">:</span>
                <span class="n">out</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchsize</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">alldata</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="bp">self</span><span class="o">.</span><span class="n">alldata</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="p">)</span>
                <span class="k">for</span> <span class="n">n</span><span class="p">,</span><span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">indices</span><span class="p">):</span>
                    <span class="n">out</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="n">n</span><span class="p">,:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alldata</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="n">idx</span><span class="p">,:]</span>
        <span class="k">return</span> <span class="n">out</span>
        
    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">dumpcfg</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">_loadinmem</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""load data into memory"""</span>
        <span class="n">nevents</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">fetch_n_entries</span><span class="p">())</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_inmem_events</span><span class="o">></span><span class="mi">0</span> <span class="ow">and</span> <span class="n">nevents</span><span class="o">></span><span class="bp">self</span><span class="o">.</span><span class="n">max_inmem_events</span><span class="p">:</span>
            <span class="n">nevents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_inmem_events</span>

        <span class="k">print</span> <span class="s2">"Attempting to load all "</span><span class="p">,</span><span class="n">nevents</span><span class="p">,</span><span class="s2">" into memory. good luck"</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="c1"># start threadio</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># get one data element to get shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
        <span class="n">firstout</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datalist</span><span class="p">:</span>
            <span class="n">firstout</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">fetch_data</span><span class="p">(</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alldata</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datalist</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alldata</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="p">(</span><span class="n">nevents</span><span class="p">,</span><span class="n">firstout</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">firstout</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alldata</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">firstout</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="mi">0</span><span class="p">,:]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">nevents</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="k">100</span>==0:
                <span class="k">print</span> <span class="s2">"loading event </span><span class="si">%d</span><span class="s2"> of </span><span class="si">%d</span><span class="s2">"</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">nevents</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datalist</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">fetch_data</span><span class="p">(</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">alldata</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>

        <span class="k">print</span> <span class="s2">"elapsed time to bring data into memory: "</span><span class="p">,</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">,</span><span class="s2">"sec"</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">batchsize</span><span class="p">):</span>
        <span class="sd">"""exposes larcv_threadio::start which is used to start the thread managers"""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batchsize</span> <span class="o">=</span> <span class="n">batchsize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">start_manager</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchsize</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">stop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">""" stops the thread managers"""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">stop_manager</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">dumpcfg</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""dump the configuration file to a string"""</span>
        <span class="k">print</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
        
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Write-configuration-files-for-the-LArCV-ThreadFiller-class">Write configuration files for the LArCV ThreadFiller class<a class="anchor-link" href="#Write-configuration-files-for-the-LArCV-ThreadFiller-class">¶</a></h2><p>We define the configurations in this block, then write to file. We will load the files later when we create LArCVDataset instances for both the training and test data.</p>
<p>A note: the configurations need to have a separate name. Also, the <code>ProcessNames</code> have to be different. This is because of the way the threads are managed.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [5]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">train_cfg</span><span class="o">=</span><span class="s2">"""ThreadProcessor: {</span>
<span class="s2">  Verbosity:3</span>
<span class="s2">  NumThreads: 3</span>
<span class="s2">  NumBatchStorage: 3</span>
<span class="s2">  RandomAccess: true</span>
<span class="s2">  InputFiles: ["</span><span class="si">%s</span><span class="s2">"]  </span>
<span class="s2">  ProcessName: ["image","label"]</span>
<span class="s2">  ProcessType: ["BatchFillerImage2D","BatchFillerPIDLabel"]</span>
<span class="s2">  ProcessList: {</span>
<span class="s2">    image: {</span>
<span class="s2">      Verbosity:3</span>
<span class="s2">      ImageProducer: "data"</span>
<span class="s2">      Channels: [2]</span>
<span class="s2">      EnableMirror: true</span>
<span class="s2">    }</span>
<span class="s2">    label: {</span>
<span class="s2">      Verbosity:3</span>
<span class="s2">      ParticleProducer: "mctruth"</span>
<span class="s2">      PdgClassList: [2212,11,211,13,22]</span>
<span class="s2">    }</span>
<span class="s2">  }</span>
<span class="s2">}</span>
<span class="s2">"""</span><span class="o">%</span><span class="p">(</span><span class="n">path_to_train_data</span><span class="p">)</span>

<span class="n">test_cfg</span><span class="o">=</span><span class="s2">"""ThreadProcessorTest: {</span>
<span class="s2">  Verbosity:3</span>
<span class="s2">  NumThreads: 2</span>
<span class="s2">  NumBatchStorage: 2</span>
<span class="s2">  RandomAccess: true</span>
<span class="s2">  InputFiles: ["</span><span class="si">%s</span><span class="s2">"]</span>
<span class="s2">  ProcessName: ["imagetest","labeltest"]</span>
<span class="s2">  ProcessType: ["BatchFillerImage2D","BatchFillerPIDLabel"]</span>
<span class="s2">  ProcessList: {</span>
<span class="s2">    imagetest: {</span>
<span class="s2">      Verbosity:3</span>
<span class="s2">      ImageProducer: "data"</span>
<span class="s2">      Channels: [2]</span>
<span class="s2">      EnableMirror: false</span>
<span class="s2">    }</span>
<span class="s2">    labeltest: {</span>
<span class="s2">      Verbosity:3</span>
<span class="s2">      ParticleProducer: "mctruth"</span>
<span class="s2">      PdgClassList: [2212,11,211,13,22]</span>
<span class="s2">    }</span>
<span class="s2">  }</span>
<span class="s2">}</span>
<span class="s2">"""</span><span class="o">%</span><span class="p">(</span><span class="n">path_to_test_data</span><span class="p">)</span>

<span class="n">train_cfg_out</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"train_dataloader.cfg"</span><span class="p">,</span><span class="s1">'w'</span><span class="p">)</span>
<span class="k">print</span> <span class="o">>></span> <span class="n">train_cfg_out</span><span class="p">,</span><span class="n">train_cfg</span>
<span class="n">train_cfg_out</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">test_cfg_out</span>  <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"valid_dataloader.cfg"</span><span class="p">,</span><span class="s1">'w'</span><span class="p">)</span>
<span class="k">print</span> <span class="o">>></span> <span class="n">test_cfg_out</span><span class="p">,</span><span class="n">test_cfg</span>
<span class="n">test_cfg_out</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Setup-Network">Setup Network<a class="anchor-link" href="#Setup-Network">¶</a></h1><h2 id="Define-network">Define network<a class="anchor-link" href="#Define-network">¶</a></h2><p>We use ResNet-18 as implemented in the torchvision module.  We reproduce it here and make a slight modification: we change the number of input channels from 3 to 1.  The original resnet expects an RGB image.  For our example, we only use the image from one plane from our hypothetical LAr TPC detector.</p>
<p>Original can be found <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py">here</a>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [6]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># define convolution without bias that we will use throughout the network</span>
<span class="k">def</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">"""3x3 convolution with padding"""</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                     <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>


<span class="c1"># implements one ResNet unit</span>
<span class="k">class</span> <span class="nc">BasicBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">expansion</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">downsample</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BasicBlock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">conv3x3</span><span class="p">(</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">planes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">conv3x3</span><span class="p">(</span><span class="n">planes</span><span class="p">,</span> <span class="n">planes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">planes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span> <span class="o">=</span> <span class="n">downsample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">+=</span> <span class="n">residual</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>
    
<span class="c1"># define the network. It provides options for </span>
<span class="k">class</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">input_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        inputs</span>
<span class="sd">        ------</span>
<span class="sd">        block: type of resnet unit</span>
<span class="sd">        layers: list of 4 ints. defines number of basic block units in each set of resnet units</span>
<span class="sd">        num_classes: output classes</span>
<span class="sd">        input_channels: number of channels in input images</span>
<span class="sd">        """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inplanes</span> <span class="o">=</span> <span class="mi">64</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ResNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                               <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># had to change stride of avgpool from original from 1 to 2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># I've added dropout to the network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout2d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c1">#print "block.expansion=",block.expansion                                                                                                                                                           </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span> <span class="o">*</span> <span class="n">block</span><span class="o">.</span><span class="n">expansion</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
                <span class="n">n</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="o">.</span><span class="n">out_channels</span>
                <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="n">n</span><span class="p">))</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
                <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplanes</span> <span class="o">!=</span> <span class="n">planes</span> <span class="o">*</span> <span class="n">block</span><span class="o">.</span><span class="n">expansion</span><span class="p">:</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span> <span class="o">*</span> <span class="n">block</span><span class="o">.</span><span class="n">expansion</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">planes</span> <span class="o">*</span> <span class="n">block</span><span class="o">.</span><span class="n">expansion</span><span class="p">),</span>
            <span class="p">)</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inplanes</span> <span class="o">=</span> <span class="n">planes</span> <span class="o">*</span> <span class="n">block</span><span class="o">.</span><span class="n">expansion</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1">#print "avepool: ",x.data.shape                                                                                                                                                                     </span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1">#print "view: ",x.data.shape                                                                                                                                                                        </span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>


    
<span class="c1"># define a helper function for ResNet-18</span>
<span class="k">def</span> <span class="nf">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">"""Constructs a ResNet-18 model.                                                                                                                                                                        </span>
<span class="sd">                                                                                                                                                                                                            </span>
<span class="sd">    Args:                                                                                                                                                                                                   </span>
<span class="sd">        pretrained (bool): If True, returns a model pre-trained on ImageNet                                                                                                                                 </span>
<span class="sd">    """</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ResNet</span><span class="p">(</span><span class="n">BasicBlock</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">pretrained</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_zoo</span><span class="o">.</span><span class="n">load_url</span><span class="p">(</span><span class="n">model_urls</span><span class="p">[</span><span class="s1">'resnet18'</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Create-instance-of-network">Create instance of network<a class="anchor-link" href="#Create-instance-of-network">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [7]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">input_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[7]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>ResNet(
  (conv1): Conv2d (1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1))
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d (64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (downsample): Sequential(
        (0): Conv2d (64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d (128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (downsample): Sequential(
        (0): Conv2d (128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (downsample): Sequential(
        (0): Conv2d (256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=2, padding=0, ceil_mode=False, count_include_pad=True)
  (dropout): Dropout2d(p=0.5, inplace)
  (fc): Linear(in_features=512, out_features=5)
)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Define-loss-function">Define loss function<a class="anchor-link" href="#Define-loss-function">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [8]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Define-optimizer-and-set-training-parameters">Define optimizer and set training parameters<a class="anchor-link" href="#Define-optimizer-and-set-training-parameters">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [9]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">1.0e-3</span>
<span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">1.0e-3</span>
<span class="n">batchsize</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">batchsize_valid</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">epochs</span>      <span class="o">=</span> <span class="mi">100</span>
<span class="n">nbatches_per_iteration</span> <span class="o">=</span> <span class="mi">10000</span><span class="o">/</span><span class="n">batchsize</span>
<span class="n">nbatches_per_valid</span>     <span class="o">=</span> <span class="mi">1000</span><span class="o">/</span><span class="n">batchsize_valid</span>

<span class="c1"># We use SGD</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Define-training-and-validation-steps">Define training and validation steps<a class="anchor-link" href="#Define-training-and-validation-steps">¶</a></h1><p>We define functions and classes to help us perform training.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Define-an-object-that-will-help-us-track-averages">Define an object that will help us track averages<a class="anchor-link" href="#Define-an-object-that-will-help-us-track-averages">¶</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [10]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="k">class</span> <span class="nc">AverageMeter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""Computes and stores the average and current value"""</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avg</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val</span> <span class="o">=</span> <span class="n">val</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sum</span> <span class="o">+=</span> <span class="n">val</span> <span class="o">*</span> <span class="n">n</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">+=</span> <span class="n">n</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sum</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">count</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training-step">Training step<a class="anchor-link" href="#Training-step">¶</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [11]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">nbatches</span><span class="p">,</span> <span class="n">iteration</span><span class="p">,</span> <span class="n">print_freq</span><span class="p">):</span>
    <span class="n">batch_time</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>
    <span class="n">data_time</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>
    <span class="n">format_time</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>
    <span class="n">train_time</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>
    <span class="n">top1</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>

    <span class="c1"># switch to train mode                                                                                                                                                                                  </span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nbatches</span><span class="p">):</span>                                                                                                                                                   
        <span class="n">batchstart</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">train_loader</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="c1"># measure data loading time                                                                                                                                                                         </span>
        <span class="n">data_time</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">end</span><span class="p">)</span>

        <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"image"</span><span class="p">]</span>
        <span class="n">lbl</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"label"</span><span class="p">]</span>
        <span class="n">img_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span> <span class="p">)</span>
        <span class="n">lbl_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="p">(</span><span class="n">lbl</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span> <span class="p">)</span>
        <span class="c1"># batch loop                                                                                                                                                                                        </span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">imgtmp</span> <span class="o">=</span> <span class="n">img</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span> <span class="p">(</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">)</span> <span class="p">)</span>
            <span class="n">img_np</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">,:,:]</span> <span class="o">=</span> <span class="n">padandcropandflip</span><span class="p">(</span><span class="n">imgtmp</span><span class="p">)</span> <span class="c1"># data augmentation                                                                                                                                 </span>
            <span class="n">lbl_np</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">lbl</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="nb">input</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">img_np</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">lbl_np</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

        <span class="c1"># measure data formatting time                                                                                                                                                                      </span>
        <span class="n">format_time</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">end</span><span class="p">)</span>

        <span class="c1"># convert into torch variable</span>
        <span class="n">input_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">target_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>

        <span class="c1"># compute output                                                                                                                                                                                    </span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_var</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target_var</span><span class="p">)</span>

        <span class="c1"># measure accuracy and record loss                                                                                                                                                                  </span>
        <span class="n">prec1</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">topk</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">top1</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">prec1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        
        <span class="c1"># compute gradient and do SGD step                                                                                                                                                                  </span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">train_time</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">end</span><span class="p">)</span>

        <span class="c1"># measure elapsed time                                                                                                                                                                              </span>
        <span class="n">batch_time</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">batchstart</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">print_freq</span><span class="o">></span><span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="n">print_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="p">(</span><span class="n">iteration</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">nbatches</span><span class="p">,</span>
                      <span class="n">batch_time</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">batch_time</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                      <span class="n">data_time</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">data_time</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                      <span class="n">format_time</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">format_time</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                      <span class="n">train_time</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">train_time</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                      <span class="n">losses</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">losses</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                      <span class="n">top1</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">top1</span><span class="o">.</span><span class="n">avg</span><span class="p">)</span>
            <span class="k">print</span> <span class="s2">"Iteration: [</span><span class="si">%d</span><span class="s2">][</span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2">]</span><span class="se">\t</span><span class="s2">Time </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)</span><span class="se">\t</span><span class="s2">Data </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)</span><span class="se">\t</span><span class="s2">Format </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)</span><span class="se">\t</span><span class="s2">Train </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)</span><span class="se">\t</span><span class="s2">Loss </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)</span><span class="se">\t</span><span class="s2">Prec@1 </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)"</span><span class="o">%</span><span class="k">status</span>
            
    <span class="k">return</span> <span class="n">losses</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span><span class="n">top1</span><span class="o">.</span><span class="n">avg</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Validation-step">Validation step<a class="anchor-link" href="#Validation-step">¶</a></h3><p>Here we process the test data and accumilate the accuracy.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [12]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="n">val_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">nbatches</span><span class="p">,</span> <span class="n">print_freq</span><span class="p">):</span>
    <span class="n">batch_time</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>
    <span class="n">top1</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>

    <span class="c1"># switch to evaluate mode                                                                                                                                                                               </span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nbatches</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">val_loader</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"imagetest"</span><span class="p">]</span>
        <span class="n">lbl</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"labeltest"</span><span class="p">]</span>
        <span class="n">img_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span> <span class="p">)</span>
        <span class="n">lbl_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="p">(</span><span class="n">lbl</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span> <span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">img_np</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">,:,:]</span> <span class="o">=</span> <span class="n">img</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span> <span class="p">(</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">)</span> <span class="p">)</span>
            <span class="n">lbl_np</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">lbl</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="nb">input</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">img_np</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">lbl_np</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

        <span class="c1"># convert into torch variable</span>
        <span class="n">input_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">volatile</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">target_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">volatile</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c1"># compute output                                                                                                                                                                                    </span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_var</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target_var</span><span class="p">)</span>

        <span class="c1"># measure accuracy and record loss                                                                                                                                                                  </span>
        <span class="n">prec1</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">topk</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">top1</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">prec1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

        <span class="c1"># measure elapsed time                                                                                                                                                                              </span>
        <span class="n">batch_time</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">end</span><span class="p">)</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">print_freq</span><span class="o">></span><span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="n">print_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">nbatches</span><span class="p">,</span><span class="n">batch_time</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">batch_time</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span><span class="n">losses</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">losses</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span><span class="n">top1</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">top1</span><span class="o">.</span><span class="n">avg</span><span class="p">)</span>
            <span class="k">print</span> <span class="s2">"Test: [</span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2">]</span><span class="se">\t</span><span class="s2">Time </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)</span><span class="se">\t</span><span class="s2">Loss </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)</span><span class="se">\t</span><span class="s2">Prec@1 </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)"</span><span class="o">%</span><span class="k">status</span>
 
    <span class="k">print</span> <span class="s2">"Test:Result* Prec@1 </span><span class="si">%.3f</span><span class="se">\t</span><span class="s2">Loss </span><span class="si">%.3f</span><span class="s2">"</span><span class="o">%</span><span class="p">(</span><span class="n">top1</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span><span class="n">losses</span><span class="o">.</span><span class="n">avg</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">top1</span><span class="o">.</span><span class="n">avg</span><span class="p">),</span><span class="nb">float</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">avg</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="utility-functions">utility functions<a class="anchor-link" href="#utility-functions">¶</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [13]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="k">def</span> <span class="nf">adjust_learning_rate</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="sd">"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""</span>
    <span class="c1">#lr = lr * (0.5 ** (epoch // 300))                                                                                                                                                                      </span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="c1">#lr = lr*0.992                                                                                                                                                                                          </span>
    <span class="c1">#print "adjust learning rate to ",lr                                                                                                                                                                    </span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s1">'lr'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

<span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">topk</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,)):</span>
    <span class="sd">"""Computes the precision@k for the specified values of k"""</span>
    <span class="n">maxk</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">topk</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">pred</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">maxk</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">pred</span><span class="p">))</span>

    <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">topk</span><span class="p">:</span>
        <span class="n">correct_k</span> <span class="o">=</span> <span class="n">correct</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">correct_k</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mf">100.0</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">res</span>

<span class="k">def</span> <span class="nf">dump_lr_schedule</span><span class="p">(</span> <span class="n">startlr</span><span class="p">,</span> <span class="n">numepochs</span> <span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">numepochs</span><span class="p">):</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">startlr</span><span class="o">*</span><span class="p">(</span><span class="mf">0.5</span><span class="o">**</span><span class="p">(</span><span class="n">epoch</span><span class="o">//</span><span class="mi">300</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">epoch</span><span class="o">%</span><span class="k">10</span>==0:
            <span class="k">print</span> <span class="s2">"Epoch [</span><span class="si">%d</span><span class="s2">] lr=</span><span class="si">%.3e</span><span class="s2">"</span><span class="o">%</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span><span class="n">lr</span><span class="p">)</span>
    <span class="k">print</span> <span class="s2">"Epoch [</span><span class="si">%d</span><span class="s2">] lr=</span><span class="si">%.3e</span><span class="s2">"</span><span class="o">%</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span><span class="n">lr</span><span class="p">)</span>
    <span class="k">return</span>

<span class="k">def</span> <span class="nf">padandcropandflip</span><span class="p">(</span><span class="n">npimg2d</span><span class="p">):</span>
    <span class="n">imgpad</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="p">(</span><span class="mi">264</span><span class="p">,</span><span class="mi">264</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span> <span class="p">)</span>
    <span class="n">imgpad</span><span class="p">[</span><span class="mi">4</span><span class="p">:</span><span class="mi">256</span><span class="o">+</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">:</span><span class="mi">256</span><span class="o">+</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">npimg2d</span><span class="p">[:,:]</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span><span class="o">></span><span class="mf">0.5</span><span class="p">:</span>
        <span class="n">imgpad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span> <span class="n">imgpad</span><span class="p">,</span> <span class="mi">0</span> <span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span><span class="o">></span><span class="mf">0.5</span><span class="p">:</span>
        <span class="n">imgpad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span> <span class="n">imgpad</span><span class="p">,</span> <span class="mi">1</span> <span class="p">)</span>
    <span class="n">randx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">randy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">imgpad</span><span class="p">[</span><span class="n">randx</span><span class="p">:</span><span class="n">randx</span><span class="o">+</span><span class="mi">256</span><span class="p">,</span><span class="n">randy</span><span class="p">:</span><span class="n">randy</span><span class="o">+</span><span class="mi">256</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">is_best</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s1">'checkpoint.pth.tar'</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">></span><span class="mi">0</span><span class="p">:</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="s2">"checkpoint.</span><span class="si">%d</span><span class="s2">th.tar"</span><span class="o">%</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_best</span><span class="p">:</span>
        <span class="n">shutil</span><span class="o">.</span><span class="n">copyfile</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">'model_best.pth.tar'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Load-the-datasets-and-start-data-loading-threads">Load the datasets and start data loading threads<a class="anchor-link" href="#Load-the-datasets-and-start-data-loading-threads">¶</a></h1><h3 id="Training-data">Training data<a class="anchor-link" href="#Training-data">¶</a></h3><p>For the training data, we ask that all the data is loaded into memory. Since we need to get many, many batches to train the network, reducing the time to get a batch of images will pay off in the long run.</p>
<p>However, we first pay an upfront cost: this step takes a LONG time.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [14]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1">#capevents = 100 # for debugging, to keep the time this step takes to a minimum</span>
<span class="c1">#iotrain = LArCVDataset("train_dataloader.cfg", "ThreadProcessor", loadallinmem=True, max_inmem_events=capevents)</span>
<span class="n">iotrain</span> <span class="o">=</span> <span class="n">LArCVDataset</span><span class="p">(</span><span class="s2">"train_dataloader.cfg"</span><span class="p">,</span> <span class="s2">"ThreadProcessor"</span><span class="p">,</span> <span class="n">loadallinmem</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">iotrain</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="n">batchsize</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Attempting to load all  50000  into memory. good luck
loading event 100 of 50000
loading event 200 of 50000
loading event 300 of 50000
loading event 400 of 50000
loading event 500 of 50000
loading event 600 of 50000
loading event 700 of 50000
loading event 800 of 50000
loading event 900 of 50000
loading event 1000 of 50000
loading event 1100 of 50000
loading event 1200 of 50000
loading event 1300 of 50000
loading event 1400 of 50000
loading event 1500 of 50000
loading event 1600 of 50000
loading event 1700 of 50000
loading event 1800 of 50000
loading event 1900 of 50000
loading event 2000 of 50000
loading event 2100 of 50000
loading event 2200 of 50000
loading event 2300 of 50000
loading event 2400 of 50000
loading event 2500 of 50000
loading event 2600 of 50000
loading event 2700 of 50000
loading event 2800 of 50000
loading event 2900 of 50000
loading event 3000 of 50000
loading event 3100 of 50000
loading event 3200 of 50000
loading event 3300 of 50000
loading event 3400 of 50000
loading event 3500 of 50000
loading event 3600 of 50000
loading event 3700 of 50000
loading event 3800 of 50000
loading event 3900 of 50000
loading event 4000 of 50000
loading event 4100 of 50000
loading event 4200 of 50000
loading event 4300 of 50000
loading event 4400 of 50000
loading event 4500 of 50000
loading event 4600 of 50000
loading event 4700 of 50000
loading event 4800 of 50000
loading event 4900 of 50000
loading event 5000 of 50000
loading event 5100 of 50000
loading event 5200 of 50000
loading event 5300 of 50000
loading event 5400 of 50000
loading event 5500 of 50000
loading event 5600 of 50000
loading event 5700 of 50000
loading event 5800 of 50000
loading event 5900 of 50000
loading event 6000 of 50000
loading event 6100 of 50000
loading event 6200 of 50000
loading event 6300 of 50000
loading event 6400 of 50000
loading event 6500 of 50000
loading event 6600 of 50000
loading event 6700 of 50000
loading event 6800 of 50000
loading event 6900 of 50000
loading event 7000 of 50000
loading event 7100 of 50000
loading event 7200 of 50000
loading event 7300 of 50000
loading event 7400 of 50000
loading event 7500 of 50000
loading event 7600 of 50000
loading event 7700 of 50000
loading event 7800 of 50000
loading event 7900 of 50000
loading event 8000 of 50000
loading event 8100 of 50000
loading event 8200 of 50000
loading event 8300 of 50000
loading event 8400 of 50000
loading event 8500 of 50000
loading event 8600 of 50000
loading event 8700 of 50000
loading event 8800 of 50000
loading event 8900 of 50000
loading event 9000 of 50000
loading event 9100 of 50000
loading event 9200 of 50000
loading event 9300 of 50000
loading event 9400 of 50000
loading event 9500 of 50000
loading event 9600 of 50000
loading event 9700 of 50000
loading event 9800 of 50000
loading event 9900 of 50000
loading event 10000 of 50000
loading event 10100 of 50000
loading event 10200 of 50000
loading event 10300 of 50000
loading event 10400 of 50000
loading event 10500 of 50000
loading event 10600 of 50000
loading event 10700 of 50000
loading event 10800 of 50000
loading event 10900 of 50000
loading event 11000 of 50000
loading event 11100 of 50000
loading event 11200 of 50000
loading event 11300 of 50000
loading event 11400 of 50000
loading event 11500 of 50000
loading event 11600 of 50000
loading event 11700 of 50000
loading event 11800 of 50000
loading event 11900 of 50000
loading event 12000 of 50000
loading event 12100 of 50000
loading event 12200 of 50000
loading event 12300 of 50000
loading event 12400 of 50000
loading event 12500 of 50000
loading event 12600 of 50000
loading event 12700 of 50000
loading event 12800 of 50000
loading event 12900 of 50000
loading event 13000 of 50000
loading event 13100 of 50000
loading event 13200 of 50000
loading event 13300 of 50000
loading event 13400 of 50000
loading event 13500 of 50000
loading event 13600 of 50000
loading event 13700 of 50000
loading event 13800 of 50000
loading event 13900 of 50000
loading event 14000 of 50000
loading event 14100 of 50000
loading event 14200 of 50000
loading event 14300 of 50000
loading event 14400 of 50000
loading event 14500 of 50000
loading event 14600 of 50000
loading event 14700 of 50000
loading event 14800 of 50000
loading event 14900 of 50000
loading event 15000 of 50000
loading event 15100 of 50000
loading event 15200 of 50000
loading event 15300 of 50000
loading event 15400 of 50000
loading event 15500 of 50000
loading event 15600 of 50000
loading event 15700 of 50000
loading event 15800 of 50000
loading event 15900 of 50000
loading event 16000 of 50000
loading event 16100 of 50000
loading event 16200 of 50000
loading event 16300 of 50000
loading event 16400 of 50000
loading event 16500 of 50000
loading event 16600 of 50000
loading event 16700 of 50000
loading event 16800 of 50000
loading event 16900 of 50000
loading event 17000 of 50000
loading event 17100 of 50000
loading event 17200 of 50000
loading event 17300 of 50000
loading event 17400 of 50000
loading event 17500 of 50000
loading event 17600 of 50000
loading event 17700 of 50000
loading event 17800 of 50000
loading event 17900 of 50000
loading event 18000 of 50000
loading event 18100 of 50000
loading event 18200 of 50000
loading event 18300 of 50000
loading event 18400 of 50000
loading event 18500 of 50000
loading event 18600 of 50000
loading event 18700 of 50000
loading event 18800 of 50000
loading event 18900 of 50000
loading event 19000 of 50000
loading event 19100 of 50000
loading event 19200 of 50000
loading event 19300 of 50000
loading event 19400 of 50000
loading event 19500 of 50000
loading event 19600 of 50000
loading event 19700 of 50000
loading event 19800 of 50000
loading event 19900 of 50000
loading event 20000 of 50000
loading event 20100 of 50000
loading event 20200 of 50000
loading event 20300 of 50000
loading event 20400 of 50000
loading event 20500 of 50000
loading event 20600 of 50000
loading event 20700 of 50000
loading event 20800 of 50000
loading event 20900 of 50000
loading event 21000 of 50000
loading event 21100 of 50000
loading event 21200 of 50000
loading event 21300 of 50000
loading event 21400 of 50000
loading event 21500 of 50000
loading event 21600 of 50000
loading event 21700 of 50000
loading event 21800 of 50000
loading event 21900 of 50000
loading event 22000 of 50000
loading event 22100 of 50000
loading event 22200 of 50000
loading event 22300 of 50000
loading event 22400 of 50000
loading event 22500 of 50000
loading event 22600 of 50000
loading event 22700 of 50000
loading event 22800 of 50000
loading event 22900 of 50000
loading event 23000 of 50000
loading event 23100 of 50000
loading event 23200 of 50000
loading event 23300 of 50000
loading event 23400 of 50000
loading event 23500 of 50000
loading event 23600 of 50000
loading event 23700 of 50000
loading event 23800 of 50000
loading event 23900 of 50000
loading event 24000 of 50000
loading event 24100 of 50000
loading event 24200 of 50000
loading event 24300 of 50000
loading event 24400 of 50000
loading event 24500 of 50000
loading event 24600 of 50000
loading event 24700 of 50000
loading event 24800 of 50000
loading event 24900 of 50000
loading event 25000 of 50000
loading event 25100 of 50000
loading event 25200 of 50000
loading event 25300 of 50000
loading event 25400 of 50000
loading event 25500 of 50000
loading event 25600 of 50000
loading event 25700 of 50000
loading event 25800 of 50000
loading event 25900 of 50000
loading event 26000 of 50000
loading event 26100 of 50000
loading event 26200 of 50000
loading event 26300 of 50000
loading event 26400 of 50000
loading event 26500 of 50000
loading event 26600 of 50000
loading event 26700 of 50000
loading event 26800 of 50000
loading event 26900 of 50000
loading event 27000 of 50000
loading event 27100 of 50000
loading event 27200 of 50000
loading event 27300 of 50000
loading event 27400 of 50000
loading event 27500 of 50000
loading event 27600 of 50000
loading event 27700 of 50000
loading event 27800 of 50000
loading event 27900 of 50000
loading event 28000 of 50000
loading event 28100 of 50000
loading event 28200 of 50000
loading event 28300 of 50000
loading event 28400 of 50000
loading event 28500 of 50000
loading event 28600 of 50000
loading event 28700 of 50000
loading event 28800 of 50000
loading event 28900 of 50000
loading event 29000 of 50000
loading event 29100 of 50000
loading event 29200 of 50000
loading event 29300 of 50000
loading event 29400 of 50000
loading event 29500 of 50000
loading event 29600 of 50000
loading event 29700 of 50000
loading event 29800 of 50000
loading event 29900 of 50000
loading event 30000 of 50000
loading event 30100 of 50000
loading event 30200 of 50000
loading event 30300 of 50000
loading event 30400 of 50000
loading event 30500 of 50000
loading event 30600 of 50000
loading event 30700 of 50000
loading event 30800 of 50000
loading event 30900 of 50000
loading event 31000 of 50000
loading event 31100 of 50000
loading event 31200 of 50000
loading event 31300 of 50000
loading event 31400 of 50000
loading event 31500 of 50000
loading event 31600 of 50000
loading event 31700 of 50000
loading event 31800 of 50000
loading event 31900 of 50000
loading event 32000 of 50000
loading event 32100 of 50000
loading event 32200 of 50000
loading event 32300 of 50000
loading event 32400 of 50000
loading event 32500 of 50000
loading event 32600 of 50000
loading event 32700 of 50000
loading event 32800 of 50000
loading event 32900 of 50000
loading event 33000 of 50000
loading event 33100 of 50000
loading event 33200 of 50000
loading event 33300 of 50000
loading event 33400 of 50000
loading event 33500 of 50000
loading event 33600 of 50000
loading event 33700 of 50000
loading event 33800 of 50000
loading event 33900 of 50000
loading event 34000 of 50000
loading event 34100 of 50000
loading event 34200 of 50000
loading event 34300 of 50000
loading event 34400 of 50000
loading event 34500 of 50000
loading event 34600 of 50000
loading event 34700 of 50000
loading event 34800 of 50000
loading event 34900 of 50000
loading event 35000 of 50000
loading event 35100 of 50000
loading event 35200 of 50000
loading event 35300 of 50000
loading event 35400 of 50000
loading event 35500 of 50000
loading event 35600 of 50000
loading event 35700 of 50000
loading event 35800 of 50000
loading event 35900 of 50000
loading event 36000 of 50000
loading event 36100 of 50000
loading event 36200 of 50000
loading event 36300 of 50000
loading event 36400 of 50000
loading event 36500 of 50000
loading event 36600 of 50000
loading event 36700 of 50000
loading event 36800 of 50000
loading event 36900 of 50000
loading event 37000 of 50000
loading event 37100 of 50000
loading event 37200 of 50000
loading event 37300 of 50000
loading event 37400 of 50000
loading event 37500 of 50000
loading event 37600 of 50000
loading event 37700 of 50000
loading event 37800 of 50000
loading event 37900 of 50000
loading event 38000 of 50000
loading event 38100 of 50000
loading event 38200 of 50000
loading event 38300 of 50000
loading event 38400 of 50000
loading event 38500 of 50000
loading event 38600 of 50000
loading event 38700 of 50000
loading event 38800 of 50000
loading event 38900 of 50000
loading event 39000 of 50000
loading event 39100 of 50000
loading event 39200 of 50000
loading event 39300 of 50000
loading event 39400 of 50000
loading event 39500 of 50000
loading event 39600 of 50000
loading event 39700 of 50000
loading event 39800 of 50000
loading event 39900 of 50000
loading event 40000 of 50000
loading event 40100 of 50000
loading event 40200 of 50000
loading event 40300 of 50000
loading event 40400 of 50000
loading event 40500 of 50000
loading event 40600 of 50000
loading event 40700 of 50000
loading event 40800 of 50000
loading event 40900 of 50000
loading event 41000 of 50000
loading event 41100 of 50000
loading event 41200 of 50000
loading event 41300 of 50000
loading event 41400 of 50000
loading event 41500 of 50000
loading event 41600 of 50000
loading event 41700 of 50000
loading event 41800 of 50000
loading event 41900 of 50000
loading event 42000 of 50000
loading event 42100 of 50000
loading event 42200 of 50000
loading event 42300 of 50000
loading event 42400 of 50000
loading event 42500 of 50000
loading event 42600 of 50000
loading event 42700 of 50000
loading event 42800 of 50000
loading event 42900 of 50000
loading event 43000 of 50000
loading event 43100 of 50000
loading event 43200 of 50000
loading event 43300 of 50000
loading event 43400 of 50000
loading event 43500 of 50000
loading event 43600 of 50000
loading event 43700 of 50000
loading event 43800 of 50000
loading event 43900 of 50000
loading event 44000 of 50000
loading event 44100 of 50000
loading event 44200 of 50000
loading event 44300 of 50000
loading event 44400 of 50000
loading event 44500 of 50000
loading event 44600 of 50000
loading event 44700 of 50000
loading event 44800 of 50000
loading event 44900 of 50000
loading event 45000 of 50000
loading event 45100 of 50000
loading event 45200 of 50000
loading event 45300 of 50000
loading event 45400 of 50000
loading event 45500 of 50000
loading event 45600 of 50000
loading event 45700 of 50000
loading event 45800 of 50000
loading event 45900 of 50000
loading event 46000 of 50000
loading event 46100 of 50000
loading event 46200 of 50000
loading event 46300 of 50000
loading event 46400 of 50000
loading event 46500 of 50000
loading event 46600 of 50000
loading event 46700 of 50000
loading event 46800 of 50000
loading event 46900 of 50000
loading event 47000 of 50000
loading event 47100 of 50000
loading event 47200 of 50000
loading event 47300 of 50000
loading event 47400 of 50000
loading event 47500 of 50000
loading event 47600 of 50000
loading event 47700 of 50000
loading event 47800 of 50000
loading event 47900 of 50000
loading event 48000 of 50000
loading event 48100 of 50000
loading event 48200 of 50000
loading event 48300 of 50000
loading event 48400 of 50000
loading event 48500 of 50000
loading event 48600 of 50000
loading event 48700 of 50000
loading event 48800 of 50000
loading event 48900 of 50000
loading event 49000 of 50000
loading event 49100 of 50000
loading event 49200 of 50000
loading event 49300 of 50000
loading event 49400 of 50000
loading event 49500 of 50000
loading event 49600 of 50000
loading event 49700 of 50000
loading event 49800 of 50000
loading event 49900 of 50000
elapsed time to bring data into memory:  2599.11497784 sec
ThreadProcessor : {
  InputFiles : ["/home/taritree/working/dlphysics/testset/train_50k.root"]
  NumBatchStorage : 3
  NumThreads : 3
  ProcessName : ["image","label"]
  ProcessType : ["BatchFillerImage2D","BatchFillerPIDLabel"]
  RandomAccess : true
  Verbosity : 3
  ProcessList : {
    image : {
      Channels : [2]
      EnableMirror : true
      ImageProducer : "data"
      Verbosity : 3
    }

    label : {
      ParticleProducer : "mctruth"
      PdgClassList : [2212,11,211,13,22]
      Verbosity : 3
    }

  }

}

<span class="ansi-yellow-intense-fg"> setting verbosity </span>3
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>Error in <tprotoclass::finddatamember>: data member with index 0 is not found in class thread
Error in <createrealdata>: Cannot find data member # 0 of class thread for parent larcv::ThreadProcessor!
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Validation-data">Validation data<a class="anchor-link" href="#Validation-data">¶</a></h3><p>For the validation data, we do not load data into memory all at once. We will use the validation only periodically, in between many training steps. During those training steps, the thread filler will load data into memory.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [15]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">iovalid</span> <span class="o">=</span> <span class="n">LArCVDataset</span><span class="p">(</span><span class="s2">"valid_dataloader.cfg"</span><span class="p">,</span> <span class="s2">"ThreadProcessorTest"</span><span class="p">)</span>
<span class="n">iovalid</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="n">batchsize_valid</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre><span class="ansi-yellow-intense-fg"> setting verbosity </span>3
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Training-Loop">Training Loop<a class="anchor-link" href="#Training-Loop">¶</a></h1><p>For each iteration of the training loop, we</p>
<ul>
<li>set the learning rate</li>
<li>perform a training iteration which involves forward and backward passes for the number of batches set in <code>nbatches_per_iteration</code> </li>
<li>run a validation iteration over <code>nbatches_per_valid</code></li>
<li>for both the training and validation iterations, we save the average loss and average accuracy over the batches. Values are stored in a numpy array</li>
<li>we update the plot the training versus validation loss</li>
<li>every 10 epochs (i.e. 50 iterations), we save the state of the model and the optimizer</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [16]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">best_prec1</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="c1"># define plots</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'epoch'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'loss'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">1.0e-3</span><span class="p">,</span><span class="mf">10.0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">"log"</span><span class="p">)</span>

<span class="c1"># iterations:</span>
<span class="c1"># there are 50k events in the training set</span>
<span class="c1"># we use 10k images per training iteration</span>
<span class="c1"># therefore, 1 epoch is 5 iterations</span>
<span class="n">start_iteration</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">start_epoch</span>
<span class="n">end_iteration</span>   <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">epochs</span>
<span class="n">num_iterations</span>  <span class="o">=</span> <span class="n">end_iteration</span> <span class="o">-</span> <span class="n">start_iteration</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start_epoch</span><span class="p">,</span><span class="n">epochs</span><span class="p">,</span><span class="n">num_iterations</span><span class="p">)</span>

<span class="c1"># numpy arrays for loss and accuracy</span>
<span class="n">y_train_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">)</span>
<span class="n">y_train_acc</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">)</span>
<span class="n">y_valid_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">)</span>
<span class="n">y_valid_acc</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">)</span>


<span class="k">for</span> <span class="n">iiter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">num_iterations</span><span class="p">):</span>
    
    <span class="n">iteration</span> <span class="o">=</span> <span class="n">start_iteration</span> <span class="o">+</span> <span class="n">iiter</span>
    <span class="n">epoch</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">iteration</span><span class="p">)</span><span class="o">/</span><span class="mf">5.0</span>
    
    <span class="c1"># set the learning rate</span>
    <span class="n">adjust_learning_rate</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">iteration</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
    <span class="n">iterout</span> <span class="o">=</span> <span class="s2">"Iteration [</span><span class="si">%d</span><span class="s2">]: "</span><span class="o">%</span><span class="p">(</span><span class="n">iteration</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">iterout</span> <span class="o">+=</span> <span class="s2">"lr=</span><span class="si">%.3e</span><span class="s2">"</span><span class="o">%</span><span class="p">(</span><span class="n">param_group</span><span class="p">[</span><span class="s1">'lr'</span><span class="p">])</span>
    <span class="k">print</span> <span class="n">iterout</span>

    <span class="c1"># train for one iteration                                                                                                                                                                               </span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">train_ave_loss</span><span class="p">,</span> <span class="n">train_ave_acc</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">iotrain</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> 
                                              <span class="n">nbatches_per_iteration</span><span class="p">,</span> <span class="n">iteration</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">,</span><span class="n">e</span><span class="p">:</span>
        <span class="k">print</span> <span class="s2">"Error in training routine!"</span>
        <span class="k">print</span> <span class="n">e</span><span class="o">.</span><span class="n">message</span>
        <span class="k">print</span> <span class="n">e</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">traceback</span><span class="o">.</span><span class="n">print_exc</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="k">break</span>
    <span class="k">print</span> <span class="s2">"Iteration [</span><span class="si">%d</span><span class="s2">] train aveloss=</span><span class="si">%.3f</span><span class="s2"> aveacc=</span><span class="si">%.3f</span><span class="s2">"</span><span class="o">%</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span>
                                                           <span class="n">train_ave_loss</span><span class="p">,</span>
                                                           <span class="n">train_ave_acc</span><span class="p">)</span>
    <span class="n">y_train_loss</span><span class="p">[</span><span class="n">iiter</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_ave_loss</span>
    <span class="n">y_train_acc</span><span class="p">[</span><span class="n">iiter</span><span class="p">]</span>  <span class="o">=</span> <span class="n">train_ave_acc</span>

    <span class="c1"># evaluate on validation set                                                                                                                                                                        </span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">prec1</span><span class="p">,</span><span class="n">valid_loss</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">iovalid</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">nbatches_per_valid</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">,</span><span class="n">e</span><span class="p">:</span>
        <span class="k">print</span> <span class="s2">"Error in validation routine!"</span>
        <span class="k">print</span> <span class="n">e</span><span class="o">.</span><span class="n">message</span>
        <span class="k">print</span> <span class="n">e</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">traceback</span><span class="o">.</span><span class="n">print_exc</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="k">break</span>
    <span class="k">print</span> <span class="s2">"Test[</span><span class="si">%d</span><span class="s2">]:Result* Prec@1 </span><span class="si">%.3f</span><span class="se">\t</span><span class="s2">Loss </span><span class="si">%.3f</span><span class="s2">"</span><span class="o">%</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span><span class="n">prec1</span><span class="p">,</span><span class="n">valid_loss</span><span class="p">)</span>
    <span class="n">y_valid_loss</span><span class="p">[</span><span class="n">iiter</span><span class="p">]</span> <span class="o">=</span> <span class="n">valid_loss</span>
    <span class="n">y_valid_acc</span><span class="p">[</span><span class="n">iiter</span><span class="p">]</span>  <span class="o">=</span> <span class="n">prec1</span>
        
    <span class="c1"># plot up to current iteration</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="n">iiter</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">y_train_loss</span><span class="p">[:</span><span class="n">iiter</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="s1">'b'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="n">iiter</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">y_valid_loss</span><span class="p">[:</span><span class="n">iiter</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="s1">'r'</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
    
    <span class="c1"># remember best prec@1 and save checkpoint                                                                                                                                                          </span>
    <span class="n">is_best</span> <span class="o">=</span> <span class="n">prec1</span> <span class="o">></span> <span class="n">best_prec1</span>
    <span class="n">best_prec1</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">prec1</span><span class="p">,</span> <span class="n">best_prec1</span><span class="p">)</span>
    <span class="n">save_checkpoint</span><span class="p">({</span>
        <span class="s1">'epoch'</span><span class="p">:</span> <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s1">'state_dict'</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
        <span class="s1">'best_prec1'</span><span class="p">:</span> <span class="n">best_prec1</span><span class="p">,</span>
        <span class="s1">'optimizer'</span> <span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="p">},</span> <span class="n">is_best</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">iteration</span><span class="o">%</span><span class="k">50</span>==0:
        <span class="n">save_checkpoint</span><span class="p">({</span>
            <span class="s1">'epoch'</span><span class="p">:</span> <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s1">'state_dict'</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s1">'best_prec1'</span><span class="p">:</span> <span class="n">best_prec1</span><span class="p">,</span>
            <span class="s1">'optimizer'</span> <span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
        <span class="p">},</span> <span class="bp">False</span><span class="p">,</span> <span class="n">iteration</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div id="38835020-e22b-4ed0-92d4-af6cb9606031"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#38835020-e22b-4ed0-92d4-af6cb9606031');
/* Put everything inside the global mpl namespace */
window.mpl = {};

mpl.get_websocket_type = function() {
    if (typeof(WebSocket) !== 'undefined') {
        return WebSocket;
    } else if (typeof(MozWebSocket) !== 'undefined') {
        return MozWebSocket;
    } else {
        alert('Your browser does not have WebSocket support.' +
              'Please try Chrome, Safari or Firefox ≥ 6. ' +
              'Firefox 4 and 5 are also supported but you ' +
              'have to enable WebSockets in about:config.');
    };
}

mpl.figure = function(figure_id, websocket, ondownload, parent_element) {
    this.id = figure_id;

    this.ws = websocket;

    this.supports_binary = (this.ws.binaryType != undefined);

    if (!this.supports_binary) {
        var warnings = document.getElementById("mpl-warnings");
        if (warnings) {
            warnings.style.display = 'block';
            warnings.textContent = (
                "This browser does not support binary websocket messages. " +
                    "Performance may be slow.");
        }
    }

    this.imageObj = new Image();

    this.context = undefined;
    this.message = undefined;
    this.canvas = undefined;
    this.rubberband_canvas = undefined;
    this.rubberband_context = undefined;
    this.format_dropdown = undefined;

    this.image_mode = 'full';

    this.root = $('<div/>');
    this._root_extra_style(this.root)
    this.root.attr('style', 'display: inline-block');

    $(parent_element).append(this.root);

    this._init_header(this);
    this._init_canvas(this);
    this._init_toolbar(this);

    var fig = this;

    this.waiting = false;

    this.ws.onopen =  function () {
            fig.send_message("supports_binary", {value: fig.supports_binary});
            fig.send_message("send_image_mode", {});
            fig.send_message("refresh", {});
        }

    this.imageObj.onload = function() {
            if (fig.image_mode == 'full') {
                // Full images could contain transparency (where diff images
                // almost always do), so we need to clear the canvas so that
                // there is no ghosting.
                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);
            }
            fig.context.drawImage(fig.imageObj, 0, 0);
        };

    this.imageObj.onunload = function() {
        this.ws.close();
    }

    this.ws.onmessage = this._make_on_message_function(this);

    this.ondownload = ondownload;
}

mpl.figure.prototype._init_header = function() {
    var titlebar = $(
        '<div class="ui-dialog-titlebar ui-widget-header ui-corner-all ' +
        'ui-helper-clearfix"/>');
    var titletext = $(
        '<div class="ui-dialog-title" style="width: 100%; ' +
        'text-align: center; padding: 3px;"/>');
    titlebar.append(titletext)
    this.root.append(titlebar);
    this.header = titletext[0];
}



mpl.figure.prototype._canvas_extra_style = function(canvas_div) {

}


mpl.figure.prototype._root_extra_style = function(canvas_div) {

}

mpl.figure.prototype._init_canvas = function() {
    var fig = this;

    var canvas_div = $('<div/>');

    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');

    function canvas_keyboard_event(event) {
        return fig.key_event(event, event['data']);
    }

    canvas_div.keydown('key_press', canvas_keyboard_event);
    canvas_div.keyup('key_release', canvas_keyboard_event);
    this.canvas_div = canvas_div
    this._canvas_extra_style(canvas_div)
    this.root.append(canvas_div);

    var canvas = $('<canvas/>');
    canvas.addClass('mpl-canvas');
    canvas.attr('style', "left: 0; top: 0; z-index: 0; outline: 0")

    this.canvas = canvas[0];
    this.context = canvas[0].getContext("2d");

    var rubberband = $('<canvas/>');
    rubberband.attr('style', "position: absolute; left: 0; top: 0; z-index: 1;")

    var pass_mouse_events = true;

    canvas_div.resizable({
        start: function(event, ui) {
            pass_mouse_events = false;
        },
        resize: function(event, ui) {
            fig.request_resize(ui.size.width, ui.size.height);
        },
        stop: function(event, ui) {
            pass_mouse_events = true;
            fig.request_resize(ui.size.width, ui.size.height);
        },
    });

    function mouse_event_fn(event) {
        if (pass_mouse_events)
            return fig.mouse_event(event, event['data']);
    }

    rubberband.mousedown('button_press', mouse_event_fn);
    rubberband.mouseup('button_release', mouse_event_fn);
    // Throttle sequential mouse events to 1 every 20ms.
    rubberband.mousemove('motion_notify', mouse_event_fn);

    rubberband.mouseenter('figure_enter', mouse_event_fn);
    rubberband.mouseleave('figure_leave', mouse_event_fn);

    canvas_div.on("wheel", function (event) {
        event = event.originalEvent;
        event['data'] = 'scroll'
        if (event.deltaY < 0) {
            event.step = 1;
        } else {
            event.step = -1;
        }
        mouse_event_fn(event);
    });

    canvas_div.append(canvas);
    canvas_div.append(rubberband);

    this.rubberband = rubberband;
    this.rubberband_canvas = rubberband[0];
    this.rubberband_context = rubberband[0].getContext("2d");
    this.rubberband_context.strokeStyle = "#000000";

    this._resize_canvas = function(width, height) {
        // Keep the size of the canvas, canvas container, and rubber band
        // canvas in synch.
        canvas_div.css('width', width)
        canvas_div.css('height', height)

        canvas.attr('width', width);
        canvas.attr('height', height);

        rubberband.attr('width', width);
        rubberband.attr('height', height);
    }

    // Set the figure to an initial 600x600px, this will subsequently be updated
    // upon first draw.
    this._resize_canvas(600, 600);

    // Disable right mouse context menu.
    $(this.rubberband_canvas).bind("contextmenu",function(e){
        return false;
    });

    function set_focus () {
        canvas.focus();
        canvas_div.focus();
    }

    window.setTimeout(set_focus, 100);
}

mpl.figure.prototype._init_toolbar = function() {
    var fig = this;

    var nav_element = $('<div/>')
    nav_element.attr('style', 'width: 100%');
    this.root.append(nav_element);

    // Define a callback function for later on.
    function toolbar_event(event) {
        return fig.toolbar_button_onclick(event['data']);
    }
    function toolbar_mouse_event(event) {
        return fig.toolbar_button_onmouseover(event['data']);
    }

    for(var toolbar_ind in mpl.toolbar_items) {
        var name = mpl.toolbar_items[toolbar_ind][0];
        var tooltip = mpl.toolbar_items[toolbar_ind][1];
        var image = mpl.toolbar_items[toolbar_ind][2];
        var method_name = mpl.toolbar_items[toolbar_ind][3];

        if (!name) {
            // put a spacer in here.
            continue;
        }
        var button = $('<button/>');
        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +
                        'ui-button-icon-only');
        button.attr('role', 'button');
        button.attr('aria-disabled', 'false');
        button.click(method_name, toolbar_event);
        button.mouseover(tooltip, toolbar_mouse_event);

        var icon_img = $('<span/>');
        icon_img.addClass('ui-button-icon-primary ui-icon');
        icon_img.addClass(image);
        icon_img.addClass('ui-corner-all');

        var tooltip_span = $('<span/>');
        tooltip_span.addClass('ui-button-text');
        tooltip_span.html(tooltip);

        button.append(icon_img);
        button.append(tooltip_span);

        nav_element.append(button);
    }

    var fmt_picker_span = $('<span/>');

    var fmt_picker = $('<select/>');
    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');
    fmt_picker_span.append(fmt_picker);
    nav_element.append(fmt_picker_span);
    this.format_dropdown = fmt_picker[0];

    for (var ind in mpl.extensions) {
        var fmt = mpl.extensions[ind];
        var option = $(
            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);
        fmt_picker.append(option)
    }

    // Add hover states to the ui-buttons
    $( ".ui-button" ).hover(
        function() { $(this).addClass("ui-state-hover");},
        function() { $(this).removeClass("ui-state-hover");}
    );

    var status_bar = $('<span class="mpl-message"/>');
    nav_element.append(status_bar);
    this.message = status_bar[0];
}

mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {
    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,
    // which will in turn request a refresh of the image.
    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});
}

mpl.figure.prototype.send_message = function(type, properties) {
    properties['type'] = type;
    properties['figure_id'] = this.id;
    this.ws.send(JSON.stringify(properties));
}

mpl.figure.prototype.send_draw_message = function() {
    if (!this.waiting) {
        this.waiting = true;
        this.ws.send(JSON.stringify({type: "draw", figure_id: this.id}));
    }
}


mpl.figure.prototype.handle_save = function(fig, msg) {
    var format_dropdown = fig.format_dropdown;
    var format = format_dropdown.options[format_dropdown.selectedIndex].value;
    fig.ondownload(fig, format);
}


mpl.figure.prototype.handle_resize = function(fig, msg) {
    var size = msg['size'];
    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {
        fig._resize_canvas(size[0], size[1]);
        fig.send_message("refresh", {});
    };
}

mpl.figure.prototype.handle_rubberband = function(fig, msg) {
    var x0 = msg['x0'];
    var y0 = fig.canvas.height - msg['y0'];
    var x1 = msg['x1'];
    var y1 = fig.canvas.height - msg['y1'];
    x0 = Math.floor(x0) + 0.5;
    y0 = Math.floor(y0) + 0.5;
    x1 = Math.floor(x1) + 0.5;
    y1 = Math.floor(y1) + 0.5;
    var min_x = Math.min(x0, x1);
    var min_y = Math.min(y0, y1);
    var width = Math.abs(x1 - x0);
    var height = Math.abs(y1 - y0);

    fig.rubberband_context.clearRect(
        0, 0, fig.canvas.width, fig.canvas.height);

    fig.rubberband_context.strokeRect(min_x, min_y, width, height);
}

mpl.figure.prototype.handle_figure_label = function(fig, msg) {
    // Updates the figure title.
    fig.header.textContent = msg['label'];
}

mpl.figure.prototype.handle_cursor = function(fig, msg) {
    var cursor = msg['cursor'];
    switch(cursor)
    {
    case 0:
        cursor = 'pointer';
        break;
    case 1:
        cursor = 'default';
        break;
    case 2:
        cursor = 'crosshair';
        break;
    case 3:
        cursor = 'move';
        break;
    }
    fig.rubberband_canvas.style.cursor = cursor;
}

mpl.figure.prototype.handle_message = function(fig, msg) {
    fig.message.textContent = msg['message'];
}

mpl.figure.prototype.handle_draw = function(fig, msg) {
    // Request the server to send over a new figure.
    fig.send_draw_message();
}

mpl.figure.prototype.handle_image_mode = function(fig, msg) {
    fig.image_mode = msg['mode'];
}

mpl.figure.prototype.updated_canvas_event = function() {
    // Called whenever the canvas gets updated.
    this.send_message("ack", {});
}

// A function to construct a web socket function for onmessage handling.
// Called in the figure constructor.
mpl.figure.prototype._make_on_message_function = function(fig) {
    return function socket_on_message(evt) {
        if (evt.data instanceof Blob) {
            /* FIXME: We get "Resource interpreted as Image but
             * transferred with MIME type text/plain:" errors on
             * Chrome.  But how to set the MIME type?  It doesn't seem
             * to be part of the websocket stream */
            evt.data.type = "image/png";

            /* Free the memory for the previous frames */
            if (fig.imageObj.src) {
                (window.URL || window.webkitURL).revokeObjectURL(
                    fig.imageObj.src);
            }

            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(
                evt.data);
            fig.updated_canvas_event();
            fig.waiting = false;
            return;
        }
        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == "data:image/png;base64") {
            fig.imageObj.src = evt.data;
            fig.updated_canvas_event();
            fig.waiting = false;
            return;
        }

        var msg = JSON.parse(evt.data);
        var msg_type = msg['type'];

        // Call the  "handle_{type}" callback, which takes
        // the figure and JSON message as its only arguments.
        try {
            var callback = fig["handle_" + msg_type];
        } catch (e) {
            console.log("No handler for the '" + msg_type + "' message type: ", msg);
            return;
        }

        if (callback) {
            try {
                // console.log("Handling '" + msg_type + "' message: ", msg);
                callback(fig, msg);
            } catch (e) {
                console.log("Exception inside the 'handler_" + msg_type + "' callback:", e, e.stack, msg);
            }
        }
    };
}

// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas
mpl.findpos = function(e) {
    //this section is from http://www.quirksmode.org/js/events_properties.html
    var targ;
    if (!e)
        e = window.event;
    if (e.target)
        targ = e.target;
    else if (e.srcElement)
        targ = e.srcElement;
    if (targ.nodeType == 3) // defeat Safari bug
        targ = targ.parentNode;

    // jQuery normalizes the pageX and pageY
    // pageX,Y are the mouse positions relative to the document
    // offset() returns the position of the element relative to the document
    var x = e.pageX - $(targ).offset().left;
    var y = e.pageY - $(targ).offset().top;

    return {"x": x, "y": y};
};

/*
 * return a copy of an object with only non-object keys
 * we need this to avoid circular references
 * http://stackoverflow.com/a/24161582/3208463
 */
function simpleKeys (original) {
  return Object.keys(original).reduce(function (obj, key) {
    if (typeof original[key] !== 'object')
        obj[key] = original[key]
    return obj;
  }, {});
}

mpl.figure.prototype.mouse_event = function(event, name) {
    var canvas_pos = mpl.findpos(event)

    if (name === 'button_press')
    {
        this.canvas.focus();
        this.canvas_div.focus();
    }

    var x = canvas_pos.x;
    var y = canvas_pos.y;

    this.send_message(name, {x: x, y: y, button: event.button,
                             step: event.step,
                             guiEvent: simpleKeys(event)});

    /* This prevents the web browser from automatically changing to
     * the text insertion cursor when the button is pressed.  We want
     * to control all of the cursor setting manually through the
     * 'cursor' event from matplotlib */
    event.preventDefault();
    return false;
}

mpl.figure.prototype._key_event_extra = function(event, name) {
    // Handle any extra behaviour associated with a key event
}

mpl.figure.prototype.key_event = function(event, name) {

    // Prevent repeat events
    if (name == 'key_press')
    {
        if (event.which === this._key)
            return;
        else
            this._key = event.which;
    }
    if (name == 'key_release')
        this._key = null;

    var value = '';
    if (event.ctrlKey && event.which != 17)
        value += "ctrl+";
    if (event.altKey && event.which != 18)
        value += "alt+";
    if (event.shiftKey && event.which != 16)
        value += "shift+";

    value += 'k';
    value += event.which.toString();

    this._key_event_extra(event, name);

    this.send_message(name, {key: value,
                             guiEvent: simpleKeys(event)});
    return false;
}

mpl.figure.prototype.toolbar_button_onclick = function(name) {
    if (name == 'download') {
        this.handle_save(this, null);
    } else {
        this.send_message("toolbar_button", {name: name});
    }
};

mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {
    this.message.textContent = tooltip;
};
mpl.toolbar_items = [["Home", "Reset original view", "fa fa-home icon-home", "home"], ["Back", "Back to  previous view", "fa fa-arrow-left icon-arrow-left", "back"], ["Forward", "Forward to next view", "fa fa-arrow-right icon-arrow-right", "forward"], ["", "", "", ""], ["Pan", "Pan axes with left mouse, zoom with right", "fa fa-arrows icon-move", "pan"], ["Zoom", "Zoom to rectangle", "fa fa-square-o icon-check-empty", "zoom"], ["", "", "", ""], ["Download", "Download plot", "fa fa-floppy-o icon-save", "download"]];

mpl.extensions = ["eps", "jpeg", "pdf", "png", "ps", "raw", "svg", "tif"];

mpl.default_extension = "png";var comm_websocket_adapter = function(comm) {
    // Create a "websocket"-like object which calls the given IPython comm
    // object with the appropriate methods. Currently this is a non binary
    // socket, so there is still some room for performance tuning.
    var ws = {};

    ws.close = function() {
        comm.close()
    };
    ws.send = function(m) {
        //console.log('sending', m);
        comm.send(m);
    };
    // Register the callback with on_msg.
    comm.on_msg(function(msg) {
        //console.log('receiving', msg['content']['data'], msg);
        // Pass the mpl event to the overriden (by mpl) onmessage function.
        ws.onmessage(msg['content']['data'])
    });
    return ws;
}

mpl.mpl_figure_comm = function(comm, msg) {
    // This is the function which gets called when the mpl process
    // starts-up an IPython Comm through the "matplotlib" channel.

    var id = msg.content.data.id;
    // Get hold of the div created by the display call when the Comm
    // socket was opened in Python.
    var element = $("#" + id);
    var ws_proxy = comm_websocket_adapter(comm)

    function ondownload(figure, format) {
        window.open(figure.imageObj.src);
    }

    var fig = new mpl.figure(id, ws_proxy,
                           ondownload,
                           element.get(0));

    // Call onopen now - mpl needs it, as it is assuming we've passed it a real
    // web socket which is closed, not our websocket->open comm proxy.
    ws_proxy.onopen();

    fig.parent_element = element.get(0);
    fig.cell_info = mpl.find_output_cell("<div id='" + id + "'></div>");
    if (!fig.cell_info) {
        console.error("Failed to find cell for figure", id, fig);
        return;
    }

    var output_index = fig.cell_info[2]
    var cell = fig.cell_info[0];

};

mpl.figure.prototype.handle_close = function(fig, msg) {
    fig.root.unbind('remove')

    // Update the output cell to use the data from the current canvas.
    fig.push_to_output();
    var dataURL = fig.canvas.toDataURL();
    // Re-enable the keyboard manager in IPython - without this line, in FF,
    // the notebook keyboard shortcuts fail.
    IPython.keyboard_manager.enable()
    $(fig.parent_element).html('<img src="' + dataURL + '">');
    fig.close_ws(fig, msg);
}

mpl.figure.prototype.close_ws = function(fig, msg){
    fig.send_message('closing', msg);
    // fig.ws.close()
}

mpl.figure.prototype.push_to_output = function(remove_interactive) {
    // Turn the data on the canvas into data in the output cell.
    var dataURL = this.canvas.toDataURL();
    this.cell_info[1]['text/html'] = '<img src="' + dataURL + '">';
}

mpl.figure.prototype.updated_canvas_event = function() {
    // Tell IPython that the notebook contents must change.
    IPython.notebook.set_dirty(true);
    this.send_message("ack", {});
    var fig = this;
    // Wait a second, then push the new image to the DOM so
    // that it is saved nicely (might be nice to debounce this).
    setTimeout(function () { fig.push_to_output() }, 1000);
}

mpl.figure.prototype._init_toolbar = function() {
    var fig = this;

    var nav_element = $('<div/>')
    nav_element.attr('style', 'width: 100%');
    this.root.append(nav_element);

    // Define a callback function for later on.
    function toolbar_event(event) {
        return fig.toolbar_button_onclick(event['data']);
    }
    function toolbar_mouse_event(event) {
        return fig.toolbar_button_onmouseover(event['data']);
    }

    for(var toolbar_ind in mpl.toolbar_items){
        var name = mpl.toolbar_items[toolbar_ind][0];
        var tooltip = mpl.toolbar_items[toolbar_ind][1];
        var image = mpl.toolbar_items[toolbar_ind][2];
        var method_name = mpl.toolbar_items[toolbar_ind][3];

        if (!name) { continue; };

        var button = $('<button class="btn btn-default" href="#" title="' + name + '"><i class="fa ' + image + ' fa-lg"></i></button>');
        button.click(method_name, toolbar_event);
        button.mouseover(tooltip, toolbar_mouse_event);
        nav_element.append(button);
    }

    // Add the status bar.
    var status_bar = $('<span class="mpl-message" style="text-align:right; float: right;"/>');
    nav_element.append(status_bar);
    this.message = status_bar[0];

    // Add the close button to the window.
    var buttongrp = $('<div class="btn-group inline pull-right"></div>');
    var button = $('<button class="btn btn-mini btn-primary" href="#" title="Stop Interaction"><i class="fa fa-power-off icon-remove icon-large"></i></button>');
    button.click(function (evt) { fig.handle_close(fig, {}); } );
    button.mouseover('Stop Interaction', toolbar_mouse_event);
    buttongrp.append(button);
    var titlebar = this.root.find($('.ui-dialog-titlebar'));
    titlebar.prepend(buttongrp);
}

mpl.figure.prototype._root_extra_style = function(el){
    var fig = this
    el.on("remove", function(){
	fig.close_ws(fig, {});
    });
}

mpl.figure.prototype._canvas_extra_style = function(el){
    // this is important to make the div 'focusable
    el.attr('tabindex', 0)
    // reach out to IPython and tell the keyboard manager to turn it's self
    // off when our div gets focus

    // location in version 3
    if (IPython.notebook.keyboard_manager) {
        IPython.notebook.keyboard_manager.register_events(el);
    }
    else {
        // location in version 2
        IPython.keyboard_manager.register_events(el);
    }

}

mpl.figure.prototype._key_event_extra = function(event, name) {
    var manager = IPython.notebook.keyboard_manager;
    if (!manager)
        manager = IPython.keyboard_manager;

    // Check for shift+enter
    if (event.shiftKey && event.which == 13) {
        this.canvas_div.blur();
        event.shiftKey = false;
        // Send a "J" for go to next cell
        event.which = 74;
        event.keyCode = 74;
        manager.command_mode();
        manager.handle_keydown(event);
    }
}

mpl.figure.prototype.handle_save = function(fig, msg) {
    fig.ondownload(fig, null);
}


mpl.find_output_cell = function(html_output) {
    // Return the cell and output element which can be found *uniquely* in the notebook.
    // Note - this is a bit hacky, but it is done because the "notebook_saving.Notebook"
    // IPython event is triggered only after the cells have been serialised, which for
    // our purposes (turning an active figure into a static one), is too late.
    var cells = IPython.notebook.get_cells();
    var ncells = cells.length;
    for (var i=0; i<ncells; i++) {
        var cell = cells[i];
        if (cell.cell_type === 'code'){
            for (var j=0; j<cell.output_area.outputs.length; j++) {
                var data = cell.output_area.outputs[j];
                if (data.data) {
                    // IPython >= 3 moved mimebundle to data attribute of output
                    data = data.data;
                }
                if (data['text/html'] == html_output) {
                    return [cell, data, j];
                }
            }
        }
    }
}

// Register the function which deals with the matplotlib target/channel.
// The kernel may be null if the page has been refreshed.
if (IPython.notebook.kernel != null) {
    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);
}

</script>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_html rendered_html output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAAAgAElEQVR4Xu2dCdjkVJW/f83WrN3QIggIjAiyCi6IgiKgYMtoK6AOKArqIMu0zF8WRRBZZEBExGVoBhfQkU1URrEV7FFRNoVBQEBW2UF2bLrZmrX/z6lK+PLlSyo3dVKpVNWbRx67u3KTm/eeVN46d8kksUEAAhCAAAQgAAEIjBSBSSN1tVwsBCAAAQhAAAIQgIAQQIIAAhCAAAQgAAEIjBgBBHDEGpzLhQAEIAABCEAAAgggMQABCEAAAhCAAARGjAACOGINzuVCAAIQgAAEIAABBJAYgAAEIAABCEAAAiNGAAEcsQbnciEAAQhAAAIQgAACSAxAAAIQgAAEIACBESOAAI5Yg3O5EIAABCAAAQhAAAEkBiAAAQhAAAIQgMCIEUAAR6zBuVwIQAACEIAABCCAABIDEIAABCAAAQhAYMQIIIAj1uBcLgQgAAEIQAACEEAAiQEIQAACEIAABCAwYgQQwBFrcC4XAhCAAAQgAAEIIIDEAAQgAAEIQAACEBgxAgjgiDU4lwsBCEAAAhCAAAQQQGIAAhCAAAQgAAEIjBgBBHDEGpzLhQAEIAABCEAAAgggMQABCEAAAhCAAARGjAACOGINzuVCAAIQgAAEIAABBJAYgAAEIAABCEAAAiNGAAEcsQbnciEAAQhAAAIQgAACSAxAAAIQgAAEIACBESOAAI5Yg3O5EIAABCAAAQhAAAEkBiAAAQhAAAIQgMCIEUAAR6zBuVwIQAACEIAABCCAABIDEIAABCAAAQhAYMQIIIAj1uBcLgQgAAEIQAACEEAAiQEIQAACEIAABCAwYgQQwBFrcC4XAhCAAAQgAAEIIIDEAAQgAAEIQAACEBgxAgjgiDU4lwsBCEAAAhCAAAQQQGIAAhCAAAQgAAEIjBgBBHDEGpzLhQAEIAABCEAAAgggMQABCEAAAhCAAARGjAACOGINzuVCAAIQgAAEIAABBJAYgAAEIAABCEAAAiNGAAEcsQbnciEAAQhAAAIQgAACSAxAAAIQgAAEIACBESOAAGY3uHG5UNL6kv5L0mEjFhdcLgQgAAEIQAACQ0wAAcxv3NUkvVPS2gjgEN8BXBoEIAABCEBgBAkggJ0bfXdJr0YAR/DO4JIhAAEIQAACQ0wAAUQAhzi8uTQIQAACEIAABLIIDKsA7ixppqRNJC0raXFJL6YAHClpD0lTJF0Z7X99ah8ygNw3EIAABCAAAQgMHYFhFcDtJE2TtLSk72UI4GclfVrS9pJuk3S4pN0kvUbSU4lWNgG0MYBfHLqW54IgAAEIQAACEBhZAsMqgHGDbiXpggwBvF3SCZJOjHZcVNL9kvaTdEb0b2dKem0kkSaJJosvpCLF+K0q6fGRjSAuHAIQgAAEIDCYBJaTdJ+khYNZfV+tR1EArcv3MUmbS7o8gW+OpOskHVgCqc0UvrfE/uwKAQhAAAIQgEBzCLxS0t+bU536ajKKAmiNfXe0xt/NCdQ/kjRf0p4l8JtMzrvnnns0ZYr9kc1D4JBDDtExxxzjOQRlJcGxujCAZQ9YTp3aPui8edUdvMyR+n3+MnXN2JeYdAKMis+fP1+rr766/c0C0p79I7eNogBWmQFsCeC8efMQwApunf33318nnGA982weAnD00BtfFpY9YDkpeuws7FOvW7/P70RKTDoBJgRwavvHAAJYDdLGHaXMGEAbB7B/YgxgyMUggCGUAvfhiy0QVMFucKyGox0Flj1g2W8B6/f5nUiJSSdABPAlgMOaAVwkmvhhAni+JBvoaRM4no0Ge9o4P5sF/B5JNiHk0GgW8LqpWcBFkdYSwJkzZ2rGjBmaPn160f583oHAnDlzYFhBhMCxAojRIWDZA5b9FrB+n9+JlJh0ApRkDGfPnq1Zs2aRAfTjbNwRbPmW7ydm9pjoWn/DNpIuimp7hKS9Ijn8c846gEUXRgawiBCfQwACEEgS6LeA9fv8dUSDda8/84w0ebIUX28d5x2gc9gYQLqAB6jBGlhVBLCBjUKVIACBBhPot4D1+/x1NM1ZZ0kf+Yi08cbSNdfUccaBOwcCKA1rF3BdwYgA1kWa80AAAsNBoN8C1u/z19GKe+8tffvb0lJLSU8l321Qx8kH4xwIIALojVQE0EuQ8hCAwGgR6LeA9fv8dbT2TjtJP/tZuwt4wYI6zjhw50AAEUBv0DIJxEuQ8hCAwGgR6LeA9fv8dbT21ltLF14oLb649KzNfWRLEmASSJsGXcC++4IMoI8fpSEwGARMGmxQ/RJLDEZ9m1rLgw6SjjuuXTvWAexdK73lLdLll0uLLSY991zvzjPARyYDiAB6wxcB9BKkPAQGgcAoZI3qaIfkjFQEMErD9GBh7A03lG64QVp0Uen55+to2YE7BwKIAHqDFgH0EqQ8BAaBAAJYTSv1WwBPOUXaY4/+ZiDTJHsRW696lXTnne0lYF58sZq2G7KjIIAIoDekEUAvQcpDYBAI9OIhPQjXXXUd+y2AH/uYdPrpwy+Aq6wiPfBAs66z6lhyHg8BRACdISQE0EuQ8hAYBAIIYDWt1G8B3Hxz6bLLyotRL9u/F8deaSXp4YfLX2c1rTwQR0EAEUBvoDIL2EuQ8hAYBAK9eEgPwnVXXcd+C+C0adLcueXFqJft34tjr7CC9Nhj5a+z6vZu6PGYBdxuGGYB+wKUDKCPH6UhMBgEevGQHowrr7aW/RZAWxYlnhRRZhJKL9u/F8dedlnpyScRwA7RSwYQAfR+uSGAXoKUh8AgEOjFQ3oQrrvqOvZbALs9fy/bvxfHtjeAxAtAlxHdqtu7wcdDABFAb3gigF6ClIfAIBDoxUN6EK676jrmCVhdfEdFAJdcsr1upW0IYGYUI4AIoPfrDQH0EqQ8BAaBQF2CMggsPHVEACfS60Vs2YLl8QLQCCACmHPPMgbQ82UmZgH78FEaAgNCoBcP6QG59EqriQDWI4D2BpAXXiAD2CF4yQCSAfR+tzEL2EuQ8hAYBAIIYDWt1EQBtIWSrV7JuqWvtpft34tj2xtA4gWgyQBOiF1mAbeRkAH0fa3RBezjR2kIDAaBXjykB+PKq61lEwUwpG1D9gkllT5WlceO67DIImNj/xBAuoDpAg69O0vthwCWwsXOEBhQAr14SA8oCle1EcCxTGMsZr2IrW4nu7gad7AK0wVMBtAbsQiglyDlITAIBHrxkB6E6666jr0SwND2yTp/SNmQfUJZ1ZEBRAALWwMBRAALg6RgBwTQS5DyEGg6gW7fHtH06+pH/RDAwckAbrSRdP/90qOP9iNSen5OBBAB9AYZAuglSHkINJ0A2ZTqWggBHBwBrDLrWV0EVXYkBBAB9AYTAuglSHkINJ0AAlhdCyGACGB10eQ6EgKIALoCSKwD6OVHeQg0nwACWF0bIYAIYHXR5DoSAogAugIoFsCZM2dqxowZmj59uvd4lIcABJpGAAGsrkVCBNBeYWavMrOFjG2mrK1pV7SFdlcO2iSQmMXzz4dxiDlVEbOhTIvapoGfsw5gu1FYB9AXnHQB+/hRGgLNJ1DFw7T5V1lPDUMEMLlPXKuitexCZWXQBDBe0Hn77aXzzgtvoypiNpRpeK0atScZQATQG5AIoJcg5SHQdAJVPEybfo111Q8BLNcFHPNad13pppvCW6mKmEUAw3kP6J5kAH0NhwD6+FEaAs0nUMXDtPlXWU8NEcDuBHCVVaT77gtvoypiFgEM5z2geyKAvoZDAH38KA2B5hOo4mHa/Kusp4bDKoBlZCm9b6ey8WfLLy/NnRveRlXEbJlrCq9ZY/akC5guYG8wIoBegpSHQNMJVPEwLXONWQ/eYXkYN1kATbBMtLK2Iv5FnyeP2Y0ALr209PnPtyeCHHJIcTRVEbNlrqm4Ro3bAwFEAL1BiQB6CVIeAk0nUMXDtMw1jooAHnSQdOyxbTLJa07yjrnVMQnEzpV3niIZKvrcK4BLLCE9+2z7KEUskjxD9+9GesvEdAP3RQARQG9YIoBegpSHQNMJIIDVtVCS5ZvfLF12GQIY0gVs+8TihwBWEo8IIALoDSQE0EuQ8hBoOoF+CWAye1Mmw9RknkmWL3uZ9MgjCGCIACbbFAGsJMIRQATQG0gtAWQhaC9GykOgwQQQwOoaJ929G8tMU7qATz9d2nXXiddbJOBFnyeP2M0YQASwuhiUxELQ0e+uSqmO3sHIAI5em3PFo0ag1wKYJwTDngHMu75+jgHcYw/pu99FAI1AGakdwO8EMoBkAL1hiwB6CVIeAk0ngABW10JNzwCuv750ww0IIAJYXcw3+EisA+hrHATQx4/SEGg+AQSwujZqugAuu6z0+OMIIAJYXcw3+EgIoK9xEEAfP0pDoPkEEMDq2qjpApjslk5edVF3aNHnnY6VLrvJJtK117Zn/XbTHZ6Ut7zrCWnRMtcUcryG7UMXMF3A3pBEAL0EKQ+BphMYRQHs1cMfASx+FZx3QgwCGPSNggAigEGB0mEnBNBLkPIQaDqBKgUwS6yaOAkEARwflUU8ij4vkwEMFcBNN5Vuuy37FXFVxGyZa2r6PZxRPwQQAfSGLQLoJUh5CDSdQBUP0/gaEcDxrd20ZWCa0AUcKoCdBK2KmEUAm/7N5K4fYwB9CBFAHz9KQ6D5BKp4mCKAbQJFXcBf/KJ01FETY6Jo8eNQWclqy7w6dcrapWsYev4kgyz5TX/eaQwgAuj67iADSAbQFUCSEEAvQcpDoOkEEMDqWqhIAFdZRbr/fgQwzkQigNXFXupICCAC6A0uBNBLkPIQaDoBBLC6FioSwLwz9SoD+MMfSrvvPv6sWecqyvAVfd4pm5g3BrRoFjAZQFdcIoAIoCuAyAB68VEeAgNAAAGsrpGaJoBvf7t08cUIYFYLl5Ha6iKktiMhgAigN9h4F7CXIOUh0HQCCGB1LdQ0AVx5ZemhhxDAERNA3gXcbnAmgfi+2ugC9vGjNASaT2CUBHD77aVf/3qsTYq6Xsu2XtMEsNMYu07dtunrLpMt69Tl23oqR49luoDLRlep/ckAIoClAiZjZwTQS5DyEGg6gVESwJAZsZ72QgCrWwiaMYCeSBQCiAC6AogxgF58lIfAABBAAKtrJAQQAawumlxHQgARQFcAIYBefJSHwAAQQACraySvAOZlvUK7YNNtSRdwftuGMq0uOmo9EgKIAHoDji5gL0HKQ6DpBBDA6loIASQDWF00uY6EACKArgAiA+jFR3kIDAABBLC6RkIAwwXwqqukN7xhIvu8N4gk9wyNWdvvJz+RPvjBiechA1hd3Df0SMwC9jUMGUAfP0pDoPkEQh+mIVeS9VDNmxWadbyqZ+Wmz9H0SSBZ/L7xDWm//dpXUsRnkLqAP/AB6Zxzei+Aiy4qPf88Ahhy/w7ZPgigr0ERQB8/SkOg+QQQwOraqBcZwJe/XHrkkd4JoElYnCHLE8wy2bLQZWBWXHHsupItUHUGcJFFpBdeQACri/KBORIC6GsqBNDHj9KDTKDMQ28YrjMkw1R0nWQAxxNKy0wev07SU0bQu8kA7ruvdOKJnQWzzL0QKoCWmcsSs6oF0Orz4osIYNG9O4SfI4C+RkUAffwoPcgEyjz0huE6EUB/K/YiA9hrAbRxeFdfXb8AdiPDcZlQJp3u4SG/v5kEwiQQ7xcaAuglSPnBJTBMD4iQByEC6I/VQRTAtdaS7rij9wJ4+eXSZpuNTRIpK4DxW1yy3iCS1XU9d640bVr+dQ3T/Z3BEgFEAL1faAiglyDlB5fAMD0gEMB2HHomgYTEwyAK4DLLSE891XsBPOgg6dhjuxfAJP+QdrziirZw5v2wCWnPwf324k0gvAvYHb0IoBshBxhYAsP0gEAAEcD4Rkxny0K6U0PGdqa7Z9Nj+SyDd9559QngrFnSpz+NAEpTJc0f2O9hR8UZA+iAxzqAPniUHnACCGD5BgwRhXT2JnmWomVOytdofImQzFHeOULiYRAzgHUJ4AYbSNdfX58A/r//J33rWwggAuj91hjZ8mQAR7bpufCXugt7LSV1oCYDSAaw7gzgJZdIb33rmPCtsop03331CeCOO0o//zkCiADW8Q07lOdAAIeyWbmoIAIhGZ+gAzVgJwQQAaxbAG2h55/+dEz4pk6VHnusPgF829ukSy9FABHABnwBD2YVWgI4c+ZMzZgxQ9OnTx/Mq6DWEOiGAAJYnhpdwOOZlVkHcJ11pFtvnSgsIV208VnT+2Z1t/dyDKBl/bbcsl0bm118221jwjd5srRgQX0CuOGG0g03jKQAzpkzR7Nnz9YsGweJAJb/HqNEiwAZQAJhdAkggOXbPs3MZMPexGBbiAz1uru9yWMA80RvkATQZvkefHC7vadMkebNGy98WUu4pKMsbyHoMrOAX/ta6aabxl4BlxVXw3R/Z9ypLAPDMjDlv8DHl0AAvQQpP7gEhukB0a8u4Cx5YRLIxHsiLUZJYWmSAJ5/vvTud7frnxVTO+0k/exnY9eXdV2d2j/rh0KWEBaJfNHnefUf3G+rCTVHABFAbzgjgF6ClB9cAghg+bZLM0MAx8tSHtFBEcB4Lb88gXr726WLLx67yssuk97ylnwhzOJRRQawjAA+/bS05JLlY73hJRBABNAbogiglyDlB5cAAli+7RDA8cxCur3jrNcgdAHbxIpY8LLuj+RbRey6vvlNyZZjibe6uoDLCOD990uveEX5WG94CQQQAfSGKALoJUj5wSUwrALYSdKSD+puWg4BHG4BXHNN6c47x2c1k93VK64oPfroGINdd5XOOKPZAnjjjdJ663UT7Y0ugwAigN4ARQC9BCk/uAQQwPJtVySXL7wgLbpo/nEHYRLIzJnSiSdmX0Ne5ilk3NsgZADjmbx29Vn3x9JLS9alGm8mVjYZo8kZwPgdxeWjvdElEEAE0BugCKCXIOUHlwACWL7tigSw6IiDIIBJCUpfT68F8BvfGN+l2un8ed2tnmVg7Hx5Y/TsM5vxnTy+da0+8ECzBfC3v5Xe+c6iyBy4zxFABNAbtAiglyDlB5fAMArgUkuNZWg6jU/rVsRGQQCTElS3AG60kXTddfn3VDqLmJV57KUAps9nS8HMT7yGtoljAO1tIe9//+B+T+XUHAFEAL1BjQB6CVJ+cAkMowAmW6NqAVx//bHuvtDJD+no6FY8Q6MsZHJA3rHyumiT+/c6A7j44tKzz1YjgLvvLp16qrTYYuMzdOmj28zf446buE/W/ZG+/iWWGF/fJgrg6adLNlZxyDYEEAH0hjQC6CVI+cElgACWa7uyS75kHR0BbFPJWwewU/bRPgvNAJ59trTLLhNboFN2MN67Uxdw2bGOnWIgL5t8xx3Sq141vmRRvTstBP3tb0t77lku1gdgbwQQAfSGKQLoJUj5wSWAAJZrOwRw4mvOQjOhoesAViWAa6wh3XPPYArg178u7bdfdQJYNK6y3F3QmL0RQATQG4wIoJcg5QeXAAJYru0QwMERwLxMXVEmLSmgIV3A6Qiqogv4Ax+QzjmnOgE88kjpsMPKxfoA7I0AIoDeMEUAvQQpP7gEEMBybYcAIoB1dAFvvLF07bXVCaC9u/iYY8rF+gDsjQAigN4wbQmg/W/hQvsjGwRGiAACWK6x+ymA8bnf8x7pl7/Mr/egTwKpqgt4kDKAt9wirbPOmFyvsopkb+9IbkWZy05jAK07+YQTysX6AOyNACKA3jBFAL0EKT+4BBDAcm3XBAEsI0hF+6avvgmzgIvqHDoJZJAE8OSTpb32GhPAqVOlefOqE8B99pFOOqlcrA/A3gggAugNUwTQS5Dyg0sgRABD9mkCgawHftXLwDRNAJdcUnryyfFvHiED2J5h7BXA3/1O2nbbdmTvtpv03//d/nMvuoD//d/b7xSOj23tumDB+Lvq73+XVl117N9C2jne55OflE45pQl3aaV1QAARQG9AIYBegpQfXAIhcheyTxMIjKIAhkxSKLPsTC8zgF/+smRj0eKt18vAeAXQumRvvbVd29VWk+69t3cCaKL5m9+MCaC9StBeKZjczj1Xet/7uhNAWwPQ1gIcsg0BRAC9IY0AeglSfnAJhMhdyD4hBKo6Tt65EMBsQWmKAKbbrQkCeOCB0te+NjGisjKI3S53kxWvdiwb97fuuu1PX/c66eqrO2cXDz9cOuKI7gTwgx+UfvKTkLt0oPZBABFAb8BGAjhXCxcu7z0W5SEwOARCsj12NVWJW1XHGXQBjOtfRsyS7WB/trI77ST97Gfto1XV1R0SE3ldj0Vdo00UwFe+UrKu1ay6dXudocvA2LIsRx3VPvPqq0t3391ZAD/6Uem007oTQMscWgZxyDYEEAH0hnRLAJfX7Zq7MLXyuvfIlB9sAr0Wln7TCXnYI4DjW6mKMYBVCWBoXcqIZlFMfO5z0le/Op5JaGasiQLYqZu41wI4Y8bYbO4VV5QefrizAG6zjXTBBd0J4PbbS+ed1+9vnMrPjwAigN6gagngevqTblz4Fu+xKD9MBBDAdmtWxaGq4+TF2KB0AQ+yAE6bJs2dW58AFo1xzMu2hU4C6acArreedPPNbZbLLCM98URnAbT3UN9wQ3cC+I53SDapZcg2BBABzAvpnSV9RtKLkr4p6cc5O7YE8K2arUv13nGvpxyye4XLKUug18JStj5V71+U7YnPVxWHqo6DAE6c5VpXF3BZye4Us8msVFG2LW+84CAL4MorSw891CY0eXJ71m+nbnTrrk6+2i6PWZJ5vM+WW0oXXVT1N0jfj4cAIoBZQWhS9ydJm1r+QtIVkiy993jGzi0B3FGn6mf6BALY91u6QRXotbBUmV3rBhsCWJ5aaLdryJHLdM0mY8X+nPde3U6CFlKnopioUgCnTBlb624UBdCu//HokWSzfp9/vrMAxt3E6R9m8d87LQS9+ebSH/8YEgEDtQ8CiABmBex2kj4oaa/ow29LsilQv80TwH11pP5ThyGAA3X797iyCGAbcFUcqjrOsGQAY5ELDeO0nIXKaBnR7EYAbSbt8ceHXsX4/fIyl+l/H8YM4BJLSM89N8ajaPLIcstJ8+eP7V8mA7jpptIVlgcZrg0BRACzInoXSRtLOiT68FhJV0s6O08Av6WP6t91GgI4XN8PvqvptbBUKVfdXGnRwz6daSgjEln16TXPstmpbq4nVLpC26NMHboVwKI14JLtUhQTnbooQ685uV8/BPCcc9ozqJP3X7rudSwDkyVwnfja4tBPPx0ugLbv0ku394+XmemmjRpcBgEcTgG08XszJW0iaVlJi0dj+ZKheKSkPSRZF+6V0f7XRzu8S5Ld4XtHfy/MAP5SW+i9uhQBbPDNXnvVei0sCGC1TdqtAJZp50EUQHt7RNZSJ1mCPwoCuO++0re+NXgCuPji0rPPhgugtbmNG7Rto42k666r9n5rwNEQwOEUQOvCnSbJfr58L0MAPyvp05K2l3SbpMPtZT2SXiPpqUgKL5X0JkmLSLpc0hadxgBeozW0ie5CABtwUzemCmXEoNtK13GOvLoVPeyzBKHb66xDdhHA8NefJduxmwzg8stLjz3miYZ22X5kAN/wBulKyxl0eK1brzOA118vbbjheH5FXcDxOMH0fRn/PZ1NNuHb2DrCJKVnEPtbrhFHQACHUwDj4NpKki18lM4A3i7pBEknRjsuKul+SftJOiP6t1KzgB/QZL1CCxDARtzWDalEHXJWxzkQwGwx6jTOrAwzT7doHV3ASdHKuq5uBNAz7i9Zh34I4MtfPjb7Nq/tei2AJ5wg7b9/OQFMt2PRGMBLLpFs9q9tr3nN2JIzDfl6raIaCODoCaB1+dpPz82jzF4cR3MkWY77wJKB1ZoFbP3Np2gv7bPf0po+fXrrP7YRJ9BrOQvNwPWqGULPXxWHqo4TImfprEjWgz4WgTL1GsQu4F4IYFG2KjRm+yGAyUxavwTwE5+Qvv/93gqgLfz8nve0z/HqV4+91zi0bRq635w5c2T/2fbss89q1qxZ9sepkhIzZBpa+R5Uy5Y5GdYtKwNogxrutqS2pGgVzdbl/ygKgD1LwmgJ4MNaTO/VxbqMxaBL4hvi3cuIQTcYQgWsm2OHlAk9f1UcqjpOVQIYi1GZejVZAHfeWfpxznKnnTKN3WQAB1kAk0LcLwG0hZmTb/VIx2JejL/4omT/mcQWZQDPPlvaxeZDSlpjDemuu0K+FQZqHzKAZADjgHVlAO/VMjpA39WPFn54oG4AKttDAmXEoJtqhApYN8cOKRN6/qo4VHUcBDB7HcBOXdFNEsB11pH+9rd2K/YjAxgigFkx1mmx7eT+IXJsY/OuvXb8WULK7bmn9J3vSPH/p8+b/Pspp0h72DxJtSeDJBeRDvl+GIB9EMDRE0ALy6wxgPdJskEV8RjA0PBtZQDv1dI6UYfqWB3MOMBQcsO+X53CUmYsWFXcEcDxIhXSBk3OAA6KAH7lK9JBBw2+AFq36m02BzG1hYjcmmtOzMiFlLNM3t13SyutNDaOMT59On6/+U3pM/YyLEmrrCLdZ4/I4doQwOEUQJu5axM/rAv4fEnLSXrBuvztN2M0zs9mAdsAB5PBQ6NZwOtGs4DLRHlLAP+tdZKt9RX9HgEsQ2+Y90UA261bFYeqjpMXc1kCVJS1ST50R0kAjZWthWdr4tXdBZzFPK87Mytm0hKe1+4hQtxpnyyxS94P9maNLWxxiS4E8GUvkx59dHzBEAG0t4fYYtC2iHRySRg7Ujp+jz1WOvjg9jnstXMPPDBU39Y2DnD27NmMARyqVm1fzO6SbISsyV7rlov+vI2k+IWGR0Rv+jA5/HNqHcAySFoCOE/SvVpfG+qGsfuo1w+sMrVk3/oJdGr/KmIjNAPXqysPPX8V11qlSCKA5aUjLQdZ0pcWkCwhzpPFMjFahwAW1afoh0FW+axZ493Ipx3bFmh+ylYsS2whAmhrASbfHpIun/z7YYdJRx3V/hfLGD74YBGVgfucDOBwZgDrDMSXBOq2S/gAACAASURBVHAxLaVl9FRbAEMfjnXWtJfnquoh38s61n3sNJO8h2a39ao7xvKuJ65/Xgasqtio6jgIIAKYjNkyWbx0rJcpW6UA5mXwiuqzyCLtSSBZ24wZ0i9+MfbJAQdIttyMbZZxfOSRbr+pGlsOAUQAvcH5kgAuo0V0lL6oI3W4FrbWj462kK4hby36Xb7XD+d+X1/Z82fJGQJYluL4/XstvHnZGKtF3oN1ULuA0y1RlD0KyQCuu+74teLIAI6nXKUAZolcURuG3H3JNps5UzrppHapadMmdjmHHK/h+yCACKA3RNsCOG+epkydqhe0iFbSg/qt3qnXK5ql1UkAe/1Q815daHkEsFhWEMDQaMrer9f3CgKY3z4hApgllel/ows4f8xkMmFQlMnLaqmqBfBf/1U69dT2mezNLXPn+u7fBpZGABFAb1i2F4KeOVMzZszQu979bn1BR2uq5ukgHdc+dvLLM76xX3hBsl9xvX6oea8utDwCiADW1QWcvqdCY7RoP68AhtSrU2a4qH55D/2sf8+6HztJRZE8ZH2HxdfbKTvaTwG07JVlsfK+g4vq36k9+j0GsA4B3HVX6cwz22eaOrWaV/d1E+M9KsMkkDbYYV4IukehM+6wYxlAm2E1aZIe17I6VZ/Q/9N/lvvyGeSuYgRw8AWw6KHWlDGAIaLVzZ2PAOZT61YA82KmSDiL5CskFu2dvVdfPXakD3xA+ulPJ/7o7jbb1np6lnh8VtkFXIcAfuhDbV62Lbdce/bwkG1kABFAb0hPEMDWHBBJt+rVelAr6226dOLM4Kxfnwigty2aUz50DKBHZqrOHhcdL+Shm26BomOWabEqj5V1XgSwdwKY/r6rQwDz2jMdR2UkLiZU9GMpT9CS0pjHoFs23ZZL1tUWl37ta9v/8r73SbNnt/+8zDLSE0+UuVsHYl8EEAH0Bup4AYxu8Bc1SXdqDS2mF7Vm681z7W1hp4Rr1i9Eb+3Klu82k9dtubL1G5T9EcB2S1UpbVUeCwEcT6BIHrwZQI8AxosXJ+Ur5McIAlj+29IW2Lb1/2x797ul6J25rWVnnnyy/PEaXgIBRAC9ITpRAO2Itsjm5MmthQgXiZYjXEu36k/aXCvpEf1eW2obXdz5S7gfGcFuRa7bcl763vK9qjcCWF4Ai9pi2ARwvfWkG28s142Yjvcy4y77MQYwL9tWJJzJ6/zSlyRbk25UBPDKK6U3vrH8N1sZplksk2e0Y22zjfSHP7T/damlJq47WL6GjSuBACKA3qDMFsBU9mOSFuofWkEr6LHWTOFddYY20A3aV/+pv+nV2kxXZr+f01u7suWLHsJ5x+u2XNn6Vb1/r+qNACKAWbGalrBuHtrpB3X6POnY22oradFFpd//Pv/uKapHNxnALFlN3m+hXa/Jum23nfS//zvx7TKhXNPXWXTdecR62QX8qU9J3/1u+W+6bq4lr4xN+thoI+nSSxHA8i0xUCVKjGIdqOuqq7LjZgFPnz59/HlTX0zxq0m+rv30R22hI3SEfqiP6X2arS11iXbWmfqRdm0fo64MYNaXctlz90qket2Kvao3AjhRAItiuqgthi0DGPMIFaGseyHrPs3LunW6l4rkIU8AOy0s3AsBzBsmM0wCaJm3TrLeSUrLxlKndn/zm6XLL2+fbfJkacGCXn8b13p8ZgFHX9G1Uh++k+VnAO1ac76YbC12ywTaGMHva3dtpYu1tm7TLjpLZ+kjekTT9HL9QwsTX7x2qHFjCMtKWhb7Tl00Zdqq6OFd5lh17tureiOA9QpgFe2Y9fAsyvRkZZQ6xW+oqITeA/0WwNB6pmW3SDiTx03uOwoCmB7zGMq4DNP4mJ3KWDe0dUcPqQDaZdEFTBdw6O2Vt19XAph1sPM0XZP1rN6kP+tFLaLHtLw21RV6VC+XZMq4iI7X/jpAX9fFequ2XHjJ+Ids3s2cfkjkdcOUfZglL6LX2RlvK4XIb5XnGGYBTD/Mkw+TNMMs4cnjXCRxnWKsqGxI2yKA+ZS6kYv00dIiF5qtGjUBtNeuPfpoSMSO3+f226W11ipXrlO7brKJdM017ePZq+eeeabcsQdgbwQQAfSGaWcBtKOX7JJ5SktqaS2QzSQ+RzvpX3WKLtTWOkO7aA+dolfqvlb38Qf0Y83XCq15xda1/NwzC7XE5Jwe/aJMRtZDPavbJ08mkxSryEx6WyWkfC+lFQEc/+OkkyTGnxVJHAI4MarrzACGClvevYcAhv1gtyVXbMbt4otLzz0X8k3W3uf446UDDwzfP/7Ot278rDjaYAPphhvax7O62MTGIdsQQATQG9LFAph+60f6i9RucrvBOmzx2oLxLk9qaR2mI7WPTtb3tIc+pe9obd3eeZkZewm43ewhX9Dxl0OWwCbLd8qaZJ2nikyCt8XSwpG81l4eu1PmtZvzVi2wRccr+iFTJCNFnHstgGWOH7dH0Q+nsllzr0Sl46SIeej9VrRf0ech8YsAtinlfQ/EDONxlbEIhrC1fXbYQfr5z0P3bu9nbWIThOzZkN6S73ZebLFyMlquFn3bGwFEAL3BVyyA6TPkdYul//355yW78XI2k8Iz9JHWf7P1fk3WAp2tnfVWXapT9HEdqq/oBq2r9XSzJmhf1hd63sOsUzdeWQEsegh7W6NM+SLhKXOsTm2cNW6pinNXcYxkvYuOhwBOjAgEMPwuQQDDBDAmuvLK0oMPhvNNjtkLLWVtYs8YS1Kkt7XXlm69tf2vCGAo0YHbj1nAvibrPAs469h5D9oiOSrIHvyLztbeOlm/1rv1VX2udeYP6wydqY+2/nyXVtSaeqRdI68AdqpLpy7gomv0tUW50kXCU+5o4/emC3j8wy6mExIbcXyGSHW8T0hcFe2TlZkhA9jdXVCUMSz6PHnWQR4DaDEVx1DRD8E0aVsn8qabwvn/0z9Jd94Zvn98n1nvkyUb0lvyeEMogMwCjr6iy0UMe6cIlM8A2gGKHkZF4ph+oEbHs6zgddpQm+ivrT0+qW/rFO3d+rPlCk/XR7WFLtblemvn7uJOzVz05R3ykG/COEEEsFhak3tUlQHMi/2i9uj0ecj9VLTPMApg6Nd10T0depzk91LRj8SCH7QvnXKQBfAjH5HOPLN9KWUFsOxyMMsvLz32WLmWsjrZEi9Z4/uSs5Gtm/gvf5G23VZ64IH2OS66SNpwQ8kmrQzoRhcwXcDe0O1OALs5a/oBlZSovG7aSZO0QJP1jCbrQa2kO/UqTdf/ts7+al2vW7VR+ZqEPCw61S35ZVj+7NWVKBIOz5k6yVLZbsO8enjrH5dPj1HNa59hEsAsGUQAPRE/vmzRd0TR58mjDbIA2r1l8tSNANrbT+wtKKGbva7tqadC9x6rk5V7+umJ5VZbTfr739v/buMSbY3b88+XDjlEOvpoafXVpbPOkt72tnLnbNDeCCAC6A3H+gSwU03zBDAuE33eWkxm4cIJyxPajONKxwI0XQA7CPO4L+tuo2OQBDCrmzOdoc3iVRRzxq5TuU7nyMoQV5kBRADLCVvZ+6BI8PI+z5KeKgUwfR1F9cy77qKhAXnCl7wnis5d9pVwZWcNx3Vcdtns9/y+4hVj2T4TwE03lf7v/9oZQxNN+/+bby6/9EzZWOrh/gggAugNr2YIYNFVpB6e6efyNdq49UaS4/U53aK1tY7ag39dUlj0JdnPbmAEsPPyRAjgWIYkS2Tj+61sNje027Pofk6eP71vN+cokpHQ+uRxCRWvrHo0UQDtet70JumKK/LJFHX5FjEv+jyr3ct+p9r+9tq3+fMnXsdKK0kPPdT+dxPANdeU7rij/fe4e9hE0N4TPKAbAuh8xg9ou1dZ7YEUwCQAe16Y9N2gDfR+zdZe+i/trh9qnpbV9vpNa43BZ55eqCWXKqmDCGBYnJX90o6PWrYLOKuLM1RkmpwBDOUQ2sXbSTiKRKaoLbuRs05RVJQpDYvA7ElhoWWz9iuSl7zPB0kAi/iEfP9VNU6yqC55n1sdV1wxe+Hp5ILUVk/7+yOPtJebOe+8tviVHXPYbT17VA4BRAC9oTVYApj3gJo0qSV69qq5z+to7adv6Sztog/rR7I1B9fRbdpdp+p72lOLKWPJgG4oFj0sQ4+Z1Z1XVLbqDGC6DmUe9N1yCBWftDAWPWRt/zoygOnzFF1P3udF5Tpdf9ZDGgHMvns+/nHpBz8ourPanyOAEyd9FP14KPt5WEt03qtTBnDaNOkf/2iXt3ssHmNo4/9sHOD6648tFF1FXfpwDAQQAfSG3dAIYBKEyeBF2lK3am0drC/rYa2sdXWjrtdGekCv0NJ6XCvocR+7rC6S+MsmS0LyztYkAQx5+GV90XdDMlR8BkEAy44nTApqFofQbCcCGCZsZeO6TgG0majxGyvK3kdF9Sx7vHFfovYtmjEONt6n6NxFn3vqlqyDZfIWLJh4tBVWkObOHbuGeL3A3/1OshnK9t8FF1RRi74dAwFEAL3BV34dQO8Ze1E+J2Nlub7FtLD1g36zSX/QH7VtK0u4lf7QWm/wdq2l1+la3anV9U+6p3zNOmVbQjNjybrb+yrtvZVFW1GXZui503JV9kFZRnTT19StAGaxST9s6s4A9ksAbWyTzdRMswzN6OZxy/tRUiYzXBTDebHTzTlCZCNkH6/cFGWns340dnO9IWy9+5TpAt5uu/b31q9+NXbWMry7raud4xOfaGd2bTyjTfSIeU6ZMjY20P7N/rMZzTZe0KTx/e8v/+aRbuvZg3KsAxi5fQ/YjtIhByMDWNQiyS/R9CvjEjKwMNpvST2tjfRXPaSVdIy+oH10kq7QZlpXN8s6k+/S6lpT95SbRJKXEUzWvVNmJ/nwySuTJWxZ0tZPAQzNaFYpgOn4aJoA5j3kQwSsSPazrt0jgCFd1UX3Y+jnWXHajRCFyEbIPgjgeIGzvyXbw6Tp3HPb+xQNNSjDu1O8fO1r0gEHZO+RFT/xq+GWW056PNHLY9dh4mfvKTZZtWv5yU9CI7WR+5EBJAPoDczhEMA0hYKHWHvE4PjtAm2tjXWdVtSj2knn6Bx9sLXXw1pRK8VvICmiHZqJ6vTlmCcvnX6RF53X6m3ZxSWXzH5xejcP3TLCmhbXkPp2atOQdsiS6E4PeMsM2EOjTLm8Nkm2YRm2IRmiogdr0YM5eX1Z7ZC+d+yVWrfdVkR84udF9YwlwtPOocKWFpaiqymqe97n6X/fbz/phBMmLpyf9UPwfe+TfvGLoprV93kci8ku1nRs/eY30rvelT1esIhh6JVkHafTD9y4q9eWh3niifZyL/bdZ1s8EcRmCL/jHdKPfhRai0buhwAigN7AHC0B7EDruUmL606tqdfoVi2hBZquOTpP/6wXtJi+rT20p05tCeG9WlmrK+cdl3lfVrZSvX1mAla0dcpehT54OmVWqsq6pEUiS/JCH+6dvtDTxy3ilyUWZbJoZeStagFMX1tePHWSyiKp68TPUzak7ln7hMZIaL1D75HQ42XtF3qOLKG342UJYOgxQ+K/rPDmXaP9++GHjy3onPXjIlk2byhCaJ1DWXf6vojfDBJP+rAxltdf3z6yvR7OloK57DJp1VXby8EM8IYAIoDe8B1OAezmC3DSJN2hNbWWJr6P8m26SBdrK92uV+nVuk1v0aWtDOGtWkdv0NVaVk9qrpbXOdpRe+j749uk7C/hTgIS+pAokrxOktlNRGXVuagOeQKZJ5LdZNLy5LFTm8T1tveL2uK0RaKQfKDH+5Zt87xzdHMcj8R1cz5v3auIxbSYZMVKmWsr2rfsfZjXJZ+ut2WmHn7Y9/1RVQzm3dOdhnhUIYDpBaHzfgTlxZ1l8S3zZ5lLe0PI//yPtNNO7b3f+Ebpz3/u5huukWUQQATQG5jDK4BlyUyaJHujyKKy942M7yk9eNJR+rIO02/1Dm2n37U+/6B+oiN0hFbVfVpB7XdY/qdm6tOa9dLYQcsYllx9cKzWZbI/WQ/99PV7H4plRagbAYzraBnTeDJMOosS0q5VSEWZ85TJMIYc17MPAjjxDS7xDR36IyJEALN+XOTdh6EC6Pkx4Wn3rHjrRgCT9S9imBfjq6wi3X9//vfgrrtKp5+ef4e8/OXt9f6yRNi62eMxjJ57rCFlEUDH87UhbdjvaiCA6S/yInGJXkW3i07TmdqtJXjv1G+0v07QfE3Vv+jHekLLaqrm69earuX1D22i67SUxi9VME4OFy6UTVCpTBbT12C/hK1LJL11+yWd98AomtgROhkieXyPAIY88D0M8urmOab3G8EjAlXWu+yxumnnrId8lrh4BbDTD6x0zG+9tXThhe2apa8p6+9FE3ZCOXravUkCaLN6vx/1otiEDsvEd8o6puv+qldJd0a9ODYe8Lnn2t999h24zz7SSSd577DGlEcAEUBvMCKAIQKYRTnxxT9ZT+tGraeXaa7magVdoU31IZ2jy7SZltbT+oY+o+N1oKapvS7V3Vpdn9HX9T/6YOvvNinlrbpIF2lrLZIxQWWSntbzWlqLZnz20oOmlxm+kCjr9AAqeoilZ257RTVUJvJkIeR6sx7wWUISeqwq9rMxpvagCxHfEIn31KmozbPauHUzdP0zaLx0pY8VWp8NNmiPGevUnZmOm3/7N2nWrHxavc4A2jIou+/euc5Wu/g+sxgpegVavzKAJm82Vs+2bbeVbKLJzJnt2bshC3lbN+9VV42PBetRsMlM114rLb+8J6obVRYBRAC9AYkAdiuA0QPGOoxNzGx9Qdue0DK6R6/U+rr5pbeTbKyrdL7eqyl6vJUZ/I720F76rh7VNN2rV2oTXavF9Ywe1xRN1rMT2tQE8Qq9UZsq+mLztHrogzDkHJ2OVdd58uoZcv6QbFEnOQ05RwjHqvYJFV8Ps9C69otNnriEyqXJgo1DCxXAUB7J/fImgWTVMckx+f7e5L9ntbv9W6d4KBLtbgXwn/9ZOv/89rmvuUZ63evKEcq6rjJHeO97x9YjTF5DmWMMyL4IIALoDdXhWAjaS6Hb8llf5FnHSj0Mn9WiWmLh83ps0vK6S2u0FqO27Sq9Xq/RLVpWc/WiJreUch+dqJM1U8fqIB2k41r7mRA+pSW1mJ7XQi2iBVpMU/R02FV4JSF5ln495EOuNKRuwyqAobITwrHMPpZZMnmyBart3iiSjDLHjvctalevABaJj9XDKxbdCmBajjp1HxfVsahtijgUHT9ur6Lz2MSXhx4aiwSvAO6/v/T1r7ePF1rHbuKwz2VYCLrdAM7+gj63Yv9PTwbQ0wZ5Aph+SGU9tDK+6K6ZtHGrC9jWI/yofqjTtHtrDOE8raDpOl+/1j/rKB2qw3SUNtNl2kjXtuTwp/qAPqyzw64kTwDzsgidZKJKmUx+YRc9NEKutEgU0g+ITuMXuxm7GFLHqvfJE9r0ebzt1ilWkufKmhwTKqd57VfUrp0E0M49Y4b0y1/mk0+W/9jH2t2ONhataHxrmbZsggBmtUPy7Rlpjjvv3P26eXnZVFuy5a9/ldZZpzoBtMWd/+Vfhl4A7QLJACKAZb52svZFAL0Esx54WQKY/sLN+qUbfVFahm99/VV/1FujGcaTtLru1N16lbbQJfqT3to666q6V3/X6jpP79ZvtZ1O1F56SlO0QEvpJq2nTXVlqxu69Stp/nxNmjJ+oeO427p1sJCHuueBHsK5rJh02j9QukOq1VUmq2hcY9CJS+40rALYKfOVF5PJ+y2dCer0AyMva9QLAUw2b6cfZnnC6MkAxueOjxFPmEgfs2QIZu4eHzM9vtLW4bvrruJXGZapgy0BEy/oPsQZQASwHRRkAMvcHBP3RQB9/DqXzvoy7fTrP+NoyefOf+uj2l3f0cKFS7ec5GP6rn6oPVvL13xOx+lrOtBGereOspt+qC/rED2iFVsZxay3n9h+L2gRPa0ltayeantgdEt9Sifre9prYi9K1oMw7xe+vafWsifxFr/ruOhhmne8p54aP5u5HwLYKQOVJyrdxFhIFjl93LICmJaAkHrmnSPrYev5wWDHSzIIzZoVZQCTYhhf7957Syef3P5bHQL41a9Kn/vceNp5ktpJYqoQwHi9S/v/ZKazSnnKq+cuu0hnnVWtAMbtG78nOySmB3QfMoAIoDd0EUAvQU95GzNl690lJanM8RIPWJs6MjkxS3gxPa1r9AZtoJv0BR2lY3Ro5pEv0pZ6UCvrQ/pp63Nbuma6/rf1NpTnNHlCme/qE9pDP9CSelJPv7h0a7jXFXq9NtVf9LQW0VILX8h/00HyaJ0yDVmCaO/1tF/2naS6iF23D7Ui6Y3Pmz5+vCCtZVeKNiubXK6nKQKYl81OilQR106iEktX0Y+CTrGTVzYvxrLENCTzFbJPUTvnSXeVAljUHp3qWOU1xuexV7D94x8TexluvFFab73qBdBmON96q7TaaqGtMZD7IYAIoDdwEUAvwX6X7/TgnDRJM3SOfqkdc5Pl/6Ev6PW6Su/R+a0r+ab21b/rxNyMoWUYt9JFulBbvXTM3fV9/UCf1Ib6q27QhtpNP9CP9SEt0DKt7GTLFaKH0ks+UVYAQx5aRW3R7YOxWwFM1yctHsnP09nMbgTQup3jc4ScK0tG4nciZ2XcYllLlyvi2msBTL5Bo5sMYDpzmxdHVctRGektiu0yQh5yrF7uY2vzmRTOmzdxslBoW/SyfgNybAQQAfSGKgLoJdjv8vFDJPnwT9VpwYKx1xBPeIZZ189ii73kDa/U9bpHG7V+rXdyiOQpNtelulhv1zJ6Qs9oqXFnt7ejrK57tYPOHffvcVfzMwsWtt7XnnSKO7W61tC9bXUskovEg+96raeNdGNLOjPHhoQcq9PDP1ne/mzpzyyJKxKI5HWVEeFupC4tlllMOwlupwxgaOyHdFV65apTZjgvuxbXPzQuvHVM86paAHfbTTrttLB7JrTt6tgvzaFqznVcQx/OgQAigN6wQwC9BPtdPusB36FOZb9bbdieCVreM7ztB5N0pV6rN+ovksZL0fE6QFdpk9ZbU0K3vXSyjtXnX3rFXqdyrcuPKneovqSj9UVJz+sFLdFeVDv04R5auaz9QqEWiVaRIGQJYN71mdjbj4L4lXoJUW79e/JYRfUv+ryIXR0CWKZd0hxDY8TLoah9izgO6+frrivdckv76qwtdthB2mKLieMkh/X6u7wuBBAB7DJ0XiqGAHoJUn4cgbxnZGg20Q72e22l1+maIAFsucwi7XzfB/VjnaMPtf68n07QNXqtLtB2heP6Q5//7qb2CqBVIPme5LIVyssWF4lN0edF9eiXAHaql/eaiq455POqM4Ah52zqPsbi6qvLLxzd1OupoV4IIALoDTME0EuQ8qUI2PAf+67PmhdhItbqWV00GjeY9+q71BlX1126W/+kHfQznasdgurTbc9mp7koduL0xOcgOw6qcQ936rUMdUof9/CyOh6619cccl1xHY4/XjrggJAS7AOBlwgggAig93ZAAL0EKV8LARM2WwVmmWWyXjDxohZq0dYi2V9ceFSrPmUyjp4LiEUySCg7SEfyozJLCKbP29W19FqGEMDsZrFfQ9ZF32H8blftSaGRIIAAIoDeQEcAvQQp3xcCacHbRyfpUm2haxaOf/doXSKYByE5RPMxTW29Czpek7Fo+GZe17RdU9ak4a4bYhQF0MZIWro2OQOpa4AUhED9BBBABNAbdbwL2EuQ8o0gYA7TKZEST2axyiYn8OZNkk2Loy3RZ/suvXT3l/uEltEyemqcABYJarJLOSTbFzKe0a5lySXry5J2nEHUPU5KQmBkCfAu4HbT8yYQ3y1ABtDHj9JDTMDky8YqJqUqK1lWJHExonjpm6y3soTIXVnUnTKIsQinj9mLekyod68zjmVBsT8EBpAAGUAE0Bu2CKCXIOUhEBGIvcaGdi2++BiWl0Rs0iQdocN1pI6YwKwX4tVxQkqiBnlZ0DING79RLE8syxyLfSEAgWICCCACWBwlnfdAAL0EKQ+BEgSysoVZ4/nSK4SElCtRjaBd0xlEe2vXzTcHFW1lTdNSa9dgomjrZ4dmTcPOxl4QGD0CCCAC6I16BNBLkPIQ6BGBZE9ppwxhvF/oEoG9yDZ6EGy6qXTFFWNHeOMbpT//GUn0MKXs8BNAABFAb5QjgF6ClIdAwwlMmybNnduuZMi4wHjsY52XlSWlb3+7dNFF2fUOHUYYvemwzkvhXBCohQACiAB6Aw0B9BKkPASGnEA8gzo5kzopk8lXIq+zjvS3v41JW9VdvVmimCW1yZneVhuW2hvyIB3By0MAEUBv2COAXoKUhwAEcgmEjGVMFn7nO6Xf/a43QHfdVTr99Pyu5ZAldHpTM44KgfIEEEAEsHzUjC+BAHoJUh4CEOhIoKi7Nv58m22kCy4YL2j9GK8Yi2DWepHJC23iC04IxdEhgAAigN5oRwC9BCkPAQhUTmCzzaT/+7/xh01nE+3TqruYQy4kLaX290MPlY4+ul26TCZx7bWl224rVyakjuwz/AQQQATQG+UIoJcg5SEAgcYRyMvO1VnR9ALi6XGIaXnNWjqnzvpyrsEigAAigN6IRQC9BCkPAQgMBIGsbGEsZXVlErMW3c6Cl/X2maeear/CLznppky2cSAaiUoGE0AAEcDgYMnZEQH0EqQ8BCAwVATy1l/MytilLzxEJEPGNWYJYB5k29fkMP2e6vjtLEjiUIXnSxeDACKA3shGAL0EKQ8BCAwVgbxJKyusID32WPtSQ6TKJrX84Q/FaOwNKzfdNHG/EFFMl8oTxzKv+7PrtyV/lliiuO7s0T8CCCAC6I0+BNBLkPIQgAAEcgistpq0xx7Sl740cYfjjpM+85nx742ePFmyN7rkbdZlnewCztovVBxtv5kzpVmzxo6y1lrSHXe0/24Lgj/xhDTFnhJsjSOAACKA3qBEAL0EKQ8BCEAgkEDR0jLxYbK6kj/0IenHV9oJfQAAIABJREFUP5Y+9rH2eoZZ3b6B1ZiwW6exifbZhRdKW289lvl87rnx4pp3XnuTi73Rha16AgggAuiNKgTQS5DyEIAABHpEYJddpLPPlo4/XjrggIknSWb7FlusnbXrx2b1sLfA3Hpr++yx6CbXVOxHvYb5nAggAuiN75YAzpw5UzNmzND06dO9x6M8BCAAAQj0iUD6FXg2EWTRRduVid+LbN3I8b+9733SL37hr6w9OubMyT/OD38o7bZb+/PvfEf61Kcm7huPmQwZX+mv8WAfYc6cOZo9e7Zmtfvvp0qaP9hX1F3tJ3VXjFIRATKAhAIEIACBESNQtE7ifvtJJ5wgLbWUtGBB7+CYfM6YIb3jHdLvf98+TzfvbY7HTY7SxBUygGQAvXcmAuglSHkIQAACQ0zg3HOl979/7K0r06ZJ9s7mn/xk4kXb21vsLS62XXWV9PrXd/+2lqxM4CqrSI8+OnGiTCy0jz8uLbectM8+0kknDXGjWMpv/nxNnWrJPzKAw93Svbs6BLB3bDkyBCAAgaEhYBM/TLRsrGFye8MbpKuvbovX/PnSk09Kp54q7btve69YzmzyymmnlcORN6M57rp+29ukP/6xnTWMz5UUxy23lGwiyjXXSNtuKz34YPEs6nI17N/eCCAZQG/0IYBegpSHAAQgAIFgAiaEliW0cYh/+lO7mC1/Y2sP9mJ73evaYnr77dK3vy3tuWd7ZrItw/OWt/TijPUcEwFEAL2RhgB6CVIeAhCAAAS6IpA3FvHuu6U11ph4yLyMoP372mtLt93WuRqrripdcIG08cbSww8P9hqHCCAC2NVNlyiEAHoJUh4CEIAABLoiYLOBbcLJBhu0i6cXuo4F0Ya6xW9hsa5oyx6uv750yy3tcnG379FHS1/5imRjAXfYQfr5zydWa8cdpXnzpN/9rqsqN6YQAogAeoMRAfQSpDwEIAABCPSEQN5r+eKT2ecmkba0TLzZa/U23LA9e/kDH5DmzpUuuaQ90/jiiyUTyIMPlo45pidVru2gCCAC6A02BNBLkPIQgAAEINATAkUCGHpSG/NnS84suWT7DSo2k3nQNwQQAfTGMALoJUh5CEAAAhDoCYHQ9xr35OQNPygCiAB6QxQB9BKkPAQgAAEIQKBmAgggAugNOQTQS5DyEIAABCAAgZoJIIAIoDfkEEAvQcpDAAIQgAAEaiaAACKA3pBDAL0EKQ8BCEAAAhComQACiAB6Qw4B9BKkPAQgAAEIQKBmAgggAugNOQTQS5DyEIAABCAAgZoJIIAIoDfkEEAvQcpDAAIQgAAEaiaAACKA3pBDAL0EKQ8BCEAAAhComQACiAB6Qw4B9BKkPAQgAAEIQKBmAghgMwVwGUkvSFpQczx0czoEsBtqlIEABCAAAQj0kQAC2AwB/A9JsyVdLmnb6M8vStpB0m/6GB8hp0YAQyixDwQgAAEIQKBBBBDAZgjg3ZI2lvSYpD9IOlfSE5L2kPTmBsVLVlUQwIY3ENWDAAQgAAEIpAkggM0QwHmSpkpaUtKDklaU9JykuZJW6FPYTpJ0oaT1Jf2XpMNy6oEA9qmBOC0EIAABCECgWwIIYDME8B5Jb5H0Wkmfl7R1QgZNDPu1rSbpnZLWRgD71QScFwIQgAAEIFA9AQSwGQL4JUmfkDRZ0uck/UDSlpJOkPSm6pu91BF3l/RqBLAUM3aGAAQgAAEINJoAAtgMAbQgsckfz0q6KIoYE79lJf2+zxGEAPa5ATg9BCAAAQhAoGoCCGBzBDDZttblasvA3FGiwXeWNFPSJpE4Li7JZhIntyOjiSU2bu/KaP/rox32lmTHsIkoOyYKIYAlGoFdIQABCEAAAoNAAAFshgCeKsn+u0TSLpLOkLRQ0scknRUYSNtJmiZpaUnfk5QWwM9K+rSk7SXdJulwSbtJeo2kpzqcwwTQhPSLOfswCSSwgdgNAhCAAAQg0BQCCGAzBPD+aJydiZitBXi8pPmSvhotD1MmXraSdEGGAN4ejSk8MTrYopLsvPtFwpl1jjOjiSkmlSaNJo+WmUxuCGCZ1mFfCEAAAhCAQAMIIIDNEMB4GZjlJNmagC+Lum+tO3b5knGSJYAmaXaszSPBjA85R9J1kg4seY4JAjhz5kwtscQSrX+fPn166z82CEAAAhCAAASaQ2DOnDmy/2x79tlnNWvWLPujrTZiSaeR22y9u35vt0qaIWkjSZ+MMm32Orh7u1gHMEsAXxmJpa3pd3PiYn8UNfqeDgBkAB3wKAoBCEAAAhDoBwEygM3IANrYvK9EAWBjAO21cNbd+gVJbysZGH3JAM6bN09TppgLskEAAhCAAAQg0HQCCGAzBNDixCZaPC/pzihobHKG9an+tWQQlRkDeJ+k/TuMAQw5NRnAEErsAwEIQAACEGgQAQSwOQJoYbGSpDWi7tqHSsbJItHEDxPA8yXZeEKbsGFrC9qMYhvnZ5nG90iyCSGHRrOA1y2YBVxUjZYA2hjAGTNmMPaviBafQwACEIAABPpMwMYBzp49mzGAfW4HO71J1H9Len9UFxO2X0j6uMlVYP1suZbvR7JnRWxsox1nm8Ti0kdI2iuSwz+n1gEMPM2E3cgAdkuOchCAAAQgAIE+ESAD2IwM4MmS1pP0GUk2IcS6g+01cLdIsgWam7whgE1uHeoGAQhAAAIQyCCAADZDAG3pF3v124OJNnqFpCskrd7wyEUAG95AVA8CEIAABCCQJoAANkMAH5ZkS7U8k2igJaNlYFZseNgigA1vIKoHAQhAAAIQQAAnxkAT1gE8L5rt+/loAWib0HGMpNdJenfDw5ZJIA1vIKoHAQhAAAIQSBJgEkibRhME0BZo/k00i/euaCawLQlj7/e9seFhSwaw4Q1E9SAAAQhAAAJkAJuZAbRaLSvpvdGYv3sk/UrS4wMQsgjgADQSVYQABCAAAQgkCTAGsH8ZwC8FhuJhgfv1azcEsF/kOS8EIAABCECgSwIIYP8E8PcBbWbr+L0jYL9+7oIA9pM+54YABCAAAQh0QQAB7J8AdtFcjSyCADayWagUBCAAAQhAIJ8AAogAeu8PZgF7CVIeAhCAAAQgUCMBZgG3YTdhFnCNzV75qcgAVo6UA0IAAhCAAAR6S4AMIALojTAE0EuQ8hCAAAQgAIGaCSCACKA35BBAL0HKQwACEIAABGomgAAigN6QQwC9BCkPAQhAAAIQqJkAAogAekMOAfQSpDwEIAABCECgZgIIIALoDTlmAXsJUh4CEIAABCBQIwFmAbdhMwvYF3RkAH38KA0BCEAAAhConQAZQATQG3QIoJcg5SEAAQhAAAI1E0AAEUBvyCGAXoKUhwAEIAABCNRMAAFEAL0hhwB6CVIeAhCAAAQgUDMBBBAB9IYcAuglSHkIQAACEIBAzQQQQATQG3IIoJcg5SEAAQhAAAI1E0AAEUBvyCGAXoKUhwAEIAABCNRMAAFEAL0hxzqAXoKUhwAEIAABCNRIgHUA27BZB9AXdGQAffwoDQEIQAACEKidABlABNAbdAiglyDlIQABCEAAAjUTQAARQG/IIYBegpSHAAQgAAEI1EwAAUQAvSGHAHoJUh4CEIAABCBQMwEEEAH0hhwC6CVIeQhAAAIQgEDNBBBABNAbcgiglyDlIQABCEAAAjUTQAARQG/IIYBegpSHAAQgAAEI1EwAAUQAvSGHAHoJUh4CEIAABCBQMwEEEAH0hhwLQXsJUh4CEIAABCBQIwEWgm7DZiFoX9CRAfTxozQEIAABCECgdgJkABFAb9AhgF6ClIcABCAAAQjUTAABRAC9IYcAeglSHgIQgAAEIFAzAQQQAfSGHALoJUh5CEAAAhCAQM0EEEAE0BtyCKCXIOUhAAEIQAACNRNAABFAb8ghgF6ClIcABCAAAQjUTAABRAC9IYcAeglSHgIQgAAEIFAzAQQQAfSGHALoJUh5CEAAAhCAQM0EEEAE0BtyCKCXIOUhAAEIQAACNRNAABFAb8ghgF6ClIcABCAAAQjUTAABRAC9IYcAeglSHgIQgAAEIFAzAQQQAfSGHO8C9hKkPAQgAAEIQKBGArwLuA2bdwH7go4MoI8fpSEAAQhAAAK1EyADiAB6gw4B9BKkPAQgAAEIQKBmAgggAugNOQTQS5DyEIAABCAAgZoJIIAIoDfkEEAvQcpDAAIQgAAEaiaAACKA3pBDAL0EKQ8BCEAAAhComQACiAB6Qw4B9BKkPAQgAAEIQKBmAgggAugNOQTQS5DyEIAABCAAgZoJIIAIoDfkEEAvQcpDAAIQgAAEaiaAACKA3pBDAL0EKQ8BCEAAAhComQACiAB6Qw4B9BKkPAQgAAEIQKBmAgggAugNOQTQS5DyEIAABCAAgZoJIIAIoDfkEEAvQcpDAAIQgAAEaiaAACKA3pBDAL0EKQ8BCEAAAhComQACiAB6Qw4B9BKkPAQgAAEIQKBmAgggAugNOQTQS5DyEIAABCAAgZoJIIAIoDfkEEAvQcpDAAIQgAAEaiaAACKA3pBrCeDMmTM1Y8YMTZ8+3Xs8ykMAAhCAAAQg0EMCc+bM0ezZszVr1iw7y1RJ83t4usYeelJjazYYFSMDOBjtRC0hAAEIQAACLxEgA0gG0Hs7IIBegpSHAAQgAAEI1EwAAUQAvSGHAHoJUh4CEIAABCBQMwEEEAH0hhwC6CVIeQhAAAIQgEDNBBBABNAbcgiglyDlIQABCEAAAjUTQAARQG/IIYBegpSHAAQgAAEI1EwAAUQAvSGHAHoJUh4CEIAABCBQMwEEEAH0hhwC6CVIeQhAAAIQgEDNBBBABNAbcgiglyDlIQABCEAAAjUTQAARQG/IIYBegpSHAAQgAAEI1EwAAUQAvSGHAHoJUh4CEIAABCBQMwEEEAH0hhwC6CVIeQhAAAIQgEDNBBBABNAbcgiglyDlIQABCEAAAjUTQAARQG/IIYBegpSHAAQgAAEI1EwAAUQAvSGHAHoJUh4CEIAABCBQMwEEEAH0hhwC6CVIeQhAAAIQgEDNBBBABNAbcgiglyDlIQABCEAAAjUTQAARQG/IIYBegpSHAAQgAAEI1EwAAUQAvSGHAHoJUh4CEIAABCBQMwEEEAH0hhwC6CVIeQhAAAIQgEDNBBBABNAbcgiglyDlIQABCEAAAjUTQAARwE4h9wZJ35L0vKRFJR0g6f9SBRDAmm9aTgcBCEAAAhDwEkAAEcBOMbSypCclPSFpQ0mnSnozAui97SgPAQhAAAIQ6C8BBBABDI3A10j6gaQtEMBQZOwHAQhAAAIQaCYBBBABDInMxSX9StLXJZ2PAIYgYx8IQAACEIBAcwkggMMlgDtLmilpE0nLSjJxezEVfkdK2kOSjd27Mtr/+mifvSXZMR6TtGP0b4tIOkvSnKgLOB3NjAFs7v1NzSAAAQhAAAKZBBDA4RLA7SRNk7S0pO9lCOBnJX1a0vaSbpN0uKTdJFn37lMZETJJ0mmRKFr2L2tDAPlygQAEIAABCAwYAQRwuAQwDr+tJF2QIYC3SzpB0onRjjaz935J+0k6IyN2LRt4SjTz12QwmRmMd0cAB+ymp7oQgAAEIAABBHB0BNBEzQRuc0mXJ0Lfunavk3Rgl7dDSwBnzpypJZZYonWI6dOnt/5jgwAEIAABCECgOQTmzJkj+8+2Z599VrNmzbI/TpU0vzm1rK8mltkati0rA/hKSXdLWl/SzYkL/lHU8Ht2CYEMYJfgKAYBCEAAAhDoFwEygGQAK8kAzps3T1OmmAuyQQACEIAABCDQdAII4OgIoMVi1hjA+yTtnzMGMCR+yQCGUGIfCEAAAhCAQIMIIIDDJYC2ZIst/WJdwLZe33KSXrCufkkLo3F+Ngv4PZEMHhrNAl43ZxZwSKi+NAZwxowZjP0LIcY+EIAABCAAgT4SsHGAs2fPZgxgH9ug6lPvLun7kezZsW18o4nfNpIuik52hKS9Ijn8c2odwG7qQwawG2qUgQAEIAABCPSRABnA4coA9iOUEMB+UOecEIAABCAAAQcBBBABdIRPqygC6CVIeQhAAAIQgEDNBBBABNAbcgiglyDlIQABCEAAAjUTQAARQG/IMQnES5DyEIAABCAAgRoJMAmkDXsYF4KuMYzoAq4TNueCAAQgAAEIVEGADCAC6I0juoC9BCkPAQhAAAIQqJkAAogAekMOAfQSpDwEIAABCECgZgIIIALoDTkE0EuQ8hCAAAQgAIGaCSCACKA35BBAL0HKQwACEIAABGomgAAigN6QYxawlyDlIQABCEAAAjUSYBZwGzazgH1BRwbQx4/SEIAABCAAgdoJkAFEAL1BhwB6CVIeAhCAAAQgUDMBBBAB9IYcAuglSHkIQAACEIBAzQQQQATQG3IIoJcg5SEAAQhAAAI1E0AAEUBvyCGAXoKUhwAEIAABCNRMAAFEAL0hxyxgL0HKQwACEIAABGokwCzgNmxmAfuCjgygjx+lIQABCEAAArUTIAOIAHqDDgH0EqQ8BCAAAQhAoGYCCCAC6A05BNBLkPIQgAAEIACBmgkggAigN+QQQC9BykMAAhCAAARqJoAAIoDekEMAvQQpDwEIQAACEKiZAAKIAHpDDgH0EqQ8BCAAAQhAoGYCCCAC6A05BNBLkPIQgAAEIACBmgkggAigN+RYB9BLkPIQgAAEIACBGgmwDmAbNusA+oKODKCPH6UhAAEIQAACtRMgA4gAeoMOAfQSpDwEIAABCECgZgIIIALoDTkE0EuQ8hCAAAQgAIGaCSCACKA35BBAL0HKQwACEIAABGomgAAigN6QQwC9BCkPAQhAAAIQqJkAAogAekMOAfQSpDwEIAABCECgZgIIIALoDTkE0EuQ8hCAAAQgAIGaCSCACKA35BBAL0HKQwACEIAABGomgAAigN6QYyFoL0HKQwACEIAABGokwELQbdgsBO0LOjKAPn6UhgAEIAABCNROgAwgAugNOgTQS5DyEIAABCAAgZoJIIAIoDfkEEAvQcpDAAIQgAAEaiaAACKA3pBDAL0EKQ8BCEAAAhComQACiAB6Qw4B9BKkPAQgAAEIQKBmAgggAugNOQTQS5DyEIAABCAAgZoJIIAIoDfkEEAvQcpDAAIQgAAEaiaAACKA3pBDAL0EKQ8BCEAAAhComQACiAB6Qw4B9BKkPAQgAAEIQKBmAgggAugNOQTQS5DyEIAABCAAgZoJIIAIoDfkEEAvQcpDAAIQgAAEaiaAACKA3pDjXcBegpSHAAQgAAEI1EiAdwG3YfMuYF/QkQH08aM0BCAAAQhAoHYCZAARQG/QIYBegpSHAAQgAAEI1EwAAUQAvSGHAHoJUh4CEIAABCBQMwEEEAH0hhwC6CVIeQhAAAIQgEDNBBBABNAbcgiglyDlIQABCEAAAjUTQAARQG/IIYBegpSHAAQgAAEI1EwAAUQAvSGHAHoJUh4CEIAABCBQMwEEEAH0hhwC6CVIeQhAAAIQgEDNBBBABNAbcgiglyDlIQABCEAAAjUTQAARQG/IIYBegpSHAAQgAAEI1EwAAUQAvSGHAHoJUh4CEIAABCBQMwEEEAH0hhwC6CVIeQhAAAIQgEDNBBBABNAbcgiglyDlIQABCEAAAjUTQAARQG/IIYBegpSHAAQgAAEI1EwAAUQAvSGHAHoJUh4CEIAABCBQMwEEEAH0hhwC6CVIeQhAAAIQgEDNBBBABNAbci0BnDlzpmbMmKHp06d7j0d5CEAAAhCAAAR6SGDOnDmaPXu2Zs2aZWeZKml+D0/X2ENPamzNBqNiZAAHo52oJQQgAAEIQOAlAmQAyQB6bwcE0EuQ8hCAAAQgAIGaCSCACKA35BBAL0HKQwACEIAABGomgAAigN6QQwC9BCkPAQhAAAIQqJkAAogAekMOAfQSpDwEIAABCECgZgIIIALoDTkE0EuQ8hCAAAQgAIGaCSCACKA35BBAL0HKQwACEIAABGomgAAigN6QQwC9BCkPAQhAAAIQqJkAAogAekMOAfQSpDwEIAABCECgZgIIIALoDTkE0EuQ8hCAAAQgAIGaCSCACKA35BBAL0HKQwACEIAABGomgAAigN6QQwC9BCkPAQhAAAIQqJkAAogAekMOAfQSpDwEIAABCECgZgIIIALoDTkE0EuQ8hCAAAQgAIGaCSCACKA35BBAL0HKQwACEIAABGomgAAigN6QQwC9BCkPAQhAAAIQqJkAAogAekMOAfQSpDwEIAABCECgZgIIIALoDTkE0EuQ8hCAAAQgAIGaCSCACKA35BBAL0HKQwACEIAABGomgAAigN6QQwC9BCkPAQhAAAIQqJkAAogAekMOAfQSpDwEIAABCECgZgIIIAKYF3LLS/qVpGclLSPpm5LOyNgZAaz5puV0EIAABCAAAS8BBBAB7BRDi0h6UZJJ3o2SVkMAvbcc5SEAAQhAAAL9J4AAIoAhUWji9z1J2yOAIbjYBwIQgAAEINBsAgggAtgpQleUdI6kDSQdHElgen+6gCu8x+fMmaPp06dXeMTRPBQcq2t3WMKyOgLVHImYrIYjAjg8ArizpJmSNpG0rKTFo+7bZKQcKWmPqEv3ymj/66Md9pZkx3hM0o6p8DIRtP3t2PZ5ckMAq7kXW0fZf//9dcIJJ1R4xNE8FByra3dYwrI6AtUciZishiMCODwCuJ2kaZKWjjJ1aQH8rKRPR924t0k6XNJukl4j6amMcFoimgBiH9kxr5a0saRnEMBqbr6so/DFVg1bOFbDkR8l1XGEZXUsub+rYYkADo8AxhGxlaQLMjKAt0uy1NKJ0Y6LSrpf0n45s3vfJOl4Sc9LmhzNAv5JRti1MoD33HOPpkyxP7J5CBxyyCE65phjPIegrCQ4VhcGsIRldQSqORIxWQ1HE8DVV1/dDjZV0vxqjjpYR5k0WNUtrG2WAJqZWdft5pIuTxxhjqTrJB1YeNT8HWyCyL2O8hSFAAQgAAEIQKB/BF4p6e/9O33/zjwKAmiNe7ek9SXdnED9o8j693TgN36rSnrccQyKQgACEIAABCBQP4HlJN0naWH9p+7/GUdBAHuZAex/C1IDCEAAAhCAAAQgUJLAKAigIckaA2jWv3/OGMCSGNkdAhCAAAQgAAEIDA6BYRFAe2uHzfy1MYDnS7K07gvRTF5L7do4P5sF/J5IBg+NZgGvmzMLeHBakJpCAAIQgAAEIACBkgSGRQB3l/T9RD++XZeJ3zaSLoqYHCFpr0gO/5xaB7Aktpd277S2YLfHHOZyX44kfE1JT0i6UNLnUhNpbFrWSZHML5B0djRb22Zks+UT+Jmk90vaNpoJb3tuLelrktaT9ICkr0o6GYiZBGyS2H9IshUA7MejrRH6tmhPWwLqPyW9MZpQ9l1Jdu+zTSSwkqRvSHqHJFtOy16jaQvpx9/DxGR21BStZRsSgzyP2mw7sdxMkiWA7D5fStJdkr4u6QepZrF1hS1x9HJJN0XPoIuH7YYfFgHsR7uUXVuwH3Vs2jmPlvTTaPa1ra/4X9GbVl4fVdTi8Zpo4W3L2Nrajr+MhMaW7GHLJmBrWn5Ekq2Haf/ZUkgm2SYx9iVmrzLcQtIvJNmPpXMBOY6Ayd95US+BxedzkexdES0sf4ukUyV9KVo71HoZbJmob8JxAgF7e5Itnm8L6s+NHpz243uNaLkNYjI7aDqtZWsvNyiKQZ5HY1w7sbRXuprU2f3+SPQj2b4PPxZ9P9pRPiTpO5JmSLpMkk0U/Ur0Q3qoZgsjgN1/g5ddW7D7Mw1vSXu7ylWR6M2Lsn7/K+kV0cPDrvx90TjNlyUW5x5eIuWvzGa5XxJlq2y2e5wBPCzKCFrWKt5sLczXRpJY/kzDW8KyU7ZElD1E05sJs33522z/F6MP/13SvpLWGV4kXV/ZXySdEmVM7SDLRKskvDlaiN+y1MRkPt6spcxCYpDn0USmeesCp/e03pM7ox8r9pn9gLaXPxyQ2NGeU/bjxpIYQ7MhgN01JTOLu+OWLmXdv/YavrUSD9Z9oiV74n1XidZosi6Qv1Zz2qE6iq1n+ePooWuCEgvg/0h6UJLxjLcPRw9my9CwtQlYN5At42Rd5dY9+WpJd0iy4QrG0KTZlpCyzEG8WcbQpNsWkLWhDGzjY+xTUUb60Wii3SejNymdRUwWhkqWtBTFoI2B79Vat4UVbvAOIQJoz3Lr4j1I0mnRtfwj+t604Ufx9m1JloT4YIOvt3TVEMDSyFoFerm2YHc1GrxSJir2y2snSb+Jqm9jM2yijj1g423JaKKOjcf64+BdZk9r/G9Rlm96dBYTwHdK+r2k30qyLkwbfxVv7466OWxsFlubgC3mfk8kJhZ7lsGyLJWtE2oPEHt/uGWxTJ7jzcZUWlemjVe11QTYxghYV6+NM7VYs3G79jC17uA/EZNBYZIlLTaEo1MMmgD2aq3boEo3dKciAbSJozYsxv7/XYkMv8WtfRfYj+t4O1bSG6L9Gnq55auFAJZnZiXIAHbHLS713ujXlnVt2A0Yb9a1RgYwjK1lTS0LZV1rJjC2kQEMY5fcK76X7Qv+kMQHv466gexVkGQAw7ja8+RWSX+IMn+WWbV7/YeRTNs72MlKd2ZJBjAs1kL26iSAlvm3BMRi0TCjpxIHJAMYQnfE92Ftwe4CYNfoncw20NayVMnt7ZJsDKB1+9oActsYA5jN2eTZuiXsHZbxDznrorCxlNZ1YYOVd2C8VVCQ/k2Sves7SwBtFutxjAEM4miTtmxgvU3qsslc8XZllFG1hy5jAMsLoE3yKopBnkcTueYJ4PKSfhXFqnXp2qSv5GZjAG3MX/I1sRbDNiSEMYBBXwXDvxNrC5ZvY5vZazMpbXbVpRnFTWRs8K3dfJYNtAeKzdCyjAKzgMcDs65x45Pc7L3UtgSCdalbZuuGaCCzzWB9S5Rt/TizgCdEnsXa56NxftdG8WldwPaDxMYH2SskjaF9+a8dPTxsXBazgCeQs7wkAAAFYUlEQVTexDZO14Zq2AB6Gx9pXWkm1/8s6TZiMvdLs9Nattb9WxSDPI/G0HZiacsU2fejDeH4aLTkU7pRTAptFrAlH2xymA0DsYlg1hPALODyz/2hLdGLtQWHFlbURWm/tp6JLjJer9EG2MdCaOOqbHkY+/Vm6wCeGf0SS/9KG2ZO3V6brV8XLwNjxzCBsTXZbMFz63qzLzHLGrJNJGCDwG3tL5vYYRlBu7dtCSLbNorWprTZq5Zhtfg8CoiZBGwSjS2RY8sOWfe5DU+wGLSZwcRkftAUrWUbEoM8j9p8O7G0iV42FCHu8o3fAWxr/NmPlXiz8dU2SdGWjLFegM9EQ26G6rZnDOBQNScXAwEIQAACEIAABIoJIIDFjNgDAhCAAAQgAAEIDBUBBHCompOLgQAEIAABCEAAAsUEEMBiRuwBAQhAAAIQgAAEhooAAjhUzcnFQAACEIAABCAAgWICCGAxI/aAAAQgAAEIQAACQ0UAARyq5uRiIAABCEAAAhCAQDEBBLCYEXtAAAIQgAAEIACBoSKAAA5Vc3IxEIAABCAAAQhAoJgAAljMiD0gAAEIhBKwtxD8hyR7ow0bBCAAgcYSQAAb2zRUDAIQGEACJoD2mrg1BrDuVBkCEBghAgjgCDU2lwoBCPScAALYc8ScAAIQqIIAAlgFRY4BAQg0icDk6IXv/yJpBUnXRS9z/0v079tKspe//6ukFySdLung6M92HetJOkHSmyQ9Lem86MXw86OLXFLSoZI+JGkVSQ9JOlLSadGL6C0DaN3AdszlJf1G0iclPdEkSNQFAhAYbQII4Gi3P1cPgWEk8ANJq0Yy9qCkvSUdJuk1kvaT9AVJx0g6WtKrJJ0v6XuSvixpWUk3STpT0hcjgTtb0lxJO0awTBjXlrSbpFskvSL6zwTTMoB2rG9FAmgCarJ5RiSJw8iba4IABAaQAAI4gI1GlSEAgVwC0yQ9ImldSX9L7GWidoSkdSIhNEFcGH1ugnhA9NmHJX0jyuy9GH3+OklXRZJnGcOHJb1R0tUZtTAB/K9IJOPyx0naUNJ7aDcIQAACTSGAADalJagHBCBQBYHNJF0m6bHEwex7bnFJX5K0VCRitl+8TZd0riTr2v1s1LWb/Nyygtb9G//b5ZKmSHoyRwDTk0AOl/ROSW+v4gI5BgQgAIEqCCCAVVDkGBCAQFMI2OzbOyStKenejEqZjFnGL50BPDDq1t0lygDa58kM4JWJrKB1K2/aIQOIADYlGqgHBCCQSwABJDggAIFhI3COpMUk7SvpbknLSXqrJBujt5ekQ6IxgDbmz8YA/krSqdG/2b43RhNDrMvYJnHYeMB5qTGAa0n6eGIMoE0GsS7hrFnAZACHLcK4HggMAQEEcAgakUuAAATGEbBZwAdJ2jUat/d41C386UgArTvWJmZ8Kpr5a7N3k7OA15f09SjLZ7OATRDteCaBtlk3sk0qsVnGK0myjKBJnk30QAAJRghAYCAIIIAD0UxUEgIQqIgA2biKQHIYCEBgsAkggIPdftQeAhAoRwABLMeLvSEAgSElgAAOacNyWRCAQCYBBJDAgAAEICAJASQMIAABCEAAAhCAwIgRQABHrMG5XAhAAAIQgAAEIIAAEgMQgAAEIAABCEBgxAgggCPW4FwuBCAAAQhAAAIQQACJAQhAAAIQgAAEIDBiBBDAEWtwLhcCEIAABCAAAQgggMQABCAAAQhAAAIQGDECCOCINTiXCwEIQAACEIAABBBAYgACEIAABCAAAQiMGAEEcMQanMuFAAQgAAEIQAACCCAxAAEIQAACEIAABEaMAAI4Yg3O5UIAAhCAAAQgAIH/D8iw9rHCgOcZAAAAAElFTkSuQmCC" />
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch [0]: lr=1.000e-03
Epoch [0] train aveloss=1.524 aveacc=32.250
Test:Result* Prec@1 39.600	Loss 1.395
Test[0]:Result* Prec@1 39.600	Loss 1.395
Epoch [1]: lr=1.000e-03
Epoch [1] train aveloss=1.330 aveacc=43.060
Test:Result* Prec@1 55.000	Loss 1.127
Test[1]:Result* Prec@1 55.000	Loss 1.127
Epoch [2]: lr=1.000e-03
Epoch [2] train aveloss=1.091 aveacc=52.370
Test:Result* Prec@1 56.900	Loss 0.959
Test[2]:Result* Prec@1 56.900	Loss 0.959
Epoch [3]: lr=1.000e-03
Epoch [3] train aveloss=0.916 aveacc=59.360
Test:Result* Prec@1 60.700	Loss 0.839
Test[3]:Result* Prec@1 60.700	Loss 0.839
Epoch [4]: lr=1.000e-03
Epoch [4] train aveloss=0.843 aveacc=61.250
Test:Result* Prec@1 60.900	Loss 0.824
Test[4]:Result* Prec@1 60.900	Loss 0.824
Epoch [5]: lr=1.000e-03
Epoch [5] train aveloss=0.818 aveacc=61.550
Test:Result* Prec@1 63.500	Loss 0.777
Test[5]:Result* Prec@1 63.500	Loss 0.777
Epoch [6]: lr=1.000e-03
Epoch [6] train aveloss=0.799 aveacc=62.180
Test:Result* Prec@1 62.400	Loss 0.755
Test[6]:Result* Prec@1 62.400	Loss 0.755
Epoch [7]: lr=1.000e-03
Epoch [7] train aveloss=0.758 aveacc=63.820
Test:Result* Prec@1 63.200	Loss 0.743
Test[7]:Result* Prec@1 63.200	Loss 0.743
Epoch [8]: lr=1.000e-03
Epoch [8] train aveloss=0.756 aveacc=64.200
Test:Result* Prec@1 59.800	Loss 0.871
Test[8]:Result* Prec@1 59.800	Loss 0.871
Epoch [9]: lr=1.000e-03
Epoch [9] train aveloss=0.756 aveacc=63.440
Test:Result* Prec@1 65.200	Loss 0.709
Test[9]:Result* Prec@1 65.200	Loss 0.709
Epoch [10]: lr=1.000e-03
Epoch [10] train aveloss=0.737 aveacc=64.490
Test:Result* Prec@1 69.400	Loss 0.668
Test[10]:Result* Prec@1 69.400	Loss 0.668
Epoch [11]: lr=1.000e-03
Epoch [11] train aveloss=0.738 aveacc=64.040
Test:Result* Prec@1 69.600	Loss 0.644
Test[11]:Result* Prec@1 69.600	Loss 0.644
Epoch [12]: lr=1.000e-03
Epoch [12] train aveloss=0.728 aveacc=64.490
Test:Result* Prec@1 69.000	Loss 0.652
Test[12]:Result* Prec@1 69.000	Loss 0.652
Epoch [13]: lr=1.000e-03
Epoch [13] train aveloss=0.715 aveacc=65.090
Test:Result* Prec@1 65.200	Loss 0.710
Test[13]:Result* Prec@1 65.200	Loss 0.710
Epoch [14]: lr=1.000e-03
Epoch [14] train aveloss=0.708 aveacc=65.540
Test:Result* Prec@1 67.000	Loss 0.695
Test[14]:Result* Prec@1 67.000	Loss 0.695
Epoch [15]: lr=1.000e-03
Epoch [15] train aveloss=0.707 aveacc=65.780
Test:Result* Prec@1 64.000	Loss 0.788
Test[15]:Result* Prec@1 64.000	Loss 0.788
Epoch [16]: lr=1.000e-03
Epoch [16] train aveloss=0.710 aveacc=65.440
Test:Result* Prec@1 71.000	Loss 0.614
Test[16]:Result* Prec@1 71.000	Loss 0.614
Epoch [17]: lr=1.000e-03
Epoch [17] train aveloss=0.690 aveacc=66.060
Test:Result* Prec@1 70.500	Loss 0.591
Test[17]:Result* Prec@1 70.500	Loss 0.591
Epoch [18]: lr=1.000e-03
Epoch [18] train aveloss=0.690 aveacc=66.580
Test:Result* Prec@1 68.700	Loss 0.644
Test[18]:Result* Prec@1 68.700	Loss 0.644
Epoch [19]: lr=1.000e-03
Epoch [19] train aveloss=0.704 aveacc=64.850
Test:Result* Prec@1 64.400	Loss 0.757
Test[19]:Result* Prec@1 64.400	Loss 0.757
Epoch [20]: lr=1.000e-03
Epoch [20] train aveloss=0.684 aveacc=66.560
Test:Result* Prec@1 60.300	Loss 0.811
Test[20]:Result* Prec@1 60.300	Loss 0.811
Epoch [21]: lr=1.000e-03
Epoch [21] train aveloss=0.683 aveacc=65.900
Test:Result* Prec@1 68.400	Loss 0.642
Test[21]:Result* Prec@1 68.400	Loss 0.642
Epoch [22]: lr=1.000e-03
Epoch [22] train aveloss=0.682 aveacc=66.480
Test:Result* Prec@1 69.100	Loss 0.651
Test[22]:Result* Prec@1 69.100	Loss 0.651
Epoch [23]: lr=1.000e-03
Epoch [23] train aveloss=0.665 aveacc=67.110
Test:Result* Prec@1 68.400	Loss 0.607
Test[23]:Result* Prec@1 68.400	Loss 0.607
Epoch [24]: lr=1.000e-03
Epoch [24] train aveloss=0.685 aveacc=66.630
Test:Result* Prec@1 67.800	Loss 0.631
Test[24]:Result* Prec@1 67.800	Loss 0.631
Epoch [25]: lr=1.000e-03
Epoch [25] train aveloss=0.655 aveacc=67.620
Test:Result* Prec@1 63.400	Loss 0.732
Test[25]:Result* Prec@1 63.400	Loss 0.732
Epoch [26]: lr=1.000e-03
Epoch [26] train aveloss=0.660 aveacc=67.440
Test:Result* Prec@1 65.400	Loss 0.625
Test[26]:Result* Prec@1 65.400	Loss 0.625
Epoch [27]: lr=1.000e-03
Epoch [27] train aveloss=0.653 aveacc=67.830
Test:Result* Prec@1 67.100	Loss 0.657
Test[27]:Result* Prec@1 67.100	Loss 0.657
Epoch [28]: lr=1.000e-03
Epoch [28] train aveloss=0.649 aveacc=67.220
Test:Result* Prec@1 66.400	Loss 0.633
Test[28]:Result* Prec@1 66.400	Loss 0.633
Epoch [29]: lr=1.000e-03
Epoch [29] train aveloss=0.652 aveacc=68.070
Test:Result* Prec@1 68.800	Loss 0.616
Test[29]:Result* Prec@1 68.800	Loss 0.616
Epoch [30]: lr=1.000e-03
Epoch [30] train aveloss=0.645 aveacc=68.210
Test:Result* Prec@1 67.300	Loss 0.616
Test[30]:Result* Prec@1 67.300	Loss 0.616
Epoch [31]: lr=1.000e-03
Epoch [31] train aveloss=0.646 aveacc=68.050
Test:Result* Prec@1 66.300	Loss 0.640
Test[31]:Result* Prec@1 66.300	Loss 0.640
Epoch [32]: lr=1.000e-03
Epoch [32] train aveloss=0.659 aveacc=67.280
Test:Result* Prec@1 70.100	Loss 0.614
Test[32]:Result* Prec@1 70.100	Loss 0.614
Epoch [33]: lr=1.000e-03
Epoch [33] train aveloss=0.653 aveacc=67.730
Test:Result* Prec@1 72.900	Loss 0.574
Test[33]:Result* Prec@1 72.900	Loss 0.574
Epoch [34]: lr=1.000e-03
Epoch [34] train aveloss=0.652 aveacc=67.740
Test:Result* Prec@1 67.000	Loss 0.665
Test[34]:Result* Prec@1 67.000	Loss 0.665
Epoch [35]: lr=1.000e-03
Epoch [35] train aveloss=0.653 aveacc=68.330
Test:Result* Prec@1 69.300	Loss 0.662
Test[35]:Result* Prec@1 69.300	Loss 0.662
Epoch [36]: lr=1.000e-03
Epoch [36] train aveloss=0.636 aveacc=68.940
Test:Result* Prec@1 72.000	Loss 0.583
Test[36]:Result* Prec@1 72.000	Loss 0.583
Epoch [37]: lr=1.000e-03
Epoch [37] train aveloss=0.627 aveacc=69.470
Test:Result* Prec@1 67.500	Loss 0.611
Test[37]:Result* Prec@1 67.500	Loss 0.611
Epoch [38]: lr=1.000e-03
Epoch [38] train aveloss=0.624 aveacc=69.990
Test:Result* Prec@1 69.800	Loss 0.632
Test[38]:Result* Prec@1 69.800	Loss 0.632
Epoch [39]: lr=1.000e-03
Epoch [39] train aveloss=0.624 aveacc=70.140
Test:Result* Prec@1 68.200	Loss 0.640
Test[39]:Result* Prec@1 68.200	Loss 0.640
Epoch [40]: lr=1.000e-03
Epoch [40] train aveloss=0.618 aveacc=70.170
Test:Result* Prec@1 68.900	Loss 0.646
Test[40]:Result* Prec@1 68.900	Loss 0.646
Epoch [41]: lr=1.000e-03
Epoch [41] train aveloss=0.625 aveacc=69.740
Test:Result* Prec@1 74.500	Loss 0.564
Test[41]:Result* Prec@1 74.500	Loss 0.564
Epoch [42]: lr=1.000e-03
Epoch [42] train aveloss=0.606 aveacc=71.150
Test:Result* Prec@1 71.600	Loss 0.577
Test[42]:Result* Prec@1 71.600	Loss 0.577
Epoch [43]: lr=1.000e-03
Epoch [43] train aveloss=0.604 aveacc=71.340
Test:Result* Prec@1 74.200	Loss 0.564
Test[43]:Result* Prec@1 74.200	Loss 0.564
Epoch [44]: lr=1.000e-03
Epoch [44] train aveloss=0.595 aveacc=71.690
Test:Result* Prec@1 67.900	Loss 0.652
Test[44]:Result* Prec@1 67.900	Loss 0.652
Epoch [45]: lr=1.000e-03
Epoch [45] train aveloss=0.601 aveacc=71.570
Test:Result* Prec@1 70.800	Loss 0.624
Test[45]:Result* Prec@1 70.800	Loss 0.624
Epoch [46]: lr=1.000e-03
Epoch [46] train aveloss=0.588 aveacc=72.170
Test:Result* Prec@1 69.000	Loss 0.625
Test[46]:Result* Prec@1 69.000	Loss 0.625
Epoch [47]: lr=1.000e-03
Epoch [47] train aveloss=0.588 aveacc=72.720
Test:Result* Prec@1 70.700	Loss 0.648
Test[47]:Result* Prec@1 70.700	Loss 0.648
Epoch [48]: lr=1.000e-03
Epoch [48] train aveloss=0.601 aveacc=71.810
Test:Result* Prec@1 71.000	Loss 0.638
Test[48]:Result* Prec@1 71.000	Loss 0.638
Epoch [49]: lr=1.000e-03
Epoch [49] train aveloss=0.593 aveacc=72.860
Test:Result* Prec@1 73.600	Loss 0.573
Test[49]:Result* Prec@1 73.600	Loss 0.573
Epoch [50]: lr=1.000e-03
Epoch [50] train aveloss=0.577 aveacc=73.390
Test:Result* Prec@1 71.600	Loss 0.621
Test[50]:Result* Prec@1 71.600	Loss 0.621
Epoch [51]: lr=1.000e-03
Epoch [51] train aveloss=0.583 aveacc=73.380
Test:Result* Prec@1 77.100	Loss 0.514
Test[51]:Result* Prec@1 77.100	Loss 0.514
Epoch [52]: lr=1.000e-03
Epoch [52] train aveloss=0.571 aveacc=74.730
Test:Result* Prec@1 75.900	Loss 0.565
Test[52]:Result* Prec@1 75.900	Loss 0.565
Epoch [53]: lr=1.000e-03
Epoch [53] train aveloss=0.571 aveacc=74.160
Test:Result* Prec@1 72.200	Loss 0.597
Test[53]:Result* Prec@1 72.200	Loss 0.597
Epoch [54]: lr=1.000e-03
Epoch [54] train aveloss=0.574 aveacc=73.780
Test:Result* Prec@1 71.600	Loss 0.648
Test[54]:Result* Prec@1 71.600	Loss 0.648
Epoch [55]: lr=1.000e-03
Epoch [55] train aveloss=0.567 aveacc=74.910
Test:Result* Prec@1 69.200	Loss 0.640
Test[55]:Result* Prec@1 69.200	Loss 0.640
Epoch [56]: lr=1.000e-03
Epoch [56] train aveloss=0.552 aveacc=75.510
Test:Result* Prec@1 71.900	Loss 0.639
Test[56]:Result* Prec@1 71.900	Loss 0.639
Epoch [57]: lr=1.000e-03
Epoch [57] train aveloss=0.552 aveacc=75.970
Test:Result* Prec@1 76.200	Loss 0.581
Test[57]:Result* Prec@1 76.200	Loss 0.581
Epoch [58]: lr=1.000e-03
Epoch [58] train aveloss=0.552 aveacc=76.070
Test:Result* Prec@1 73.300	Loss 0.633
Test[58]:Result* Prec@1 73.300	Loss 0.633
Epoch [59]: lr=1.000e-03
Epoch [59] train aveloss=0.544 aveacc=76.170
Test:Result* Prec@1 69.600	Loss 0.683
Test[59]:Result* Prec@1 69.600	Loss 0.683
Epoch [60]: lr=1.000e-03
Epoch [60] train aveloss=0.538 aveacc=76.280
Test:Result* Prec@1 75.200	Loss 0.554
Test[60]:Result* Prec@1 75.200	Loss 0.554
Epoch [61]: lr=1.000e-03
Epoch [61] train aveloss=0.540 aveacc=76.620
Test:Result* Prec@1 78.400	Loss 0.486
Test[61]:Result* Prec@1 78.400	Loss 0.486
Epoch [62]: lr=1.000e-03
Epoch [62] train aveloss=0.528 aveacc=76.860
Test:Result* Prec@1 79.900	Loss 0.490
Test[62]:Result* Prec@1 79.900	Loss 0.490
Epoch [63]: lr=1.000e-03
Epoch [63] train aveloss=0.534 aveacc=77.080
Test:Result* Prec@1 71.100	Loss 0.738
Test[63]:Result* Prec@1 71.100	Loss 0.738
Epoch [64]: lr=1.000e-03
Epoch [64] train aveloss=0.521 aveacc=78.540
Test:Result* Prec@1 78.100	Loss 0.554
Test[64]:Result* Prec@1 78.100	Loss 0.554
Epoch [65]: lr=1.000e-03
Epoch [65] train aveloss=0.528 aveacc=77.670
Test:Result* Prec@1 79.200	Loss 0.472
Test[65]:Result* Prec@1 79.200	Loss 0.472
Epoch [66]: lr=1.000e-03
Epoch [66] train aveloss=0.509 aveacc=78.810
Test:Result* Prec@1 80.300	Loss 0.461
Test[66]:Result* Prec@1 80.300	Loss 0.461
Epoch [67]: lr=1.000e-03
Epoch [67] train aveloss=0.517 aveacc=78.520
Test:Result* Prec@1 75.900	Loss 0.575
Test[67]:Result* Prec@1 75.900	Loss 0.575
Epoch [68]: lr=1.000e-03
Epoch [68] train aveloss=0.504 aveacc=78.920
Test:Result* Prec@1 77.500	Loss 0.515
Test[68]:Result* Prec@1 77.500	Loss 0.515
Epoch [69]: lr=1.000e-03
Epoch [69] train aveloss=0.516 aveacc=78.270
Test:Result* Prec@1 74.500	Loss 0.618
Test[69]:Result* Prec@1 74.500	Loss 0.618
Epoch [70]: lr=1.000e-03
Epoch [70] train aveloss=0.510 aveacc=79.150
Test:Result* Prec@1 74.600	Loss 0.637
Test[70]:Result* Prec@1 74.600	Loss 0.637
Epoch [71]: lr=1.000e-03
Epoch [71] train aveloss=0.511 aveacc=78.690
Test:Result* Prec@1 77.700	Loss 0.590
Test[71]:Result* Prec@1 77.700	Loss 0.590
Epoch [72]: lr=1.000e-03
Epoch [72] train aveloss=0.494 aveacc=79.770
Test:Result* Prec@1 80.700	Loss 0.491
Test[72]:Result* Prec@1 80.700	Loss 0.491
Epoch [73]: lr=1.000e-03
Epoch [73] train aveloss=0.497 aveacc=79.920
Test:Result* Prec@1 79.800	Loss 0.479
Test[73]:Result* Prec@1 79.800	Loss 0.479
Epoch [74]: lr=1.000e-03
Epoch [74] train aveloss=0.492 aveacc=79.450
Test:Result* Prec@1 79.000	Loss 0.532
Test[74]:Result* Prec@1 79.000	Loss 0.532
Epoch [75]: lr=1.000e-03
Epoch [75] train aveloss=0.480 aveacc=80.600
Test:Result* Prec@1 77.000	Loss 0.584
Test[75]:Result* Prec@1 77.000	Loss 0.584
Epoch [76]: lr=1.000e-03
Epoch [76] train aveloss=0.483 aveacc=80.140
Test:Result* Prec@1 80.300	Loss 0.488
Test[76]:Result* Prec@1 80.300	Loss 0.488
Epoch [77]: lr=1.000e-03
Epoch [77] train aveloss=0.484 aveacc=80.320
Test:Result* Prec@1 74.500	Loss 0.630
Test[77]:Result* Prec@1 74.500	Loss 0.630
Epoch [78]: lr=1.000e-03
Epoch [78] train aveloss=0.475 aveacc=80.790
Test:Result* Prec@1 81.400	Loss 0.485
Test[78]:Result* Prec@1 81.400	Loss 0.485
Epoch [79]: lr=1.000e-03
Epoch [79] train aveloss=0.483 aveacc=80.010
Test:Result* Prec@1 77.200	Loss 0.513
Test[79]:Result* Prec@1 77.200	Loss 0.513
Epoch [80]: lr=1.000e-03
Epoch [80] train aveloss=0.472 aveacc=80.920
Test:Result* Prec@1 70.600	Loss 0.837
Test[80]:Result* Prec@1 70.600	Loss 0.837
Epoch [81]: lr=1.000e-03
Epoch [81] train aveloss=0.477 aveacc=80.800
Test:Result* Prec@1 83.600	Loss 0.409
Test[81]:Result* Prec@1 83.600	Loss 0.409
Epoch [82]: lr=1.000e-03
Epoch [82] train aveloss=0.452 aveacc=81.820
Test:Result* Prec@1 78.200	Loss 0.564
Test[82]:Result* Prec@1 78.200	Loss 0.564
Epoch [83]: lr=1.000e-03
Epoch [83] train aveloss=0.441 aveacc=82.550
Test:Result* Prec@1 79.300	Loss 0.528
Test[83]:Result* Prec@1 79.300	Loss 0.528
Epoch [84]: lr=1.000e-03
Epoch [84] train aveloss=0.473 aveacc=80.360
Test:Result* Prec@1 80.500	Loss 0.517
Test[84]:Result* Prec@1 80.500	Loss 0.517
Epoch [85]: lr=1.000e-03
Epoch [85] train aveloss=0.455 aveacc=82.020
Test:Result* Prec@1 80.800	Loss 0.466
Test[85]:Result* Prec@1 80.800	Loss 0.466
Epoch [86]: lr=1.000e-03
Epoch [86] train aveloss=0.459 aveacc=82.090
Test:Result* Prec@1 80.900	Loss 0.471
Test[86]:Result* Prec@1 80.900	Loss 0.471
Epoch [87]: lr=1.000e-03
Epoch [87] train aveloss=0.461 aveacc=81.620
Test:Result* Prec@1 81.200	Loss 0.470
Test[87]:Result* Prec@1 81.200	Loss 0.470
Epoch [88]: lr=1.000e-03
Epoch [88] train aveloss=0.455 aveacc=81.950
Test:Result* Prec@1 77.700	Loss 0.546
Test[88]:Result* Prec@1 77.700	Loss 0.546
Epoch [89]: lr=1.000e-03
Epoch [89] train aveloss=0.445 aveacc=81.880
Test:Result* Prec@1 76.900	Loss 0.563
Test[89]:Result* Prec@1 76.900	Loss 0.563
Epoch [90]: lr=1.000e-03
Epoch [90] train aveloss=0.447 aveacc=82.380
Test:Result* Prec@1 82.600	Loss 0.431
Test[90]:Result* Prec@1 82.600	Loss 0.431
Epoch [91]: lr=1.000e-03
Epoch [91] train aveloss=0.443 aveacc=82.340
Test:Result* Prec@1 82.600	Loss 0.444
Test[91]:Result* Prec@1 82.600	Loss 0.444
Epoch [92]: lr=1.000e-03
Epoch [92] train aveloss=0.449 aveacc=82.370
Test:Result* Prec@1 78.700	Loss 0.610
Test[92]:Result* Prec@1 78.700	Loss 0.610
Epoch [93]: lr=1.000e-03
Epoch [93] train aveloss=0.443 aveacc=82.320
Test:Result* Prec@1 81.100	Loss 0.465
Test[93]:Result* Prec@1 81.100	Loss 0.465
Epoch [94]: lr=1.000e-03
Epoch [94] train aveloss=0.439 aveacc=82.650
Test:Result* Prec@1 82.800	Loss 0.457
Test[94]:Result* Prec@1 82.800	Loss 0.457
Epoch [95]: lr=1.000e-03
Epoch [95] train aveloss=0.433 aveacc=82.470
Test:Result* Prec@1 80.500	Loss 0.481
Test[95]:Result* Prec@1 80.500	Loss 0.481
Epoch [96]: lr=1.000e-03
Epoch [96] train aveloss=0.443 aveacc=82.380
Test:Result* Prec@1 82.300	Loss 0.426
Test[96]:Result* Prec@1 82.300	Loss 0.426
Epoch [97]: lr=1.000e-03
Epoch [97] train aveloss=0.441 aveacc=82.440
Test:Result* Prec@1 83.700	Loss 0.390
Test[97]:Result* Prec@1 83.700	Loss 0.390
Epoch [98]: lr=1.000e-03
Epoch [98] train aveloss=0.414 aveacc=83.530
Test:Result* Prec@1 80.700	Loss 0.496
Test[98]:Result* Prec@1 80.700	Loss 0.496
Epoch [99]: lr=1.000e-03
Epoch [99] train aveloss=0.424 aveacc=83.180
Test:Result* Prec@1 81.700	Loss 0.434
Test[99]:Result* Prec@1 81.700	Loss 0.434
Epoch [100]: lr=1.000e-03
Epoch [100] train aveloss=0.443 aveacc=82.440
Test:Result* Prec@1 81.800	Loss 0.444
Test[100]:Result* Prec@1 81.800	Loss 0.444
Epoch [101]: lr=1.000e-03
Epoch [101] train aveloss=0.419 aveacc=83.310
Test:Result* Prec@1 85.000	Loss 0.385
Test[101]:Result* Prec@1 85.000	Loss 0.385
Epoch [102]: lr=1.000e-03
Epoch [102] train aveloss=0.424 aveacc=83.220
Test:Result* Prec@1 83.800	Loss 0.421
Test[102]:Result* Prec@1 83.800	Loss 0.421
Epoch [103]: lr=1.000e-03
Epoch [103] train aveloss=0.413 aveacc=83.770
Test:Result* Prec@1 80.700	Loss 0.495
Test[103]:Result* Prec@1 80.700	Loss 0.495
Epoch [104]: lr=1.000e-03
Epoch [104] train aveloss=0.421 aveacc=83.410
Test:Result* Prec@1 84.500	Loss 0.403
Test[104]:Result* Prec@1 84.500	Loss 0.403
Epoch [105]: lr=1.000e-03
Epoch [105] train aveloss=0.418 aveacc=83.690
Test:Result* Prec@1 81.900	Loss 0.448
Test[105]:Result* Prec@1 81.900	Loss 0.448
Epoch [106]: lr=1.000e-03
Epoch [106] train aveloss=0.424 aveacc=83.540
Test:Result* Prec@1 85.300	Loss 0.372
Test[106]:Result* Prec@1 85.300	Loss 0.372
Epoch [107]: lr=1.000e-03
Epoch [107] train aveloss=0.411 aveacc=83.910
Test:Result* Prec@1 82.500	Loss 0.410
Test[107]:Result* Prec@1 82.500	Loss 0.410
Epoch [108]: lr=1.000e-03
Epoch [108] train aveloss=0.419 aveacc=83.850
Test:Result* Prec@1 82.700	Loss 0.431
Test[108]:Result* Prec@1 82.700	Loss 0.431
Epoch [109]: lr=1.000e-03
Epoch [109] train aveloss=0.412 aveacc=83.690
Test:Result* Prec@1 81.900	Loss 0.447
Test[109]:Result* Prec@1 81.900	Loss 0.447
Epoch [110]: lr=1.000e-03
Epoch [110] train aveloss=0.417 aveacc=83.530
Test:Result* Prec@1 84.000	Loss 0.396
Test[110]:Result* Prec@1 84.000	Loss 0.396
Epoch [111]: lr=1.000e-03
Epoch [111] train aveloss=0.410 aveacc=83.790
Test:Result* Prec@1 80.100	Loss 0.473
Test[111]:Result* Prec@1 80.100	Loss 0.473
Epoch [112]: lr=1.000e-03
Epoch [112] train aveloss=0.408 aveacc=83.980
Test:Result* Prec@1 85.900	Loss 0.371
Test[112]:Result* Prec@1 85.900	Loss 0.371
Epoch [113]: lr=1.000e-03
Epoch [113] train aveloss=0.407 aveacc=83.940
Test:Result* Prec@1 81.000	Loss 0.452
Test[113]:Result* Prec@1 81.000	Loss 0.452
Epoch [114]: lr=1.000e-03
Epoch [114] train aveloss=0.415 aveacc=84.100
Test:Result* Prec@1 83.100	Loss 0.423
Test[114]:Result* Prec@1 83.100	Loss 0.423
Epoch [115]: lr=1.000e-03
Epoch [115] train aveloss=0.409 aveacc=84.270
Test:Result* Prec@1 81.500	Loss 0.472
Test[115]:Result* Prec@1 81.500	Loss 0.472
Epoch [116]: lr=1.000e-03
Epoch [116] train aveloss=0.401 aveacc=84.350
Test:Result* Prec@1 85.400	Loss 0.370
Test[116]:Result* Prec@1 85.400	Loss 0.370
Epoch [117]: lr=1.000e-03
Epoch [117] train aveloss=0.403 aveacc=83.950
Test:Result* Prec@1 81.400	Loss 0.513
Test[117]:Result* Prec@1 81.400	Loss 0.513
Epoch [118]: lr=1.000e-03
Epoch [118] train aveloss=0.406 aveacc=84.340
Test:Result* Prec@1 82.000	Loss 0.442
Test[118]:Result* Prec@1 82.000	Loss 0.442
Epoch [119]: lr=1.000e-03
Epoch [119] train aveloss=0.414 aveacc=83.910
Test:Result* Prec@1 80.800	Loss 0.486
Test[119]:Result* Prec@1 80.800	Loss 0.486
Epoch [120]: lr=1.000e-03
Epoch [120] train aveloss=0.406 aveacc=84.150
Test:Result* Prec@1 82.500	Loss 0.423
Test[120]:Result* Prec@1 82.500	Loss 0.423
Epoch [121]: lr=1.000e-03
Epoch [121] train aveloss=0.390 aveacc=85.060
Test:Result* Prec@1 84.900	Loss 0.382
Test[121]:Result* Prec@1 84.900	Loss 0.382
Epoch [122]: lr=1.000e-03
Epoch [122] train aveloss=0.393 aveacc=84.710
Test:Result* Prec@1 81.300	Loss 0.428
Test[122]:Result* Prec@1 81.300	Loss 0.428
Epoch [123]: lr=1.000e-03
Epoch [123] train aveloss=0.386 aveacc=84.930
Test:Result* Prec@1 84.300	Loss 0.408
Test[123]:Result* Prec@1 84.300	Loss 0.408
Epoch [124]: lr=1.000e-03
Epoch [124] train aveloss=0.384 aveacc=84.870
Test:Result* Prec@1 83.600	Loss 0.422
Test[124]:Result* Prec@1 83.600	Loss 0.422
Epoch [125]: lr=1.000e-03
Epoch [125] train aveloss=0.389 aveacc=85.070
Test:Result* Prec@1 85.500	Loss 0.381
Test[125]:Result* Prec@1 85.500	Loss 0.381
Epoch [126]: lr=1.000e-03
Epoch [126] train aveloss=0.392 aveacc=84.600
Test:Result* Prec@1 82.100	Loss 0.440
Test[126]:Result* Prec@1 82.100	Loss 0.440
Epoch [127]: lr=1.000e-03
Epoch [127] train aveloss=0.375 aveacc=85.170
Test:Result* Prec@1 83.600	Loss 0.471
Test[127]:Result* Prec@1 83.600	Loss 0.471
Epoch [128]: lr=1.000e-03
Epoch [128] train aveloss=0.409 aveacc=84.080
Test:Result* Prec@1 82.900	Loss 0.450
Test[128]:Result* Prec@1 82.900	Loss 0.450
Epoch [129]: lr=1.000e-03
Epoch [129] train aveloss=0.381 aveacc=85.270
Test:Result* Prec@1 83.300	Loss 0.416
Test[129]:Result* Prec@1 83.300	Loss 0.416
Epoch [130]: lr=1.000e-03
Epoch [130] train aveloss=0.377 aveacc=85.080
Test:Result* Prec@1 85.000	Loss 0.376
Test[130]:Result* Prec@1 85.000	Loss 0.376
Epoch [131]: lr=1.000e-03
Epoch [131] train aveloss=0.383 aveacc=84.870
Test:Result* Prec@1 85.000	Loss 0.411
Test[131]:Result* Prec@1 85.000	Loss 0.411
Epoch [132]: lr=1.000e-03
Epoch [132] train aveloss=0.386 aveacc=84.960
Test:Result* Prec@1 83.700	Loss 0.410
Test[132]:Result* Prec@1 83.700	Loss 0.410
Epoch [133]: lr=1.000e-03
Epoch [133] train aveloss=0.390 aveacc=84.150
Test:Result* Prec@1 79.200	Loss 0.551
Test[133]:Result* Prec@1 79.200	Loss 0.551
Epoch [134]: lr=1.000e-03
Epoch [134] train aveloss=0.377 aveacc=85.150
Test:Result* Prec@1 84.600	Loss 0.394
Test[134]:Result* Prec@1 84.600	Loss 0.394
Epoch [135]: lr=1.000e-03
Epoch [135] train aveloss=0.378 aveacc=85.390
Test:Result* Prec@1 77.800	Loss 0.718
Test[135]:Result* Prec@1 77.800	Loss 0.718
Epoch [136]: lr=1.000e-03
Epoch [136] train aveloss=0.393 aveacc=84.770
Test:Result* Prec@1 85.700	Loss 0.365
Test[136]:Result* Prec@1 85.700	Loss 0.365
Epoch [137]: lr=1.000e-03
Epoch [137] train aveloss=0.378 aveacc=85.310
Test:Result* Prec@1 86.600	Loss 0.345
Test[137]:Result* Prec@1 86.600	Loss 0.345
Epoch [138]: lr=1.000e-03
Epoch [138] train aveloss=0.386 aveacc=85.450
Test:Result* Prec@1 83.500	Loss 0.397
Test[138]:Result* Prec@1 83.500	Loss 0.397
Epoch [139]: lr=1.000e-03
Epoch [139] train aveloss=0.371 aveacc=85.720
Test:Result* Prec@1 84.200	Loss 0.388
Test[139]:Result* Prec@1 84.200	Loss 0.388
Epoch [140]: lr=1.000e-03
Epoch [140] train aveloss=0.366 aveacc=85.860
Test:Result* Prec@1 80.400	Loss 0.547
Test[140]:Result* Prec@1 80.400	Loss 0.547
Epoch [141]: lr=1.000e-03
Epoch [141] train aveloss=0.363 aveacc=85.830
Test:Result* Prec@1 85.300	Loss 0.379
Test[141]:Result* Prec@1 85.300	Loss 0.379
Epoch [142]: lr=1.000e-03
Epoch [142] train aveloss=0.375 aveacc=85.420
Test:Result* Prec@1 85.700	Loss 0.379
Test[142]:Result* Prec@1 85.700	Loss 0.379
Epoch [143]: lr=1.000e-03
Epoch [143] train aveloss=0.374 aveacc=85.470
Test:Result* Prec@1 83.400	Loss 0.415
Test[143]:Result* Prec@1 83.400	Loss 0.415
Epoch [144]: lr=1.000e-03
Epoch [144] train aveloss=0.365 aveacc=86.050
Test:Result* Prec@1 85.300	Loss 0.371
Test[144]:Result* Prec@1 85.300	Loss 0.371
Epoch [145]: lr=1.000e-03
Epoch [145] train aveloss=0.368 aveacc=85.560
Test:Result* Prec@1 85.700	Loss 0.360
Test[145]:Result* Prec@1 85.700	Loss 0.360
Epoch [146]: lr=1.000e-03
Epoch [146] train aveloss=0.360 aveacc=86.380
Test:Result* Prec@1 85.400	Loss 0.361
Test[146]:Result* Prec@1 85.400	Loss 0.361
Epoch [147]: lr=1.000e-03
Epoch [147] train aveloss=0.353 aveacc=86.220
Test:Result* Prec@1 84.900	Loss 0.416
Test[147]:Result* Prec@1 84.900	Loss 0.416
Epoch [148]: lr=1.000e-03
Epoch [148] train aveloss=0.354 aveacc=86.500
Test:Result* Prec@1 84.300	Loss 0.388
Test[148]:Result* Prec@1 84.300	Loss 0.388
Epoch [149]: lr=1.000e-03
Epoch [149] train aveloss=0.363 aveacc=85.690
Test:Result* Prec@1 81.100	Loss 0.459
Test[149]:Result* Prec@1 81.100	Loss 0.459
Epoch [150]: lr=1.000e-03
Epoch [150] train aveloss=0.375 aveacc=85.300
Test:Result* Prec@1 82.900	Loss 0.421
Test[150]:Result* Prec@1 82.900	Loss 0.421
Epoch [151]: lr=1.000e-03
Epoch [151] train aveloss=0.342 aveacc=86.500
Test:Result* Prec@1 82.400	Loss 0.436
Test[151]:Result* Prec@1 82.400	Loss 0.436
Epoch [152]: lr=1.000e-03
Epoch [152] train aveloss=0.375 aveacc=85.510
Test:Result* Prec@1 83.400	Loss 0.427
Test[152]:Result* Prec@1 83.400	Loss 0.427
Epoch [153]: lr=1.000e-03
Epoch [153] train aveloss=0.361 aveacc=85.980
Test:Result* Prec@1 84.800	Loss 0.413
Test[153]:Result* Prec@1 84.800	Loss 0.413
Epoch [154]: lr=1.000e-03
Epoch [154] train aveloss=0.367 aveacc=86.170
Test:Result* Prec@1 84.400	Loss 0.394
Test[154]:Result* Prec@1 84.400	Loss 0.394
Epoch [155]: lr=1.000e-03
Epoch [155] train aveloss=0.355 aveacc=85.960
Test:Result* Prec@1 84.300	Loss 0.399
Test[155]:Result* Prec@1 84.300	Loss 0.399
Epoch [156]: lr=1.000e-03
Epoch [156] train aveloss=0.356 aveacc=86.010
Test:Result* Prec@1 85.700	Loss 0.353
Test[156]:Result* Prec@1 85.700	Loss 0.353
Epoch [157]: lr=1.000e-03
Epoch [157] train aveloss=0.362 aveacc=86.010
Test:Result* Prec@1 81.500	Loss 0.505
Test[157]:Result* Prec@1 81.500	Loss 0.505
Epoch [158]: lr=1.000e-03
Epoch [158] train aveloss=0.366 aveacc=85.910
Test:Result* Prec@1 81.500	Loss 0.502
Test[158]:Result* Prec@1 81.500	Loss 0.502
Epoch [159]: lr=1.000e-03
Epoch [159] train aveloss=0.347 aveacc=86.640
Test:Result* Prec@1 85.000	Loss 0.400
Test[159]:Result* Prec@1 85.000	Loss 0.400
Epoch [160]: lr=1.000e-03
Epoch [160] train aveloss=0.357 aveacc=86.050
Test:Result* Prec@1 81.700	Loss 0.464
Test[160]:Result* Prec@1 81.700	Loss 0.464
Epoch [161]: lr=1.000e-03
Epoch [161] train aveloss=0.357 aveacc=86.250
Test:Result* Prec@1 85.800	Loss 0.352
Test[161]:Result* Prec@1 85.800	Loss 0.352
Epoch [162]: lr=1.000e-03
Epoch [162] train aveloss=0.357 aveacc=86.330
Test:Result* Prec@1 84.100	Loss 0.385
Test[162]:Result* Prec@1 84.100	Loss 0.385
Epoch [163]: lr=1.000e-03
Epoch [163] train aveloss=0.366 aveacc=85.600
Test:Result* Prec@1 72.400	Loss 1.221
Test[163]:Result* Prec@1 72.400	Loss 1.221
Epoch [164]: lr=1.000e-03
Epoch [164] train aveloss=0.360 aveacc=86.050
Test:Result* Prec@1 85.000	Loss 0.399
Test[164]:Result* Prec@1 85.000	Loss 0.399
Epoch [165]: lr=1.000e-03
Epoch [165] train aveloss=0.345 aveacc=86.680
Test:Result* Prec@1 83.200	Loss 0.423
Test[165]:Result* Prec@1 83.200	Loss 0.423
Epoch [166]: lr=1.000e-03
Epoch [166] train aveloss=0.359 aveacc=86.010
Test:Result* Prec@1 84.800	Loss 0.414
Test[166]:Result* Prec@1 84.800	Loss 0.414
Epoch [167]: lr=1.000e-03
Epoch [167] train aveloss=0.349 aveacc=86.860
Test:Result* Prec@1 85.100	Loss 0.396
Test[167]:Result* Prec@1 85.100	Loss 0.396
Epoch [168]: lr=1.000e-03
Epoch [168] train aveloss=0.342 aveacc=86.870
Test:Result* Prec@1 75.500	Loss 0.839
Test[168]:Result* Prec@1 75.500	Loss 0.839
Epoch [169]: lr=1.000e-03
Epoch [169] train aveloss=0.359 aveacc=86.240
Test:Result* Prec@1 86.100	Loss 0.380
Test[169]:Result* Prec@1 86.100	Loss 0.380
Epoch [170]: lr=1.000e-03
Epoch [170] train aveloss=0.348 aveacc=86.410
Test:Result* Prec@1 84.400	Loss 0.384
Test[170]:Result* Prec@1 84.400	Loss 0.384
Epoch [171]: lr=1.000e-03
Epoch [171] train aveloss=0.351 aveacc=86.520
Test:Result* Prec@1 86.400	Loss 0.358
Test[171]:Result* Prec@1 86.400	Loss 0.358
Epoch [172]: lr=1.000e-03
Epoch [172] train aveloss=0.357 aveacc=86.280
Test:Result* Prec@1 76.100	Loss 0.862
Test[172]:Result* Prec@1 76.100	Loss 0.862
Epoch [173]: lr=1.000e-03
Epoch [173] train aveloss=0.343 aveacc=86.520
Test:Result* Prec@1 81.400	Loss 0.464
Test[173]:Result* Prec@1 81.400	Loss 0.464
Epoch [174]: lr=1.000e-03
Epoch [174] train aveloss=0.345 aveacc=86.630
Test:Result* Prec@1 81.600	Loss 0.444
Test[174]:Result* Prec@1 81.600	Loss 0.444
Epoch [175]: lr=1.000e-03
Epoch [175] train aveloss=0.358 aveacc=86.200
Test:Result* Prec@1 84.300	Loss 0.414
Test[175]:Result* Prec@1 84.300	Loss 0.414
Epoch [176]: lr=1.000e-03
Epoch [176] train aveloss=0.346 aveacc=86.440
Test:Result* Prec@1 85.200	Loss 0.394
Test[176]:Result* Prec@1 85.200	Loss 0.394
Epoch [177]: lr=1.000e-03
Epoch [177] train aveloss=0.343 aveacc=86.620
Test:Result* Prec@1 86.200	Loss 0.341
Test[177]:Result* Prec@1 86.200	Loss 0.341
Epoch [178]: lr=1.000e-03
Epoch [178] train aveloss=0.352 aveacc=86.460
Test:Result* Prec@1 85.100	Loss 0.399
Test[178]:Result* Prec@1 85.100	Loss 0.399
Epoch [179]: lr=1.000e-03
Epoch [179] train aveloss=0.341 aveacc=86.570
Test:Result* Prec@1 85.600	Loss 0.385
Test[179]:Result* Prec@1 85.600	Loss 0.385
Epoch [180]: lr=1.000e-03
Epoch [180] train aveloss=0.333 aveacc=87.160
Test:Result* Prec@1 82.200	Loss 0.472
Test[180]:Result* Prec@1 82.200	Loss 0.472
Epoch [181]: lr=1.000e-03
Epoch [181] train aveloss=0.348 aveacc=86.410
Test:Result* Prec@1 85.700	Loss 0.358
Test[181]:Result* Prec@1 85.700	Loss 0.358
Epoch [182]: lr=1.000e-03
Epoch [182] train aveloss=0.349 aveacc=86.700
Test:Result* Prec@1 84.200	Loss 0.390
Test[182]:Result* Prec@1 84.200	Loss 0.390
Epoch [183]: lr=1.000e-03
Epoch [183] train aveloss=0.342 aveacc=86.470
Test:Result* Prec@1 85.000	Loss 0.380
Test[183]:Result* Prec@1 85.000	Loss 0.380
Epoch [184]: lr=1.000e-03
Epoch [184] train aveloss=0.343 aveacc=86.940
Test:Result* Prec@1 82.900	Loss 0.410
Test[184]:Result* Prec@1 82.900	Loss 0.410
Epoch [185]: lr=1.000e-03
Epoch [185] train aveloss=0.343 aveacc=86.580
Test:Result* Prec@1 85.900	Loss 0.340
Test[185]:Result* Prec@1 85.900	Loss 0.340
Epoch [186]: lr=1.000e-03
Epoch [186] train aveloss=0.340 aveacc=86.930
Test:Result* Prec@1 85.700	Loss 0.405
Test[186]:Result* Prec@1 85.700	Loss 0.405
Epoch [187]: lr=1.000e-03
Epoch [187] train aveloss=0.338 aveacc=86.960
Test:Result* Prec@1 85.300	Loss 0.408
Test[187]:Result* Prec@1 85.300	Loss 0.408
Epoch [188]: lr=1.000e-03
Epoch [188] train aveloss=0.333 aveacc=87.090
Test:Result* Prec@1 85.900	Loss 0.379
Test[188]:Result* Prec@1 85.900	Loss 0.379
Epoch [189]: lr=1.000e-03
Epoch [189] train aveloss=0.342 aveacc=87.140
Test:Result* Prec@1 82.000	Loss 0.467
Test[189]:Result* Prec@1 82.000	Loss 0.467
Epoch [190]: lr=1.000e-03
Epoch [190] train aveloss=0.328 aveacc=87.170
Test:Result* Prec@1 84.000	Loss 0.433
Test[190]:Result* Prec@1 84.000	Loss 0.433
Epoch [191]: lr=1.000e-03
Epoch [191] train aveloss=0.329 aveacc=87.270
Test:Result* Prec@1 82.400	Loss 0.426
Test[191]:Result* Prec@1 82.400	Loss 0.426
Epoch [192]: lr=1.000e-03
Epoch [192] train aveloss=0.329 aveacc=87.260
Test:Result* Prec@1 85.700	Loss 0.349
Test[192]:Result* Prec@1 85.700	Loss 0.349
Epoch [193]: lr=1.000e-03
Epoch [193] train aveloss=0.332 aveacc=86.950
Test:Result* Prec@1 71.600	Loss 1.126
Test[193]:Result* Prec@1 71.600	Loss 1.126
Epoch [194]: lr=1.000e-03
Epoch [194] train aveloss=0.335 aveacc=86.860
Test:Result* Prec@1 85.700	Loss 0.395
Test[194]:Result* Prec@1 85.700	Loss 0.395
Epoch [195]: lr=1.000e-03
Epoch [195] train aveloss=0.331 aveacc=87.300
Test:Result* Prec@1 84.900	Loss 0.383
Test[195]:Result* Prec@1 84.900	Loss 0.383
Epoch [196]: lr=1.000e-03
Epoch [196] train aveloss=0.333 aveacc=86.990
Test:Result* Prec@1 86.000	Loss 0.392
Test[196]:Result* Prec@1 86.000	Loss 0.392
Epoch [197]: lr=1.000e-03
Epoch [197] train aveloss=0.332 aveacc=87.120
Test:Result* Prec@1 84.500	Loss 0.400
Test[197]:Result* Prec@1 84.500	Loss 0.400
Epoch [198]: lr=1.000e-03
Epoch [198] train aveloss=0.329 aveacc=87.390
Test:Result* Prec@1 84.800	Loss 0.431
Test[198]:Result* Prec@1 84.800	Loss 0.431
Epoch [199]: lr=1.000e-03
Epoch [199] train aveloss=0.331 aveacc=87.070
Test:Result* Prec@1 84.400	Loss 0.404
Test[199]:Result* Prec@1 84.400	Loss 0.404
Epoch [200]: lr=1.000e-03
Epoch [200] train aveloss=0.337 aveacc=87.000
Test:Result* Prec@1 83.200	Loss 0.426
Test[200]:Result* Prec@1 83.200	Loss 0.426
Epoch [201]: lr=1.000e-03
Epoch [201] train aveloss=0.335 aveacc=87.060
Test:Result* Prec@1 87.700	Loss 0.323
Test[201]:Result* Prec@1 87.700	Loss 0.323
Epoch [202]: lr=1.000e-03
Epoch [202] train aveloss=0.331 aveacc=87.260
Test:Result* Prec@1 84.200	Loss 0.438
Test[202]:Result* Prec@1 84.200	Loss 0.438
Epoch [203]: lr=1.000e-03
Epoch [203] train aveloss=0.322 aveacc=87.300
Test:Result* Prec@1 86.100	Loss 0.382
Test[203]:Result* Prec@1 86.100	Loss 0.382
Epoch [204]: lr=1.000e-03
Epoch [204] train aveloss=0.328 aveacc=86.960
Test:Result* Prec@1 85.000	Loss 0.391
Test[204]:Result* Prec@1 85.000	Loss 0.391
Epoch [205]: lr=1.000e-03
Epoch [205] train aveloss=0.325 aveacc=87.460
Test:Result* Prec@1 84.400	Loss 0.381
Test[205]:Result* Prec@1 84.400	Loss 0.381
Epoch [206]: lr=1.000e-03
Epoch [206] train aveloss=0.329 aveacc=87.230
Test:Result* Prec@1 86.100	Loss 0.370
Test[206]:Result* Prec@1 86.100	Loss 0.370
Epoch [207]: lr=1.000e-03
Epoch [207] train aveloss=0.321 aveacc=87.610
Test:Result* Prec@1 82.100	Loss 0.525
Test[207]:Result* Prec@1 82.100	Loss 0.525
Epoch [208]: lr=1.000e-03
Epoch [208] train aveloss=0.331 aveacc=87.290
Test:Result* Prec@1 83.700	Loss 0.422
Test[208]:Result* Prec@1 83.700	Loss 0.422
Epoch [209]: lr=1.000e-03
Epoch [209] train aveloss=0.330 aveacc=87.400
Test:Result* Prec@1 86.400	Loss 0.365
Test[209]:Result* Prec@1 86.400	Loss 0.365
Epoch [210]: lr=1.000e-03
Epoch [210] train aveloss=0.318 aveacc=87.640
Test:Result* Prec@1 85.800	Loss 0.370
Test[210]:Result* Prec@1 85.800	Loss 0.370
Epoch [211]: lr=1.000e-03
Epoch [211] train aveloss=0.333 aveacc=87.190
Test:Result* Prec@1 78.000	Loss 0.529
Test[211]:Result* Prec@1 78.000	Loss 0.529
Epoch [212]: lr=1.000e-03
Epoch [212] train aveloss=0.311 aveacc=87.610
Test:Result* Prec@1 77.000	Loss 0.565
Test[212]:Result* Prec@1 77.000	Loss 0.565
Epoch [213]: lr=1.000e-03
Epoch [213] train aveloss=0.308 aveacc=88.220
Test:Result* Prec@1 84.000	Loss 0.391
Test[213]:Result* Prec@1 84.000	Loss 0.391
Epoch [214]: lr=1.000e-03
Epoch [214] train aveloss=0.318 aveacc=87.740
Test:Result* Prec@1 81.400	Loss 0.472
Test[214]:Result* Prec@1 81.400	Loss 0.472
Epoch [215]: lr=1.000e-03
Epoch [215] train aveloss=0.318 aveacc=87.600
Test:Result* Prec@1 87.100	Loss 0.357
Test[215]:Result* Prec@1 87.100	Loss 0.357
Epoch [216]: lr=1.000e-03
Epoch [216] train aveloss=0.304 aveacc=88.550
Test:Result* Prec@1 86.800	Loss 0.309
Test[216]:Result* Prec@1 86.800	Loss 0.309
Epoch [217]: lr=1.000e-03
Epoch [217] train aveloss=0.323 aveacc=87.420
Test:Result* Prec@1 79.600	Loss 0.758
Test[217]:Result* Prec@1 79.600	Loss 0.758
Epoch [218]: lr=1.000e-03
Epoch [218] train aveloss=0.341 aveacc=86.690
Test:Result* Prec@1 84.300	Loss 0.410
Test[218]:Result* Prec@1 84.300	Loss 0.410
Epoch [219]: lr=1.000e-03
Epoch [219] train aveloss=0.326 aveacc=87.380
Test:Result* Prec@1 85.500	Loss 0.357
Test[219]:Result* Prec@1 85.500	Loss 0.357
Epoch [220]: lr=1.000e-03
Epoch [220] train aveloss=0.329 aveacc=87.250
Test:Result* Prec@1 85.700	Loss 0.359
Test[220]:Result* Prec@1 85.700	Loss 0.359
Epoch [221]: lr=1.000e-03
Epoch [221] train aveloss=0.317 aveacc=88.040
Test:Result* Prec@1 87.800	Loss 0.331
Test[221]:Result* Prec@1 87.800	Loss 0.331
Epoch [222]: lr=1.000e-03
Epoch [222] train aveloss=0.325 aveacc=87.630
Test:Result* Prec@1 87.000	Loss 0.351
Test[222]:Result* Prec@1 87.000	Loss 0.351
Epoch [223]: lr=1.000e-03
Epoch [223] train aveloss=0.319 aveacc=87.400
Test:Result* Prec@1 83.100	Loss 0.422
Test[223]:Result* Prec@1 83.100	Loss 0.422
Epoch [224]: lr=1.000e-03
Epoch [224] train aveloss=0.309 aveacc=88.150
Test:Result* Prec@1 83.800	Loss 0.416
Test[224]:Result* Prec@1 83.800	Loss 0.416
Epoch [225]: lr=1.000e-03
Epoch [225] train aveloss=0.306 aveacc=88.390
Test:Result* Prec@1 86.500	Loss 0.349
Test[225]:Result* Prec@1 86.500	Loss 0.349
Epoch [226]: lr=1.000e-03
Epoch [226] train aveloss=0.313 aveacc=87.900
Test:Result* Prec@1 77.300	Loss 0.844
Test[226]:Result* Prec@1 77.300	Loss 0.844
Epoch [227]: lr=1.000e-03
Epoch [227] train aveloss=0.316 aveacc=87.610
Test:Result* Prec@1 82.700	Loss 0.560
Test[227]:Result* Prec@1 82.700	Loss 0.560
Epoch [228]: lr=1.000e-03
Epoch [228] train aveloss=0.314 aveacc=87.790
Test:Result* Prec@1 85.900	Loss 0.375
Test[228]:Result* Prec@1 85.900	Loss 0.375
Epoch [229]: lr=1.000e-03
Epoch [229] train aveloss=0.311 aveacc=87.900
Test:Result* Prec@1 83.700	Loss 0.394
Test[229]:Result* Prec@1 83.700	Loss 0.394
Epoch [230]: lr=1.000e-03
Epoch [230] train aveloss=0.305 aveacc=87.930
Test:Result* Prec@1 84.500	Loss 0.392
Test[230]:Result* Prec@1 84.500	Loss 0.392
Epoch [231]: lr=1.000e-03
Epoch [231] train aveloss=0.305 aveacc=87.940
Test:Result* Prec@1 81.300	Loss 0.544
Test[231]:Result* Prec@1 81.300	Loss 0.544
Epoch [232]: lr=1.000e-03
Epoch [232] train aveloss=0.323 aveacc=88.110
Test:Result* Prec@1 62.900	Loss 2.373
Test[232]:Result* Prec@1 62.900	Loss 2.373
Epoch [233]: lr=1.000e-03
Epoch [233] train aveloss=0.317 aveacc=87.900
Test:Result* Prec@1 80.600	Loss 0.487
Test[233]:Result* Prec@1 80.600	Loss 0.487
Epoch [234]: lr=1.000e-03
Epoch [234] train aveloss=0.309 aveacc=87.800
Test:Result* Prec@1 82.700	Loss 0.433
Test[234]:Result* Prec@1 82.700	Loss 0.433
Epoch [235]: lr=1.000e-03
Epoch [235] train aveloss=0.309 aveacc=87.690
Test:Result* Prec@1 81.800	Loss 0.458
Test[235]:Result* Prec@1 81.800	Loss 0.458
Epoch [236]: lr=1.000e-03
Epoch [236] train aveloss=0.301 aveacc=88.520
Test:Result* Prec@1 84.900	Loss 0.389
Test[236]:Result* Prec@1 84.900	Loss 0.389
Epoch [237]: lr=1.000e-03
Epoch [237] train aveloss=0.300 aveacc=88.500
Test:Result* Prec@1 84.900	Loss 0.396
Test[237]:Result* Prec@1 84.900	Loss 0.396
Epoch [238]: lr=1.000e-03
Epoch [238] train aveloss=0.310 aveacc=88.200
Test:Result* Prec@1 84.200	Loss 0.434
Test[238]:Result* Prec@1 84.200	Loss 0.434
Epoch [239]: lr=1.000e-03
Epoch [239] train aveloss=0.311 aveacc=88.140
Test:Result* Prec@1 84.100	Loss 0.420
Test[239]:Result* Prec@1 84.100	Loss 0.420
Epoch [240]: lr=1.000e-03
Epoch [240] train aveloss=0.303 aveacc=88.230
Test:Result* Prec@1 83.700	Loss 0.436
Test[240]:Result* Prec@1 83.700	Loss 0.436
Epoch [241]: lr=1.000e-03
Epoch [241] train aveloss=0.306 aveacc=88.190
Test:Result* Prec@1 83.400	Loss 0.405
Test[241]:Result* Prec@1 83.400	Loss 0.405
Epoch [242]: lr=1.000e-03
Epoch [242] train aveloss=0.298 aveacc=88.600
Test:Result* Prec@1 80.700	Loss 0.486
Test[242]:Result* Prec@1 80.700	Loss 0.486
Epoch [243]: lr=1.000e-03
Epoch [243] train aveloss=0.308 aveacc=88.040
Test:Result* Prec@1 72.300	Loss 1.332
Test[243]:Result* Prec@1 72.300	Loss 1.332
Epoch [244]: lr=1.000e-03
Epoch [244] train aveloss=0.305 aveacc=88.320
Test:Result* Prec@1 85.400	Loss 0.377
Test[244]:Result* Prec@1 85.400	Loss 0.377
Epoch [245]: lr=1.000e-03
Epoch [245] train aveloss=0.307 aveacc=88.130
Test:Result* Prec@1 85.300	Loss 0.352
Test[245]:Result* Prec@1 85.300	Loss 0.352
Epoch [246]: lr=1.000e-03
Epoch [246] train aveloss=0.305 aveacc=88.470
Test:Result* Prec@1 82.600	Loss 0.453
Test[246]:Result* Prec@1 82.600	Loss 0.453
Epoch [247]: lr=1.000e-03
Epoch [247] train aveloss=0.312 aveacc=88.140
Test:Result* Prec@1 86.300	Loss 0.380
Test[247]:Result* Prec@1 86.300	Loss 0.380
Epoch [248]: lr=1.000e-03
Epoch [248] train aveloss=0.297 aveacc=88.770
Test:Result* Prec@1 80.800	Loss 0.581
Test[248]:Result* Prec@1 80.800	Loss 0.581
Epoch [249]: lr=1.000e-03
Epoch [249] train aveloss=0.302 aveacc=88.650
Test:Result* Prec@1 84.900	Loss 0.421
Test[249]:Result* Prec@1 84.900	Loss 0.421
Epoch [250]: lr=1.000e-03
Epoch [250] train aveloss=0.313 aveacc=87.540
Test:Result* Prec@1 86.400	Loss 0.353
Test[250]:Result* Prec@1 86.400	Loss 0.353
Epoch [251]: lr=1.000e-03
Epoch [251] train aveloss=0.302 aveacc=88.280
Test:Result* Prec@1 87.200	Loss 0.352
Test[251]:Result* Prec@1 87.200	Loss 0.352
Epoch [252]: lr=1.000e-03
Epoch [252] train aveloss=0.303 aveacc=88.160
Test:Result* Prec@1 86.300	Loss 0.347
Test[252]:Result* Prec@1 86.300	Loss 0.347
Epoch [253]: lr=1.000e-03
Epoch [253] train aveloss=0.305 aveacc=88.330
Test:Result* Prec@1 84.900	Loss 0.404
Test[253]:Result* Prec@1 84.900	Loss 0.404
Epoch [254]: lr=1.000e-03
Epoch [254] train aveloss=0.295 aveacc=88.380
Test:Result* Prec@1 86.700	Loss 0.338
Test[254]:Result* Prec@1 86.700	Loss 0.338
Epoch [255]: lr=1.000e-03
Epoch [255] train aveloss=0.301 aveacc=88.620
Test:Result* Prec@1 75.800	Loss 0.715
Test[255]:Result* Prec@1 75.800	Loss 0.715
Epoch [256]: lr=1.000e-03
Epoch [256] train aveloss=0.287 aveacc=88.700
Test:Result* Prec@1 83.300	Loss 0.535
Test[256]:Result* Prec@1 83.300	Loss 0.535
Epoch [257]: lr=1.000e-03
Epoch [257] train aveloss=0.312 aveacc=87.530
Test:Result* Prec@1 86.400	Loss 0.372
Test[257]:Result* Prec@1 86.400	Loss 0.372
Epoch [258]: lr=1.000e-03
Epoch [258] train aveloss=0.304 aveacc=88.250
Test:Result* Prec@1 84.400	Loss 0.399
Test[258]:Result* Prec@1 84.400	Loss 0.399
Epoch [259]: lr=1.000e-03
Epoch [259] train aveloss=0.291 aveacc=88.600
Test:Result* Prec@1 84.200	Loss 0.390
Test[259]:Result* Prec@1 84.200	Loss 0.390
Epoch [260]: lr=1.000e-03
Epoch [260] train aveloss=0.303 aveacc=88.200
Test:Result* Prec@1 82.300	Loss 0.443
Test[260]:Result* Prec@1 82.300	Loss 0.443
Epoch [261]: lr=1.000e-03
Epoch [261] train aveloss=0.289 aveacc=88.700
Test:Result* Prec@1 86.100	Loss 0.370
Test[261]:Result* Prec@1 86.100	Loss 0.370
Epoch [262]: lr=1.000e-03
Epoch [262] train aveloss=0.294 aveacc=88.740
Test:Result* Prec@1 83.300	Loss 0.439
Test[262]:Result* Prec@1 83.300	Loss 0.439
Epoch [263]: lr=1.000e-03
Epoch [263] train aveloss=0.295 aveacc=88.230
Test:Result* Prec@1 83.800	Loss 0.471
Test[263]:Result* Prec@1 83.800	Loss 0.471
Epoch [264]: lr=1.000e-03
Epoch [264] train aveloss=0.298 aveacc=88.440
Test:Result* Prec@1 85.900	Loss 0.362
Test[264]:Result* Prec@1 85.900	Loss 0.362
Epoch [265]: lr=1.000e-03
Epoch [265] train aveloss=0.302 aveacc=88.340
Test:Result* Prec@1 71.200	Loss 1.262
Test[265]:Result* Prec@1 71.200	Loss 1.262
Epoch [266]: lr=1.000e-03
Epoch [266] train aveloss=0.299 aveacc=88.300
Test:Result* Prec@1 89.600	Loss 0.286
Test[266]:Result* Prec@1 89.600	Loss 0.286
Epoch [267]: lr=1.000e-03
Epoch [267] train aveloss=0.283 aveacc=89.340
Test:Result* Prec@1 85.100	Loss 0.385
Test[267]:Result* Prec@1 85.100	Loss 0.385
Epoch [268]: lr=1.000e-03
Epoch [268] train aveloss=0.301 aveacc=88.490
Test:Result* Prec@1 83.300	Loss 0.473
Test[268]:Result* Prec@1 83.300	Loss 0.473
Epoch [269]: lr=1.000e-03
Epoch [269] train aveloss=0.279 aveacc=89.220
Test:Result* Prec@1 82.800	Loss 0.428
Test[269]:Result* Prec@1 82.800	Loss 0.428
Epoch [270]: lr=1.000e-03
Epoch [270] train aveloss=0.287 aveacc=89.050
Test:Result* Prec@1 86.400	Loss 0.357
Test[270]:Result* Prec@1 86.400	Loss 0.357
Epoch [271]: lr=1.000e-03
Epoch [271] train aveloss=0.280 aveacc=89.340
Test:Result* Prec@1 82.600	Loss 0.467
Test[271]:Result* Prec@1 82.600	Loss 0.467
Epoch [272]: lr=1.000e-03
Epoch [272] train aveloss=0.292 aveacc=88.680
Test:Result* Prec@1 87.200	Loss 0.330
Test[272]:Result* Prec@1 87.200	Loss 0.330
Epoch [273]: lr=1.000e-03
Epoch [273] train aveloss=0.302 aveacc=88.580
Test:Result* Prec@1 80.500	Loss 0.661
Test[273]:Result* Prec@1 80.500	Loss 0.661
Epoch [274]: lr=1.000e-03
Epoch [274] train aveloss=0.306 aveacc=88.310
Test:Result* Prec@1 84.500	Loss 0.402
Test[274]:Result* Prec@1 84.500	Loss 0.402
Epoch [275]: lr=1.000e-03
Epoch [275] train aveloss=0.305 aveacc=88.170
Test:Result* Prec@1 86.400	Loss 0.370
Test[275]:Result* Prec@1 86.400	Loss 0.370
Epoch [276]: lr=1.000e-03
Epoch [276] train aveloss=0.282 aveacc=89.160
Test:Result* Prec@1 85.200	Loss 0.454
Test[276]:Result* Prec@1 85.200	Loss 0.454
Epoch [277]: lr=1.000e-03
Epoch [277] train aveloss=0.290 aveacc=88.520
Test:Result* Prec@1 87.300	Loss 0.357
Test[277]:Result* Prec@1 87.300	Loss 0.357
Epoch [278]: lr=1.000e-03
Epoch [278] train aveloss=0.304 aveacc=88.470
Test:Result* Prec@1 80.400	Loss 0.498
Test[278]:Result* Prec@1 80.400	Loss 0.498
Epoch [279]: lr=1.000e-03
Epoch [279] train aveloss=0.295 aveacc=88.580
Test:Result* Prec@1 85.200	Loss 0.392
Test[279]:Result* Prec@1 85.200	Loss 0.392
Epoch [280]: lr=1.000e-03
Epoch [280] train aveloss=0.273 aveacc=89.770
Test:Result* Prec@1 85.100	Loss 0.380
Test[280]:Result* Prec@1 85.100	Loss 0.380
Epoch [281]: lr=1.000e-03
Epoch [281] train aveloss=0.290 aveacc=88.840
Test:Result* Prec@1 86.700	Loss 0.329
Test[281]:Result* Prec@1 86.700	Loss 0.329
Epoch [282]: lr=1.000e-03
Epoch [282] train aveloss=0.289 aveacc=88.640
Test:Result* Prec@1 81.900	Loss 0.502
Test[282]:Result* Prec@1 81.900	Loss 0.502
Epoch [283]: lr=1.000e-03
Epoch [283] train aveloss=0.292 aveacc=88.730
Test:Result* Prec@1 84.900	Loss 0.381
Test[283]:Result* Prec@1 84.900	Loss 0.381
Epoch [284]: lr=1.000e-03
Epoch [284] train aveloss=0.279 aveacc=89.300
Test:Result* Prec@1 80.400	Loss 0.624
Test[284]:Result* Prec@1 80.400	Loss 0.624
Epoch [285]: lr=1.000e-03
Epoch [285] train aveloss=0.281 aveacc=89.300
Test:Result* Prec@1 85.200	Loss 0.366
Test[285]:Result* Prec@1 85.200	Loss 0.366
Epoch [286]: lr=1.000e-03
Epoch [286] train aveloss=0.288 aveacc=88.830
Test:Result* Prec@1 87.700	Loss 0.336
Test[286]:Result* Prec@1 87.700	Loss 0.336
Epoch [287]: lr=1.000e-03
Epoch [287] train aveloss=0.298 aveacc=88.700
Test:Result* Prec@1 85.600	Loss 0.353
Test[287]:Result* Prec@1 85.600	Loss 0.353
Epoch [288]: lr=1.000e-03
Epoch [288] train aveloss=0.282 aveacc=89.340
Test:Result* Prec@1 81.000	Loss 0.559
Test[288]:Result* Prec@1 81.000	Loss 0.559
Epoch [289]: lr=1.000e-03
Epoch [289] train aveloss=0.287 aveacc=89.080
Test:Result* Prec@1 87.800	Loss 0.337
Test[289]:Result* Prec@1 87.800	Loss 0.337
Epoch [290]: lr=1.000e-03
Epoch [290] train aveloss=0.288 aveacc=89.000
Test:Result* Prec@1 83.300	Loss 0.475
Test[290]:Result* Prec@1 83.300	Loss 0.475
Epoch [291]: lr=1.000e-03
Epoch [291] train aveloss=0.304 aveacc=88.260
Test:Result* Prec@1 87.400	Loss 0.346
Test[291]:Result* Prec@1 87.400	Loss 0.346
Epoch [292]: lr=1.000e-03
Epoch [292] train aveloss=0.285 aveacc=88.960
Test:Result* Prec@1 84.600	Loss 0.392
Test[292]:Result* Prec@1 84.600	Loss 0.392
Epoch [293]: lr=1.000e-03
Epoch [293] train aveloss=0.274 aveacc=89.320
Test:Result* Prec@1 86.400	Loss 0.357
Test[293]:Result* Prec@1 86.400	Loss 0.357
Epoch [294]: lr=1.000e-03
Epoch [294] train aveloss=0.292 aveacc=88.870
Test:Result* Prec@1 68.500	Loss 2.262
Test[294]:Result* Prec@1 68.500	Loss 2.262
Epoch [295]: lr=1.000e-03
Epoch [295] train aveloss=0.283 aveacc=88.820
Test:Result* Prec@1 79.500	Loss 0.754
Test[295]:Result* Prec@1 79.500	Loss 0.754
Epoch [296]: lr=1.000e-03
Epoch [296] train aveloss=0.277 aveacc=89.010
Test:Result* Prec@1 77.500	Loss 0.615
Test[296]:Result* Prec@1 77.500	Loss 0.615
Epoch [297]: lr=1.000e-03
Epoch [297] train aveloss=0.286 aveacc=89.150
Test:Result* Prec@1 68.600	Loss 1.727
Test[297]:Result* Prec@1 68.600	Loss 1.727
Epoch [298]: lr=1.000e-03
Epoch [298] train aveloss=0.272 aveacc=89.400
Test:Result* Prec@1 83.900	Loss 0.413
Test[298]:Result* Prec@1 83.900	Loss 0.413
Epoch [299]: lr=1.000e-03
Epoch [299] train aveloss=0.276 aveacc=89.450
Test:Result* Prec@1 86.000	Loss 0.344
Test[299]:Result* Prec@1 86.000	Loss 0.344
Epoch [300]: lr=1.000e-03
Epoch [300] train aveloss=0.273 aveacc=89.460
Test:Result* Prec@1 85.900	Loss 0.392
Test[300]:Result* Prec@1 85.900	Loss 0.392
Epoch [301]: lr=1.000e-03
Epoch [301] train aveloss=0.280 aveacc=89.110
Test:Result* Prec@1 84.400	Loss 0.407
Test[301]:Result* Prec@1 84.400	Loss 0.407
Epoch [302]: lr=1.000e-03
Epoch [302] train aveloss=0.278 aveacc=89.470
Test:Result* Prec@1 86.500	Loss 0.356
Test[302]:Result* Prec@1 86.500	Loss 0.356
Epoch [303]: lr=1.000e-03
Epoch [303] train aveloss=0.267 aveacc=89.660
Test:Result* Prec@1 46.300	Loss 7.600
Test[303]:Result* Prec@1 46.300	Loss 7.600
Epoch [304]: lr=1.000e-03
Epoch [304] train aveloss=0.272 aveacc=89.510
Test:Result* Prec@1 87.300	Loss 0.344
Test[304]:Result* Prec@1 87.300	Loss 0.344
Epoch [305]: lr=1.000e-03
Epoch [305] train aveloss=0.280 aveacc=89.090
Test:Result* Prec@1 87.200	Loss 0.333
Test[305]:Result* Prec@1 87.200	Loss 0.333
Epoch [306]: lr=1.000e-03
Epoch [306] train aveloss=0.279 aveacc=89.110
Test:Result* Prec@1 72.600	Loss 1.749
Test[306]:Result* Prec@1 72.600	Loss 1.749
Epoch [307]: lr=1.000e-03
Epoch [307] train aveloss=0.290 aveacc=88.970
Test:Result* Prec@1 85.200	Loss 0.398
Test[307]:Result* Prec@1 85.200	Loss 0.398
Epoch [308]: lr=1.000e-03
Epoch [308] train aveloss=0.275 aveacc=89.780
Test:Result* Prec@1 84.700	Loss 0.429
Test[308]:Result* Prec@1 84.700	Loss 0.429
Epoch [309]: lr=1.000e-03
Epoch [309] train aveloss=0.281 aveacc=89.170
Test:Result* Prec@1 81.100	Loss 0.452
Test[309]:Result* Prec@1 81.100	Loss 0.452
Epoch [310]: lr=1.000e-03
Epoch [310] train aveloss=0.273 aveacc=89.090
Test:Result* Prec@1 86.000	Loss 0.381
Test[310]:Result* Prec@1 86.000	Loss 0.381
Epoch [311]: lr=1.000e-03
Epoch [311] train aveloss=0.272 aveacc=89.570
Test:Result* Prec@1 73.600	Loss 1.054
Test[311]:Result* Prec@1 73.600	Loss 1.054
Epoch [312]: lr=1.000e-03
Epoch [312] train aveloss=0.273 aveacc=89.830
Test:Result* Prec@1 89.300	Loss 0.309
Test[312]:Result* Prec@1 89.300	Loss 0.309
Epoch [313]: lr=1.000e-03
Epoch [313] train aveloss=0.269 aveacc=89.950
Test:Result* Prec@1 85.000	Loss 0.396
Test[313]:Result* Prec@1 85.000	Loss 0.396
Epoch [314]: lr=1.000e-03
Epoch [314] train aveloss=0.274 aveacc=89.370
Test:Result* Prec@1 86.300	Loss 0.376
Test[314]:Result* Prec@1 86.300	Loss 0.376
Epoch [315]: lr=1.000e-03
Epoch [315] train aveloss=0.282 aveacc=89.090
Test:Result* Prec@1 87.300	Loss 0.361
Test[315]:Result* Prec@1 87.300	Loss 0.361
Epoch [316]: lr=1.000e-03
Epoch [316] train aveloss=0.272 aveacc=89.780
Test:Result* Prec@1 82.600	Loss 0.476
Test[316]:Result* Prec@1 82.600	Loss 0.476
Epoch [317]: lr=1.000e-03
Epoch [317] train aveloss=0.273 aveacc=89.430
Test:Result* Prec@1 80.000	Loss 0.489
Test[317]:Result* Prec@1 80.000	Loss 0.489
Epoch [318]: lr=1.000e-03
Epoch [318] train aveloss=0.268 aveacc=90.000
Test:Result* Prec@1 85.700	Loss 0.349
Test[318]:Result* Prec@1 85.700	Loss 0.349
Epoch [319]: lr=1.000e-03
Epoch [319] train aveloss=0.269 aveacc=89.820
Test:Result* Prec@1 84.900	Loss 0.400
Test[319]:Result* Prec@1 84.900	Loss 0.400
Epoch [320]: lr=1.000e-03
Epoch [320] train aveloss=0.269 aveacc=89.710
Test:Result* Prec@1 85.300	Loss 0.405
Test[320]:Result* Prec@1 85.300	Loss 0.405
Epoch [321]: lr=1.000e-03
Epoch [321] train aveloss=0.265 aveacc=89.810
Test:Result* Prec@1 79.800	Loss 0.463
Test[321]:Result* Prec@1 79.800	Loss 0.463
Epoch [322]: lr=1.000e-03
Epoch [322] train aveloss=0.269 aveacc=89.720
Test:Result* Prec@1 87.100	Loss 0.332
Test[322]:Result* Prec@1 87.100	Loss 0.332
Epoch [323]: lr=1.000e-03
Epoch [323] train aveloss=0.265 aveacc=89.660
Test:Result* Prec@1 83.600	Loss 0.471
Test[323]:Result* Prec@1 83.600	Loss 0.471
Epoch [324]: lr=1.000e-03
Epoch [324] train aveloss=0.269 aveacc=90.080
Test:Result* Prec@1 85.600	Loss 0.387
Test[324]:Result* Prec@1 85.600	Loss 0.387
Epoch [325]: lr=1.000e-03
Epoch [325] train aveloss=0.275 aveacc=89.280
Test:Result* Prec@1 56.000	Loss 3.386
Test[325]:Result* Prec@1 56.000	Loss 3.386
Epoch [326]: lr=1.000e-03
Epoch [326] train aveloss=0.268 aveacc=89.830
Test:Result* Prec@1 84.300	Loss 0.401
Test[326]:Result* Prec@1 84.300	Loss 0.401
Epoch [327]: lr=1.000e-03
Epoch [327] train aveloss=0.266 aveacc=89.880
Test:Result* Prec@1 84.300	Loss 0.401
Test[327]:Result* Prec@1 84.300	Loss 0.401
Epoch [328]: lr=1.000e-03
Epoch [328] train aveloss=0.275 aveacc=89.200
Test:Result* Prec@1 77.800	Loss 0.620
Test[328]:Result* Prec@1 77.800	Loss 0.620
Epoch [329]: lr=1.000e-03
Epoch [329] train aveloss=0.265 aveacc=89.970
Test:Result* Prec@1 88.000	Loss 0.355
Test[329]:Result* Prec@1 88.000	Loss 0.355
Epoch [330]: lr=1.000e-03
Epoch [330] train aveloss=0.273 aveacc=89.460
Test:Result* Prec@1 84.500	Loss 0.388
Test[330]:Result* Prec@1 84.500	Loss 0.388
Epoch [331]: lr=1.000e-03
Epoch [331] train aveloss=0.262 aveacc=89.980
Test:Result* Prec@1 58.200	Loss 3.863
Test[331]:Result* Prec@1 58.200	Loss 3.863
Epoch [332]: lr=1.000e-03
Epoch [332] train aveloss=0.276 aveacc=89.350
Test:Result* Prec@1 86.100	Loss 0.406
Test[332]:Result* Prec@1 86.100	Loss 0.406
Epoch [333]: lr=1.000e-03
Epoch [333] train aveloss=0.261 aveacc=89.820
Test:Result* Prec@1 68.400	Loss 1.686
Test[333]:Result* Prec@1 68.400	Loss 1.686
Epoch [334]: lr=1.000e-03
Epoch [334] train aveloss=0.279 aveacc=89.340
Test:Result* Prec@1 76.300	Loss 0.673
Test[334]:Result* Prec@1 76.300	Loss 0.673
Epoch [335]: lr=1.000e-03
Epoch [335] train aveloss=0.266 aveacc=89.930
Test:Result* Prec@1 85.800	Loss 0.407
Test[335]:Result* Prec@1 85.800	Loss 0.407
Epoch [336]: lr=1.000e-03
Epoch [336] train aveloss=0.262 aveacc=89.880
Test:Result* Prec@1 57.100	Loss 4.300
Test[336]:Result* Prec@1 57.100	Loss 4.300
Epoch [337]: lr=1.000e-03
Epoch [337] train aveloss=0.266 aveacc=90.010
Test:Result* Prec@1 60.800	Loss 3.961
Test[337]:Result* Prec@1 60.800	Loss 3.961
Epoch [338]: lr=1.000e-03
Epoch [338] train aveloss=0.269 aveacc=89.440
Test:Result* Prec@1 84.900	Loss 0.423
Test[338]:Result* Prec@1 84.900	Loss 0.423
Epoch [339]: lr=1.000e-03
Epoch [339] train aveloss=0.268 aveacc=89.900
Test:Result* Prec@1 84.700	Loss 0.372
Test[339]:Result* Prec@1 84.700	Loss 0.372
Epoch [340]: lr=1.000e-03
Epoch [340] train aveloss=0.255 aveacc=90.260
Test:Result* Prec@1 58.200	Loss 2.503
Test[340]:Result* Prec@1 58.200	Loss 2.503
Epoch [341]: lr=1.000e-03
Epoch [341] train aveloss=0.250 aveacc=90.560
Test:Result* Prec@1 88.500	Loss 0.302
Test[341]:Result* Prec@1 88.500	Loss 0.302
Epoch [342]: lr=1.000e-03
Epoch [342] train aveloss=0.263 aveacc=89.900
Test:Result* Prec@1 52.300	Loss 4.327
Test[342]:Result* Prec@1 52.300	Loss 4.327
Epoch [343]: lr=1.000e-03
Epoch [343] train aveloss=0.271 aveacc=89.460
Test:Result* Prec@1 83.800	Loss 0.403
Test[343]:Result* Prec@1 83.800	Loss 0.403
Epoch [344]: lr=1.000e-03
Epoch [344] train aveloss=0.256 aveacc=90.010
Test:Result* Prec@1 79.900	Loss 0.538
Test[344]:Result* Prec@1 79.900	Loss 0.538
Epoch [345]: lr=1.000e-03
Epoch [345] train aveloss=0.256 aveacc=90.330
Test:Result* Prec@1 72.400	Loss 1.240
Test[345]:Result* Prec@1 72.400	Loss 1.240
Epoch [346]: lr=1.000e-03
Epoch [346] train aveloss=0.269 aveacc=90.110
Test:Result* Prec@1 89.300	Loss 0.293
Test[346]:Result* Prec@1 89.300	Loss 0.293
Epoch [347]: lr=1.000e-03
Epoch [347] train aveloss=0.256 aveacc=90.370
Test:Result* Prec@1 84.400	Loss 0.439
Test[347]:Result* Prec@1 84.400	Loss 0.439
Epoch [348]: lr=1.000e-03
Epoch [348] train aveloss=0.266 aveacc=89.800
Test:Result* Prec@1 85.400	Loss 0.359
Test[348]:Result* Prec@1 85.400	Loss 0.359
Epoch [349]: lr=1.000e-03
Epoch [349] train aveloss=0.265 aveacc=89.820
Test:Result* Prec@1 84.400	Loss 0.421
Test[349]:Result* Prec@1 84.400	Loss 0.421
Epoch [350]: lr=1.000e-03
Epoch [350] train aveloss=0.254 aveacc=90.290
Test:Result* Prec@1 85.800	Loss 0.378
Test[350]:Result* Prec@1 85.800	Loss 0.378
Epoch [351]: lr=1.000e-03
Epoch [351] train aveloss=0.249 aveacc=90.460
Test:Result* Prec@1 84.400	Loss 0.400
Test[351]:Result* Prec@1 84.400	Loss 0.400
Epoch [352]: lr=1.000e-03
Epoch [352] train aveloss=0.256 aveacc=90.420
Test:Result* Prec@1 87.300	Loss 0.330
Test[352]:Result* Prec@1 87.300	Loss 0.330
Epoch [353]: lr=1.000e-03
Epoch [353] train aveloss=0.259 aveacc=89.920
Test:Result* Prec@1 84.000	Loss 0.405
Test[353]:Result* Prec@1 84.000	Loss 0.405
Epoch [354]: lr=1.000e-03
Epoch [354] train aveloss=0.261 aveacc=90.100
Test:Result* Prec@1 84.800	Loss 0.375
Test[354]:Result* Prec@1 84.800	Loss 0.375
Epoch [355]: lr=1.000e-03
Epoch [355] train aveloss=0.264 aveacc=89.840
Test:Result* Prec@1 80.300	Loss 0.785
Test[355]:Result* Prec@1 80.300	Loss 0.785
Epoch [356]: lr=1.000e-03
Epoch [356] train aveloss=0.246 aveacc=90.700
Test:Result* Prec@1 85.100	Loss 0.399
Test[356]:Result* Prec@1 85.100	Loss 0.399
Epoch [357]: lr=1.000e-03
Epoch [357] train aveloss=0.250 aveacc=90.430
Test:Result* Prec@1 79.300	Loss 0.571
Test[357]:Result* Prec@1 79.300	Loss 0.571
Epoch [358]: lr=1.000e-03
Epoch [358] train aveloss=0.258 aveacc=90.240
Test:Result* Prec@1 86.100	Loss 0.379
Test[358]:Result* Prec@1 86.100	Loss 0.379
Epoch [359]: lr=1.000e-03
Epoch [359] train aveloss=0.259 aveacc=89.990
Test:Result* Prec@1 74.700	Loss 0.690
Test[359]:Result* Prec@1 74.700	Loss 0.690
Epoch [360]: lr=1.000e-03
Epoch [360] train aveloss=0.248 aveacc=90.570
Test:Result* Prec@1 79.700	Loss 0.547
Test[360]:Result* Prec@1 79.700	Loss 0.547
Epoch [361]: lr=1.000e-03
Epoch [361] train aveloss=0.251 aveacc=90.320
Test:Result* Prec@1 86.900	Loss 0.372
Test[361]:Result* Prec@1 86.900	Loss 0.372
Epoch [362]: lr=1.000e-03
Epoch [362] train aveloss=0.253 aveacc=90.370
Test:Result* Prec@1 70.400	Loss 1.405
Test[362]:Result* Prec@1 70.400	Loss 1.405
Epoch [363]: lr=1.000e-03
Epoch [363] train aveloss=0.242 aveacc=90.600
Test:Result* Prec@1 85.300	Loss 0.410
Test[363]:Result* Prec@1 85.300	Loss 0.410
Epoch [364]: lr=1.000e-03
Epoch [364] train aveloss=0.243 aveacc=91.130
Test:Result* Prec@1 74.200	Loss 0.873
Test[364]:Result* Prec@1 74.200	Loss 0.873
Epoch [365]: lr=1.000e-03
Epoch [365] train aveloss=0.250 aveacc=90.480
Test:Result* Prec@1 86.000	Loss 0.360
Test[365]:Result* Prec@1 86.000	Loss 0.360
Epoch [366]: lr=1.000e-03
Epoch [366] train aveloss=0.259 aveacc=90.110
Test:Result* Prec@1 84.200	Loss 0.401
Test[366]:Result* Prec@1 84.200	Loss 0.401
Epoch [367]: lr=1.000e-03
Epoch [367] train aveloss=0.257 aveacc=90.200
Test:Result* Prec@1 73.800	Loss 1.318
Test[367]:Result* Prec@1 73.800	Loss 1.318
Epoch [368]: lr=1.000e-03
Epoch [368] train aveloss=0.249 aveacc=90.730
Test:Result* Prec@1 85.000	Loss 0.419
Test[368]:Result* Prec@1 85.000	Loss 0.419
Epoch [369]: lr=1.000e-03
Epoch [369] train aveloss=0.244 aveacc=90.690
Test:Result* Prec@1 86.300	Loss 0.384
Test[369]:Result* Prec@1 86.300	Loss 0.384
Epoch [370]: lr=1.000e-03
Epoch [370] train aveloss=0.248 aveacc=90.760
Test:Result* Prec@1 86.400	Loss 0.368
Test[370]:Result* Prec@1 86.400	Loss 0.368
Epoch [371]: lr=1.000e-03
Epoch [371] train aveloss=0.243 aveacc=91.030
Test:Result* Prec@1 88.600	Loss 0.347
Test[371]:Result* Prec@1 88.600	Loss 0.347
Epoch [372]: lr=1.000e-03
Epoch [372] train aveloss=0.249 aveacc=90.270
Test:Result* Prec@1 64.800	Loss 1.248
Test[372]:Result* Prec@1 64.800	Loss 1.248
Epoch [373]: lr=1.000e-03
Epoch [373] train aveloss=0.246 aveacc=90.870
Test:Result* Prec@1 83.200	Loss 0.476
Test[373]:Result* Prec@1 83.200	Loss 0.476
Epoch [374]: lr=1.000e-03
Epoch [374] train aveloss=0.244 aveacc=90.960
Test:Result* Prec@1 56.000	Loss 2.860
Test[374]:Result* Prec@1 56.000	Loss 2.860
Epoch [375]: lr=1.000e-03
Epoch [375] train aveloss=0.250 aveacc=90.180
Test:Result* Prec@1 83.700	Loss 0.440
Test[375]:Result* Prec@1 83.700	Loss 0.440
Epoch [376]: lr=1.000e-03
Epoch [376] train aveloss=0.238 aveacc=91.020
Test:Result* Prec@1 64.800	Loss 3.449
Test[376]:Result* Prec@1 64.800	Loss 3.449
Epoch [377]: lr=1.000e-03
Epoch [377] train aveloss=0.254 aveacc=90.070
Test:Result* Prec@1 88.500	Loss 0.341
Test[377]:Result* Prec@1 88.500	Loss 0.341
Epoch [378]: lr=1.000e-03
Epoch [378] train aveloss=0.257 aveacc=90.430
Test:Result* Prec@1 83.400	Loss 0.452
Test[378]:Result* Prec@1 83.400	Loss 0.452
Epoch [379]: lr=1.000e-03
Epoch [379] train aveloss=0.243 aveacc=90.840
Test:Result* Prec@1 78.100	Loss 0.595
Test[379]:Result* Prec@1 78.100	Loss 0.595
Epoch [380]: lr=1.000e-03
Epoch [380] train aveloss=0.250 aveacc=90.560
Test:Result* Prec@1 80.200	Loss 0.544
Test[380]:Result* Prec@1 80.200	Loss 0.544
Epoch [381]: lr=1.000e-03
Epoch [381] train aveloss=0.237 aveacc=91.060
Test:Result* Prec@1 77.900	Loss 0.654
Test[381]:Result* Prec@1 77.900	Loss 0.654
Epoch [382]: lr=1.000e-03
Epoch [382] train aveloss=0.233 aveacc=91.340
Test:Result* Prec@1 84.200	Loss 0.405
Test[382]:Result* Prec@1 84.200	Loss 0.405
Epoch [383]: lr=1.000e-03
Epoch [383] train aveloss=0.233 aveacc=91.060
Test:Result* Prec@1 85.500	Loss 0.397
Test[383]:Result* Prec@1 85.500	Loss 0.397
Epoch [384]: lr=1.000e-03
Epoch [384] train aveloss=0.246 aveacc=90.690
Test:Result* Prec@1 74.700	Loss 0.882
Test[384]:Result* Prec@1 74.700	Loss 0.882
Epoch [385]: lr=1.000e-03
Epoch [385] train aveloss=0.249 aveacc=90.440
Test:Result* Prec@1 87.100	Loss 0.338
Test[385]:Result* Prec@1 87.100	Loss 0.338
Epoch [386]: lr=1.000e-03
Epoch [386] train aveloss=0.232 aveacc=91.100
Test:Result* Prec@1 88.400	Loss 0.313
Test[386]:Result* Prec@1 88.400	Loss 0.313
Epoch [387]: lr=1.000e-03
Epoch [387] train aveloss=0.249 aveacc=90.530
Test:Result* Prec@1 83.600	Loss 0.442
Test[387]:Result* Prec@1 83.600	Loss 0.442
Epoch [388]: lr=1.000e-03
Epoch [388] train aveloss=0.235 aveacc=91.120
Test:Result* Prec@1 74.900	Loss 1.073
Test[388]:Result* Prec@1 74.900	Loss 1.073
Epoch [389]: lr=1.000e-03
Epoch [389] train aveloss=0.247 aveacc=90.630
Test:Result* Prec@1 75.200	Loss 0.698
Test[389]:Result* Prec@1 75.200	Loss 0.698
Epoch [390]: lr=1.000e-03
Epoch [390] train aveloss=0.233 aveacc=91.090
Test:Result* Prec@1 87.000	Loss 0.355
Test[390]:Result* Prec@1 87.000	Loss 0.355
Epoch [391]: lr=1.000e-03
Epoch [391] train aveloss=0.241 aveacc=90.900
Test:Result* Prec@1 85.300	Loss 0.406
Test[391]:Result* Prec@1 85.300	Loss 0.406
Epoch [392]: lr=1.000e-03
Epoch [392] train aveloss=0.240 aveacc=90.900
Test:Result* Prec@1 64.600	Loss 2.227
Test[392]:Result* Prec@1 64.600	Loss 2.227
Epoch [393]: lr=1.000e-03
Epoch [393] train aveloss=0.232 aveacc=91.040
Test:Result* Prec@1 77.400	Loss 0.635
Test[393]:Result* Prec@1 77.400	Loss 0.635
Epoch [394]: lr=1.000e-03
Epoch [394] train aveloss=0.222 aveacc=91.300
Test:Result* Prec@1 79.600	Loss 0.618
Test[394]:Result* Prec@1 79.600	Loss 0.618
Epoch [395]: lr=1.000e-03
Epoch [395] train aveloss=0.235 aveacc=91.110
Test:Result* Prec@1 80.400	Loss 0.700
Test[395]:Result* Prec@1 80.400	Loss 0.700
Epoch [396]: lr=1.000e-03
Epoch [396] train aveloss=0.239 aveacc=90.990
Test:Result* Prec@1 85.900	Loss 0.397
Test[396]:Result* Prec@1 85.900	Loss 0.397
Epoch [397]: lr=1.000e-03
Epoch [397] train aveloss=0.230 aveacc=91.260
Test:Result* Prec@1 74.500	Loss 0.769
Test[397]:Result* Prec@1 74.500	Loss 0.769
Epoch [398]: lr=1.000e-03
Epoch [398] train aveloss=0.243 aveacc=91.130
Test:Result* Prec@1 77.200	Loss 0.672
Test[398]:Result* Prec@1 77.200	Loss 0.672
Epoch [399]: lr=1.000e-03
Epoch [399] train aveloss=0.231 aveacc=91.290
Test:Result* Prec@1 82.900	Loss 0.438
Test[399]:Result* Prec@1 82.900	Loss 0.438
Epoch [400]: lr=1.000e-03
Epoch [400] train aveloss=0.236 aveacc=91.180
Test:Result* Prec@1 83.700	Loss 0.491
Test[400]:Result* Prec@1 83.700	Loss 0.491
Epoch [401]: lr=1.000e-03
Epoch [401] train aveloss=0.236 aveacc=91.000
Test:Result* Prec@1 87.500	Loss 0.341
Test[401]:Result* Prec@1 87.500	Loss 0.341
Epoch [402]: lr=1.000e-03
Epoch [402] train aveloss=0.220 aveacc=91.740
Test:Result* Prec@1 71.800	Loss 1.175
Test[402]:Result* Prec@1 71.800	Loss 1.175
Epoch [403]: lr=1.000e-03
Epoch [403] train aveloss=0.229 aveacc=91.520
Test:Result* Prec@1 41.500	Loss 6.934
Test[403]:Result* Prec@1 41.500	Loss 6.934
Epoch [404]: lr=1.000e-03
Epoch [404] train aveloss=0.226 aveacc=91.660
Test:Result* Prec@1 37.900	Loss 11.160
Test[404]:Result* Prec@1 37.900	Loss 11.160
Epoch [405]: lr=1.000e-03
Epoch [405] train aveloss=0.239 aveacc=90.840
Test:Result* Prec@1 83.700	Loss 0.456
Test[405]:Result* Prec@1 83.700	Loss 0.456
Epoch [406]: lr=1.000e-03
Epoch [406] train aveloss=0.242 aveacc=90.910
Test:Result* Prec@1 80.000	Loss 0.620
Test[406]:Result* Prec@1 80.000	Loss 0.620
Epoch [407]: lr=1.000e-03
Epoch [407] train aveloss=0.231 aveacc=91.540
Test:Result* Prec@1 83.700	Loss 0.425
Test[407]:Result* Prec@1 83.700	Loss 0.425
Epoch [408]: lr=1.000e-03
Epoch [408] train aveloss=0.230 aveacc=91.280
Test:Result* Prec@1 72.400	Loss 1.100
Test[408]:Result* Prec@1 72.400	Loss 1.100
Epoch [409]: lr=1.000e-03
Epoch [409] train aveloss=0.233 aveacc=91.260
Test:Result* Prec@1 83.900	Loss 0.432
Test[409]:Result* Prec@1 83.900	Loss 0.432
Epoch [410]: lr=1.000e-03
Epoch [410] train aveloss=0.232 aveacc=91.300
Test:Result* Prec@1 52.500	Loss 6.349
Test[410]:Result* Prec@1 52.500	Loss 6.349
Epoch [411]: lr=1.000e-03
Epoch [411] train aveloss=0.242 aveacc=90.950
Test:Result* Prec@1 72.700	Loss 0.940
Test[411]:Result* Prec@1 72.700	Loss 0.940
Epoch [412]: lr=1.000e-03
Epoch [412] train aveloss=0.226 aveacc=91.550
Test:Result* Prec@1 84.400	Loss 0.443
Test[412]:Result* Prec@1 84.400	Loss 0.443
Epoch [413]: lr=1.000e-03
Epoch [413] train aveloss=0.230 aveacc=90.940
Test:Result* Prec@1 85.200	Loss 0.432
Test[413]:Result* Prec@1 85.200	Loss 0.432
Epoch [414]: lr=1.000e-03
Epoch [414] train aveloss=0.228 aveacc=91.190
Test:Result* Prec@1 50.900	Loss 4.841
Test[414]:Result* Prec@1 50.900	Loss 4.841
Epoch [415]: lr=1.000e-03
Epoch [415] train aveloss=0.224 aveacc=91.660
Test:Result* Prec@1 83.800	Loss 0.481
Test[415]:Result* Prec@1 83.800	Loss 0.481
Epoch [416]: lr=1.000e-03
Epoch [416] train aveloss=0.224 aveacc=91.400
Test:Result* Prec@1 53.800	Loss 4.397
Test[416]:Result* Prec@1 53.800	Loss 4.397
Epoch [417]: lr=1.000e-03
Epoch [417] train aveloss=0.218 aveacc=92.050
Test:Result* Prec@1 59.200	Loss 4.799
Test[417]:Result* Prec@1 59.200	Loss 4.799
Epoch [418]: lr=1.000e-03
Epoch [418] train aveloss=0.224 aveacc=91.280
Test:Result* Prec@1 85.000	Loss 0.423
Test[418]:Result* Prec@1 85.000	Loss 0.423
Epoch [419]: lr=1.000e-03
Epoch [419] train aveloss=0.230 aveacc=91.690
Test:Result* Prec@1 76.300	Loss 0.796
Test[419]:Result* Prec@1 76.300	Loss 0.796
Epoch [420]: lr=1.000e-03
Epoch [420] train aveloss=0.214 aveacc=91.840
Test:Result* Prec@1 73.200	Loss 1.248
Test[420]:Result* Prec@1 73.200	Loss 1.248
Epoch [421]: lr=1.000e-03
Epoch [421] train aveloss=0.219 aveacc=92.030
Test:Result* Prec@1 65.700	Loss 1.678
Test[421]:Result* Prec@1 65.700	Loss 1.678
Epoch [422]: lr=1.000e-03
Epoch [422] train aveloss=0.214 aveacc=92.050
Test:Result* Prec@1 62.900	Loss 2.700
Test[422]:Result* Prec@1 62.900	Loss 2.700
Epoch [423]: lr=1.000e-03
Epoch [423] train aveloss=0.212 aveacc=92.030
Test:Result* Prec@1 83.400	Loss 0.406
Test[423]:Result* Prec@1 83.400	Loss 0.406
Epoch [424]: lr=1.000e-03
Epoch [424] train aveloss=0.218 aveacc=91.770
Test:Result* Prec@1 85.500	Loss 0.365
Test[424]:Result* Prec@1 85.500	Loss 0.365
Epoch [425]: lr=1.000e-03
Epoch [425] train aveloss=0.231 aveacc=91.230
Test:Result* Prec@1 86.900	Loss 0.366
Test[425]:Result* Prec@1 86.900	Loss 0.366
Epoch [426]: lr=1.000e-03
Epoch [426] train aveloss=0.219 aveacc=91.620
Test:Result* Prec@1 88.300	Loss 0.319
Test[426]:Result* Prec@1 88.300	Loss 0.319
Epoch [427]: lr=1.000e-03
Epoch [427] train aveloss=0.236 aveacc=91.300
Test:Result* Prec@1 85.900	Loss 0.397
Test[427]:Result* Prec@1 85.900	Loss 0.397
Epoch [428]: lr=1.000e-03
Epoch [428] train aveloss=0.227 aveacc=91.660
Test:Result* Prec@1 76.600	Loss 0.729
Test[428]:Result* Prec@1 76.600	Loss 0.729
Epoch [429]: lr=1.000e-03
Epoch [429] train aveloss=0.222 aveacc=91.360
Test:Result* Prec@1 46.900	Loss 5.534
Test[429]:Result* Prec@1 46.900	Loss 5.534
Epoch [430]: lr=1.000e-03
Epoch [430] train aveloss=0.224 aveacc=91.280
Test:Result* Prec@1 75.600	Loss 0.968
Test[430]:Result* Prec@1 75.600	Loss 0.968
Epoch [431]: lr=1.000e-03
Epoch [431] train aveloss=0.227 aveacc=91.330
Test:Result* Prec@1 83.100	Loss 0.449
Test[431]:Result* Prec@1 83.100	Loss 0.449
Epoch [432]: lr=1.000e-03
Epoch [432] train aveloss=0.216 aveacc=91.870
Test:Result* Prec@1 88.200	Loss 0.380
Test[432]:Result* Prec@1 88.200	Loss 0.380
Epoch [433]: lr=1.000e-03
Epoch [433] train aveloss=0.215 aveacc=91.890
Test:Result* Prec@1 81.800	Loss 0.499
Test[433]:Result* Prec@1 81.800	Loss 0.499
Epoch [434]: lr=1.000e-03
Epoch [434] train aveloss=0.211 aveacc=92.070
Test:Result* Prec@1 87.800	Loss 0.362
Test[434]:Result* Prec@1 87.800	Loss 0.362
Epoch [435]: lr=1.000e-03
Epoch [435] train aveloss=0.212 aveacc=92.080
Test:Result* Prec@1 73.100	Loss 1.183
Test[435]:Result* Prec@1 73.100	Loss 1.183
Epoch [436]: lr=1.000e-03
Epoch [436] train aveloss=0.213 aveacc=92.160
Test:Result* Prec@1 84.600	Loss 0.474
Test[436]:Result* Prec@1 84.600	Loss 0.474
Epoch [437]: lr=1.000e-03
Epoch [437] train aveloss=0.206 aveacc=91.960
Test:Result* Prec@1 78.800	Loss 0.770
Test[437]:Result* Prec@1 78.800	Loss 0.770
Epoch [438]: lr=1.000e-03
Epoch [438] train aveloss=0.225 aveacc=91.200
Test:Result* Prec@1 86.500	Loss 0.384
Test[438]:Result* Prec@1 86.500	Loss 0.384
Epoch [439]: lr=1.000e-03
Epoch [439] train aveloss=0.213 aveacc=92.270
Test:Result* Prec@1 82.400	Loss 0.554
Test[439]:Result* Prec@1 82.400	Loss 0.554
Epoch [440]: lr=1.000e-03
Epoch [440] train aveloss=0.223 aveacc=91.660
Test:Result* Prec@1 80.900	Loss 0.519
Test[440]:Result* Prec@1 80.900	Loss 0.519
Epoch [441]: lr=1.000e-03
Epoch [441] train aveloss=0.219 aveacc=91.810
Test:Result* Prec@1 86.900	Loss 0.381
Test[441]:Result* Prec@1 86.900	Loss 0.381
Epoch [442]: lr=1.000e-03
Epoch [442] train aveloss=0.225 aveacc=91.720
Test:Result* Prec@1 43.100	Loss 9.824
Test[442]:Result* Prec@1 43.100	Loss 9.824
Epoch [443]: lr=1.000e-03
Epoch [443] train aveloss=0.211 aveacc=92.300
Test:Result* Prec@1 84.400	Loss 0.425
Test[443]:Result* Prec@1 84.400	Loss 0.425
Epoch [444]: lr=1.000e-03
Epoch [444] train aveloss=0.225 aveacc=91.690
Test:Result* Prec@1 82.200	Loss 0.482
Test[444]:Result* Prec@1 82.200	Loss 0.482
Epoch [445]: lr=1.000e-03
Epoch [445] train aveloss=0.217 aveacc=91.680
Test:Result* Prec@1 85.800	Loss 0.364
Test[445]:Result* Prec@1 85.800	Loss 0.364
Epoch [446]: lr=1.000e-03
Epoch [446] train aveloss=0.209 aveacc=92.220
Test:Result* Prec@1 79.700	Loss 0.518
Test[446]:Result* Prec@1 79.700	Loss 0.518
Epoch [447]: lr=1.000e-03
Epoch [447] train aveloss=0.213 aveacc=92.250
Test:Result* Prec@1 48.700	Loss 5.823
Test[447]:Result* Prec@1 48.700	Loss 5.823
Epoch [448]: lr=1.000e-03
Epoch [448] train aveloss=0.218 aveacc=91.770
Test:Result* Prec@1 83.600	Loss 0.452
Test[448]:Result* Prec@1 83.600	Loss 0.452
Epoch [449]: lr=1.000e-03
Epoch [449] train aveloss=0.210 aveacc=92.220
Test:Result* Prec@1 59.700	Loss 4.269
Test[449]:Result* Prec@1 59.700	Loss 4.269
Epoch [450]: lr=1.000e-03
Epoch [450] train aveloss=0.207 aveacc=92.240
Test:Result* Prec@1 87.300	Loss 0.351
Test[450]:Result* Prec@1 87.300	Loss 0.351
Epoch [451]: lr=1.000e-03
Epoch [451] train aveloss=0.204 aveacc=92.310
Test:Result* Prec@1 85.900	Loss 0.415
Test[451]:Result* Prec@1 85.900	Loss 0.415
Epoch [452]: lr=1.000e-03
Epoch [452] train aveloss=0.202 aveacc=92.580
Test:Result* Prec@1 79.500	Loss 0.875
Test[452]:Result* Prec@1 79.500	Loss 0.875
Epoch [453]: lr=1.000e-03
Epoch [453] train aveloss=0.209 aveacc=92.260
Test:Result* Prec@1 82.600	Loss 0.478
Test[453]:Result* Prec@1 82.600	Loss 0.478
Epoch [454]: lr=1.000e-03
Epoch [454] train aveloss=0.202 aveacc=92.490
Test:Result* Prec@1 69.500	Loss 1.559
Test[454]:Result* Prec@1 69.500	Loss 1.559
Epoch [455]: lr=1.000e-03
Epoch [455] train aveloss=0.218 aveacc=91.620
Test:Result* Prec@1 84.300	Loss 0.417
Test[455]:Result* Prec@1 84.300	Loss 0.417
Epoch [456]: lr=1.000e-03
Epoch [456] train aveloss=0.204 aveacc=92.430
Test:Result* Prec@1 85.800	Loss 0.487
Test[456]:Result* Prec@1 85.800	Loss 0.487
Epoch [457]: lr=1.000e-03
Epoch [457] train aveloss=0.205 aveacc=92.070
Test:Result* Prec@1 85.200	Loss 0.422
Test[457]:Result* Prec@1 85.200	Loss 0.422
Epoch [458]: lr=1.000e-03
Epoch [458] train aveloss=0.211 aveacc=92.190
Test:Result* Prec@1 34.800	Loss 6.962
Test[458]:Result* Prec@1 34.800	Loss 6.962
Epoch [459]: lr=1.000e-03
Epoch [459] train aveloss=0.204 aveacc=92.360
Test:Result* Prec@1 82.300	Loss 0.496
Test[459]:Result* Prec@1 82.300	Loss 0.496
Epoch [460]: lr=1.000e-03
Epoch [460] train aveloss=0.213 aveacc=92.030
Test:Result* Prec@1 85.000	Loss 0.424
Test[460]:Result* Prec@1 85.000	Loss 0.424
Epoch [461]: lr=1.000e-03
Epoch [461] train aveloss=0.204 aveacc=92.610
Test:Result* Prec@1 83.900	Loss 0.540
Test[461]:Result* Prec@1 83.900	Loss 0.540
Epoch [462]: lr=1.000e-03
Epoch [462] train aveloss=0.216 aveacc=91.780
Test:Result* Prec@1 82.700	Loss 0.519
Test[462]:Result* Prec@1 82.700	Loss 0.519
Epoch [463]: lr=1.000e-03
Epoch [463] train aveloss=0.197 aveacc=92.510
Test:Result* Prec@1 84.900	Loss 0.406
Test[463]:Result* Prec@1 84.900	Loss 0.406
Epoch [464]: lr=1.000e-03
Epoch [464] train aveloss=0.203 aveacc=92.550
Test:Result* Prec@1 86.200	Loss 0.392
Test[464]:Result* Prec@1 86.200	Loss 0.392
Epoch [465]: lr=1.000e-03
Epoch [465] train aveloss=0.208 aveacc=92.380
Test:Result* Prec@1 71.900	Loss 1.535
Test[465]:Result* Prec@1 71.900	Loss 1.535
Epoch [466]: lr=1.000e-03
Epoch [466] train aveloss=0.203 aveacc=92.580
Test:Result* Prec@1 86.500	Loss 0.377
Test[466]:Result* Prec@1 86.500	Loss 0.377
Epoch [467]: lr=1.000e-03
Epoch [467] train aveloss=0.198 aveacc=92.460
Test:Result* Prec@1 70.400	Loss 1.050
Test[467]:Result* Prec@1 70.400	Loss 1.050
Epoch [468]: lr=1.000e-03
Epoch [468] train aveloss=0.197 aveacc=92.710
Test:Result* Prec@1 51.600	Loss 4.418
Test[468]:Result* Prec@1 51.600	Loss 4.418
Epoch [469]: lr=1.000e-03
Epoch [469] train aveloss=0.201 aveacc=92.530
Test:Result* Prec@1 84.400	Loss 0.446
Test[469]:Result* Prec@1 84.400	Loss 0.446
Epoch [470]: lr=1.000e-03
Epoch [470] train aveloss=0.196 aveacc=92.870
Test:Result* Prec@1 87.200	Loss 0.364
Test[470]:Result* Prec@1 87.200	Loss 0.364
Epoch [471]: lr=1.000e-03
Epoch [471] train aveloss=0.214 aveacc=92.110
Test:Result* Prec@1 73.900	Loss 0.925
Test[471]:Result* Prec@1 73.900	Loss 0.925
Epoch [472]: lr=1.000e-03
Epoch [472] train aveloss=0.198 aveacc=92.720
Test:Result* Prec@1 82.600	Loss 0.581
Test[472]:Result* Prec@1 82.600	Loss 0.581
Epoch [473]: lr=1.000e-03
Epoch [473] train aveloss=0.204 aveacc=92.250
Test:Result* Prec@1 73.600	Loss 1.049
Test[473]:Result* Prec@1 73.600	Loss 1.049
Epoch [474]: lr=1.000e-03
Epoch [474] train aveloss=0.208 aveacc=92.310
Test:Result* Prec@1 63.600	Loss 2.152
Test[474]:Result* Prec@1 63.600	Loss 2.152
Epoch [475]: lr=1.000e-03
Epoch [475] train aveloss=0.199 aveacc=92.540
Test:Result* Prec@1 81.900	Loss 0.724
Test[475]:Result* Prec@1 81.900	Loss 0.724
Epoch [476]: lr=1.000e-03
Epoch [476] train aveloss=0.194 aveacc=92.810
Test:Result* Prec@1 85.800	Loss 0.396
Test[476]:Result* Prec@1 85.800	Loss 0.396
Epoch [477]: lr=1.000e-03
Epoch [477] train aveloss=0.199 aveacc=92.670
Test:Result* Prec@1 83.300	Loss 0.512
Test[477]:Result* Prec@1 83.300	Loss 0.512
Epoch [478]: lr=1.000e-03
Epoch [478] train aveloss=0.204 aveacc=92.520
Test:Result* Prec@1 83.200	Loss 0.526
Test[478]:Result* Prec@1 83.200	Loss 0.526
Epoch [479]: lr=1.000e-03
Epoch [479] train aveloss=0.189 aveacc=93.260
Test:Result* Prec@1 82.800	Loss 0.537
Test[479]:Result* Prec@1 82.800	Loss 0.537
Epoch [480]: lr=1.000e-03
Epoch [480] train aveloss=0.195 aveacc=92.590
Test:Result* Prec@1 86.400	Loss 0.401
Test[480]:Result* Prec@1 86.400	Loss 0.401
Epoch [481]: lr=1.000e-03
Epoch [481] train aveloss=0.199 aveacc=92.810
Test:Result* Prec@1 49.700	Loss 5.792
Test[481]:Result* Prec@1 49.700	Loss 5.792
Epoch [482]: lr=1.000e-03
Epoch [482] train aveloss=0.202 aveacc=92.470
Test:Result* Prec@1 69.400	Loss 1.258
Test[482]:Result* Prec@1 69.400	Loss 1.258
Epoch [483]: lr=1.000e-03
Epoch [483] train aveloss=0.190 aveacc=93.050
Test:Result* Prec@1 75.400	Loss 0.914
Test[483]:Result* Prec@1 75.400	Loss 0.914
Epoch [484]: lr=1.000e-03
Epoch [484] train aveloss=0.198 aveacc=92.760
Test:Result* Prec@1 85.600	Loss 0.479
Test[484]:Result* Prec@1 85.600	Loss 0.479
Epoch [485]: lr=1.000e-03
Epoch [485] train aveloss=0.201 aveacc=92.610
Test:Result* Prec@1 83.200	Loss 0.636
Test[485]:Result* Prec@1 83.200	Loss 0.636
Epoch [486]: lr=1.000e-03
Epoch [486] train aveloss=0.188 aveacc=92.760
Test:Result* Prec@1 42.100	Loss 7.551
Test[486]:Result* Prec@1 42.100	Loss 7.551
Epoch [487]: lr=1.000e-03
Epoch [487] train aveloss=0.188 aveacc=93.110
Test:Result* Prec@1 81.300	Loss 0.721
Test[487]:Result* Prec@1 81.300	Loss 0.721
Epoch [488]: lr=1.000e-03
Epoch [488] train aveloss=0.193 aveacc=92.980
Test:Result* Prec@1 84.700	Loss 0.456
Test[488]:Result* Prec@1 84.700	Loss 0.456
Epoch [489]: lr=1.000e-03
Epoch [489] train aveloss=0.201 aveacc=92.450
Test:Result* Prec@1 83.800	Loss 0.471
Test[489]:Result* Prec@1 83.800	Loss 0.471
Epoch [490]: lr=1.000e-03
Epoch [490] train aveloss=0.198 aveacc=92.670
Test:Result* Prec@1 84.700	Loss 0.413
Test[490]:Result* Prec@1 84.700	Loss 0.413
Epoch [491]: lr=1.000e-03
Epoch [491] train aveloss=0.189 aveacc=92.940
Test:Result* Prec@1 57.800	Loss 3.538
Test[491]:Result* Prec@1 57.800	Loss 3.538
Epoch [492]: lr=1.000e-03
Epoch [492] train aveloss=0.197 aveacc=92.670
Test:Result* Prec@1 72.700	Loss 1.395
Test[492]:Result* Prec@1 72.700	Loss 1.395
Epoch [493]: lr=1.000e-03
Epoch [493] train aveloss=0.196 aveacc=92.990
Test:Result* Prec@1 83.600	Loss 0.459
Test[493]:Result* Prec@1 83.600	Loss 0.459
Epoch [494]: lr=1.000e-03
Epoch [494] train aveloss=0.203 aveacc=92.570
Test:Result* Prec@1 85.500	Loss 0.406
Test[494]:Result* Prec@1 85.500	Loss 0.406
Epoch [495]: lr=1.000e-03
Epoch [495] train aveloss=0.196 aveacc=92.920
Test:Result* Prec@1 52.800	Loss 4.839
Test[495]:Result* Prec@1 52.800	Loss 4.839
Epoch [496]: lr=1.000e-03
Epoch [496] train aveloss=0.186 aveacc=93.180
Test:Result* Prec@1 85.900	Loss 0.366
Test[496]:Result* Prec@1 85.900	Loss 0.366
Epoch [497]: lr=1.000e-03
Epoch [497] train aveloss=0.190 aveacc=93.030
Test:Result* Prec@1 84.900	Loss 0.432
Test[497]:Result* Prec@1 84.900	Loss 0.432
Epoch [498]: lr=1.000e-03
Epoch [498] train aveloss=0.195 aveacc=92.660
Test:Result* Prec@1 53.300	Loss 4.273
Test[498]:Result* Prec@1 53.300	Loss 4.273
Epoch [499]: lr=1.000e-03
Epoch [499] train aveloss=0.194 aveacc=93.050
Test:Result* Prec@1 85.800	Loss 0.400
Test[499]:Result* Prec@1 85.800	Loss 0.400
Epoch [500]: lr=1.000e-03
Epoch [500] train aveloss=0.183 aveacc=93.360
Test:Result* Prec@1 82.300	Loss 0.606
Test[500]:Result* Prec@1 82.300	Loss 0.606
Epoch [501]: lr=1.000e-03
Epoch [501] train aveloss=0.188 aveacc=92.990
Test:Result* Prec@1 87.200	Loss 0.360
Test[501]:Result* Prec@1 87.200	Loss 0.360
Epoch [502]: lr=1.000e-03
Epoch [502] train aveloss=0.192 aveacc=93.120
Test:Result* Prec@1 78.000	Loss 0.755
Test[502]:Result* Prec@1 78.000	Loss 0.755
Epoch [503]: lr=1.000e-03
Epoch [503] train aveloss=0.187 aveacc=93.290
Test:Result* Prec@1 57.000	Loss 3.370
Test[503]:Result* Prec@1 57.000	Loss 3.370
Epoch [504]: lr=1.000e-03
Epoch [504] train aveloss=0.175 aveacc=93.540
Test:Result* Prec@1 84.400	Loss 0.451
Test[504]:Result* Prec@1 84.400	Loss 0.451
Epoch [505]: lr=1.000e-03
Epoch [505] train aveloss=0.183 aveacc=93.380
Test:Result* Prec@1 86.800	Loss 0.377
Test[505]:Result* Prec@1 86.800	Loss 0.377
Epoch [506]: lr=1.000e-03
Epoch [506] train aveloss=0.189 aveacc=92.620
Test:Result* Prec@1 88.700	Loss 0.333
Test[506]:Result* Prec@1 88.700	Loss 0.333
Epoch [507]: lr=1.000e-03
Epoch [507] train aveloss=0.183 aveacc=93.070
Test:Result* Prec@1 84.000	Loss 0.494
Test[507]:Result* Prec@1 84.000	Loss 0.494
Epoch [508]: lr=1.000e-03
Epoch [508] train aveloss=0.195 aveacc=92.820
Test:Result* Prec@1 65.500	Loss 2.248
Test[508]:Result* Prec@1 65.500	Loss 2.248
Epoch [509]: lr=1.000e-03
Epoch [509] train aveloss=0.176 aveacc=93.580
Test:Result* Prec@1 81.700	Loss 0.575
Test[509]:Result* Prec@1 81.700	Loss 0.575
Epoch [510]: lr=1.000e-03
Epoch [510] train aveloss=0.188 aveacc=93.010
Test:Result* Prec@1 87.400	Loss 0.343
Test[510]:Result* Prec@1 87.400	Loss 0.343
Epoch [511]: lr=1.000e-03
Epoch [511] train aveloss=0.182 aveacc=93.120
Test:Result* Prec@1 84.400	Loss 0.449
Test[511]:Result* Prec@1 84.400	Loss 0.449
Epoch [512]: lr=1.000e-03
Epoch [512] train aveloss=0.179 aveacc=93.480
Test:Result* Prec@1 30.300	Loss 8.550
Test[512]:Result* Prec@1 30.300	Loss 8.550
Epoch [513]: lr=1.000e-03
Epoch [513] train aveloss=0.178 aveacc=93.370
Test:Result* Prec@1 85.900	Loss 0.492
Test[513]:Result* Prec@1 85.900	Loss 0.492
Epoch [514]: lr=1.000e-03
Epoch [514] train aveloss=0.164 aveacc=94.100
Test:Result* Prec@1 62.600	Loss 2.255
Test[514]:Result* Prec@1 62.600	Loss 2.255
Epoch [515]: lr=1.000e-03
Epoch [515] train aveloss=0.183 aveacc=93.240
Test:Result* Prec@1 85.500	Loss 0.426
Test[515]:Result* Prec@1 85.500	Loss 0.426
Epoch [516]: lr=1.000e-03
Epoch [516] train aveloss=0.177 aveacc=93.280
Test:Result* Prec@1 84.000	Loss 0.481
Test[516]:Result* Prec@1 84.000	Loss 0.481
Epoch [517]: lr=1.000e-03
Epoch [517] train aveloss=0.182 aveacc=93.180
Test:Result* Prec@1 86.200	Loss 0.443
Test[517]:Result* Prec@1 86.200	Loss 0.443
Epoch [518]: lr=1.000e-03
Epoch [518] train aveloss=0.184 aveacc=93.360
Test:Result* Prec@1 71.800	Loss 0.941
Test[518]:Result* Prec@1 71.800	Loss 0.941
Epoch [519]: lr=1.000e-03
Epoch [519] train aveloss=0.185 aveacc=92.890
Test:Result* Prec@1 83.500	Loss 0.483
Test[519]:Result* Prec@1 83.500	Loss 0.483
Epoch [520]: lr=1.000e-03
Epoch [520] train aveloss=0.176 aveacc=93.530
Test:Result* Prec@1 84.900	Loss 0.482
Test[520]:Result* Prec@1 84.900	Loss 0.482
Epoch [521]: lr=1.000e-03
Epoch [521] train aveloss=0.179 aveacc=93.270
Test:Result* Prec@1 67.400	Loss 1.898
Test[521]:Result* Prec@1 67.400	Loss 1.898
Epoch [522]: lr=1.000e-03
Epoch [522] train aveloss=0.182 aveacc=93.430
Test:Result* Prec@1 78.400	Loss 0.746
Test[522]:Result* Prec@1 78.400	Loss 0.746
Epoch [523]: lr=1.000e-03
Epoch [523] train aveloss=0.170 aveacc=93.970
Test:Result* Prec@1 59.800	Loss 2.541
Test[523]:Result* Prec@1 59.800	Loss 2.541
Epoch [524]: lr=1.000e-03
Epoch [524] train aveloss=0.179 aveacc=93.530
Test:Result* Prec@1 82.700	Loss 0.496
Test[524]:Result* Prec@1 82.700	Loss 0.496
Epoch [525]: lr=1.000e-03
Epoch [525] train aveloss=0.183 aveacc=93.550
Test:Result* Prec@1 79.800	Loss 0.838
Test[525]:Result* Prec@1 79.800	Loss 0.838
Epoch [526]: lr=1.000e-03
Epoch [526] train aveloss=0.188 aveacc=92.880
Test:Result* Prec@1 82.000	Loss 0.509
Test[526]:Result* Prec@1 82.000	Loss 0.509
Epoch [527]: lr=1.000e-03
Epoch [527] train aveloss=0.180 aveacc=93.330
Test:Result* Prec@1 79.900	Loss 0.717
Test[527]:Result* Prec@1 79.900	Loss 0.717
Epoch [528]: lr=1.000e-03
Epoch [528] train aveloss=0.175 aveacc=93.560
Test:Result* Prec@1 80.300	Loss 0.701
Test[528]:Result* Prec@1 80.300	Loss 0.701
Epoch [529]: lr=1.000e-03
Epoch [529] train aveloss=0.187 aveacc=93.200
Test:Result* Prec@1 87.400	Loss 0.367
Test[529]:Result* Prec@1 87.400	Loss 0.367
Epoch [530]: lr=1.000e-03
Epoch [530] train aveloss=0.170 aveacc=93.920
Test:Result* Prec@1 85.600	Loss 0.429
Test[530]:Result* Prec@1 85.600	Loss 0.429
Epoch [531]: lr=1.000e-03
Epoch [531] train aveloss=0.183 aveacc=93.080
Test:Result* Prec@1 86.900	Loss 0.433
Test[531]:Result* Prec@1 86.900	Loss 0.433
Epoch [532]: lr=1.000e-03
Epoch [532] train aveloss=0.181 aveacc=93.150
Test:Result* Prec@1 50.700	Loss 5.359
Test[532]:Result* Prec@1 50.700	Loss 5.359
Epoch [533]: lr=1.000e-03
Epoch [533] train aveloss=0.170 aveacc=93.980
Test:Result* Prec@1 82.400	Loss 0.487
Test[533]:Result* Prec@1 82.400	Loss 0.487
Epoch [534]: lr=1.000e-03
Epoch [534] train aveloss=0.177 aveacc=93.390
Test:Result* Prec@1 85.000	Loss 0.435
Test[534]:Result* Prec@1 85.000	Loss 0.435
Epoch [535]: lr=1.000e-03
Epoch [535] train aveloss=0.166 aveacc=94.050
Test:Result* Prec@1 85.300	Loss 0.423
Test[535]:Result* Prec@1 85.300	Loss 0.423
Epoch [536]: lr=1.000e-03
Epoch [536] train aveloss=0.182 aveacc=93.140
Test:Result* Prec@1 82.400	Loss 0.575
Test[536]:Result* Prec@1 82.400	Loss 0.575
Epoch [537]: lr=1.000e-03
Epoch [537] train aveloss=0.171 aveacc=93.730
Test:Result* Prec@1 85.500	Loss 0.547
Test[537]:Result* Prec@1 85.500	Loss 0.547
Epoch [538]: lr=1.000e-03
Epoch [538] train aveloss=0.167 aveacc=94.130
Test:Result* Prec@1 82.700	Loss 0.540
Test[538]:Result* Prec@1 82.700	Loss 0.540
Epoch [539]: lr=1.000e-03
Epoch [539] train aveloss=0.168 aveacc=93.690
Test:Result* Prec@1 84.200	Loss 0.468
Test[539]:Result* Prec@1 84.200	Loss 0.468
Epoch [540]: lr=1.000e-03
Epoch [540] train aveloss=0.179 aveacc=93.460
Test:Result* Prec@1 81.700	Loss 0.572
Test[540]:Result* Prec@1 81.700	Loss 0.572
Epoch [541]: lr=1.000e-03
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_text output_error">
<pre>

<span class="ansi-red-fg">KeyboardInterrupt</span>Traceback (most recent call last)
<span class="ansi-green-fg"><ipython-input-16-349dee4afb51></span> in <span class="ansi-cyan-fg"><module></span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>     <span class="ansi-red-fg"># train for one epoch</span>
<span class="ansi-green-intense-fg ansi-bold">     27</span>     <span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">---> 28</span><span class="ansi-red-fg">         </span>train_ave_loss<span class="ansi-blue-fg">,</span> train_ave_acc <span class="ansi-blue-fg">=</span> train<span class="ansi-blue-fg">(</span>iotrain<span class="ansi-blue-fg">,</span> model<span class="ansi-blue-fg">,</span> criterion<span class="ansi-blue-fg">,</span> optimizer<span class="ansi-blue-fg">,</span> nbatches_per_epoch<span class="ansi-blue-fg">,</span> epoch<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     29</span>     <span class="ansi-green-fg">except</span> Exception<span class="ansi-blue-fg">,</span>e<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     30</span>         <span class="ansi-green-fg">print</span> <span class="ansi-blue-fg">"Error in training routine!"</span>

<span class="ansi-green-fg"><ipython-input-11-d14b336feafe></span> in <span class="ansi-cyan-fg">train</span><span class="ansi-blue-fg">(train_loader, model, criterion, optimizer, nbatches, epoch, print_freq)</span>
<span class="ansi-green-intense-fg ansi-bold">     29</span>             img_np<span class="ansi-blue-fg">[</span>j<span class="ansi-blue-fg">,</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span><span class="ansi-blue-fg">:</span><span class="ansi-blue-fg">,</span><span class="ansi-blue-fg">:</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> padandcropandflip<span class="ansi-blue-fg">(</span>imgtmp<span class="ansi-blue-fg">)</span> <span class="ansi-red-fg"># data augmentation</span>
<span class="ansi-green-intense-fg ansi-bold">     30</span>             lbl_np<span class="ansi-blue-fg">[</span>j<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> np<span class="ansi-blue-fg">.</span>argmax<span class="ansi-blue-fg">(</span>lbl<span class="ansi-blue-fg">[</span>j<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---> 31</span><span class="ansi-red-fg">         </span>input  <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>from_numpy<span class="ansi-blue-fg">(</span>img_np<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>cuda<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     32</span>         target <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>from_numpy<span class="ansi-blue-fg">(</span>lbl_np<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>cuda<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     33</span> 

<span class="ansi-green-fg">/home/taritree/.local/lib/python2.7/site-packages/torch/_utils.pyc</span> in <span class="ansi-cyan-fg">_cuda</span><span class="ansi-blue-fg">(self, device, async)</span>
<span class="ansi-green-intense-fg ansi-bold">     67</span>         <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     68</span>             new_type <span class="ansi-blue-fg">=</span> getattr<span class="ansi-blue-fg">(</span>torch<span class="ansi-blue-fg">.</span>cuda<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>__class__<span class="ansi-blue-fg">.</span>__name__<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---> 69</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">return</span> new_type<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>size<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>copy_<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> async<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     70</span> 
<span class="ansi-green-intense-fg ansi-bold">     71</span> 

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [18]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># once the training is over. stop the fillers</span>
<span class="n">iotrain</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
<span class="n">iovalid</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Observations-from-training">Observations from training<a class="anchor-link" href="#Observations-from-training">¶</a></h2><p>For the first 30 epochs (150 iterations), the training is going well. The average loss for the training data (blue) and validation data (red) are dropping steadily and doing so together.</p>
<p>After epoch 30, the training loss keeps lowering.  However, the training and validation losses are separating. The validation loss stops improving and becomes very variable.    These are both hallmarks of overtraining.</p>
<p>Looking at the standard output, the accuracy of the validation gets stuck at about 85%.  This is expected with our training data. The 15% of events involves images where the labels are inaccurate. For example, a proton interacts with a nucleus producing a bunch of photons. Or a muon decays early into an electron.  Refer to the blog post about version 0.1.0 of the open training data. This means we probably hit the accuracy limit.</p>
<p>This is why we saved a checkpoint every 50 iterations.  We can use the model saved at epoch 30.  In a subsequent post, we'll look at the performance of that model.</p>
</div>
</div>
</div>

<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

  </div>

  <div class="tag-cloud">
    <p>
      <a href="http://deeplearnphysics.org/Blog/tag/resnet.html">resnet</a>
      <a href="http://deeplearnphysics.org/Blog/tag/pytorch.html">pytorch</a>
      <a href="http://deeplearnphysics.org/Blog/tag/classification.html">classification</a>
      <a href="http://deeplearnphysics.org/Blog/tag/example.html">example</a>
    </p>
  </div>



</article>

    <footer>
<p>
  &copy; DeepLearnPhysics 2017 - This work is licensed under a <a rel="license" href="https://opensource.org/licenses/MIT">MIT License</a>
</p>
<p>
</p>    </footer>
  </main>





<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " DeepLearnPhysics Blog ",
  "url" : "http://deeplearnphysics.org/Blog",
  "image": "profile.png",
  "description": "description!"
}
</script>
</body>
</html>