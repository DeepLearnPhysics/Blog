
<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="http://deeplearnphysics.org/Blog/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="http://deeplearnphysics.org/Blog/theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="http://deeplearnphysics.org/Blog/theme/font-awesome/css/font-awesome.min.css">




  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="index, follow" />

    <!-- Chrome, Firefox OS and Opera -->
    <meta name="theme-color" content="#333">
    <!-- Windows Phone -->
    <meta name="msapplication-navbutton-color" content="#333">
    <!-- iOS Safari -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

<meta name="author" content="Taritree" />
<meta name="description" content="An example of training a classification network on the 5-particle LArCV training data using pytorch." />
<meta name="keywords" content="resnet, pytorch, classification, example">
<meta property="og:site_name" content="DeepLearnPhysics Blog"/>
<meta property="og:title" content="PyTorch/LArCV Classification Example with Data Set (v0.1.0)"/>
<meta property="og:description" content="An example of training a classification network on the 5-particle LArCV training data using pytorch."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="http://deeplearnphysics.org/Blog/pytorch-five-particle-example.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-01-09 00:00:00-06:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="http://deeplearnphysics.org/Blog/author/taritree.html">
<meta property="article:section" content="tutorial"/>
<meta property="article:tag" content="resnet"/>
<meta property="article:tag" content="pytorch"/>
<meta property="article:tag" content="classification"/>
<meta property="article:tag" content="example"/>
<meta property="og:image" content="profile.png">


<!-- Default meta cards for twitter -->
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@dlphysics">
<meta name="twitter:creator" content="@dlphysics">
<meta name="twitter:title" content="PyTorch/LArCV Classification Example with Data Set (v0.1.0)">
<meta name="twitter:description" content="<p>An example of training a classification network on the 5-particle LArCV training data using pytorch.</p>">
<meta name="twitter:image" content="http://deeplearnphysics.org/Blog/theme/img/profile_small.png" />


  <title>DeepLearnPhysics Blog &ndash; PyTorch/LArCV Classification Example with Data Set (v0.1.0)</title>
</head>
<body>
  <aside>
    <div>
      <a href="http://deeplearnphysics.org/Blog">
        <img src="http://deeplearnphysics.org/Blog/theme/img/profile.png" alt="Blog" title="Blog">
      </a>
      <h1><a href="http://deeplearnphysics.org/Blog">Blog</a></h1>

<p>DeepLearnPhysics Group</p>

      <ul class="social">
        <li><a class="sc-home" href="http://deeplearnphysics.org" target="_blank"><i class="fa fa-home"></i></a></li>
        <li><a class="sc-twitter" href="https://twitter.com/dlphysics" target="_blank"><i class="fa fa-twitter"></i></a></li>
        <li><a class="sc-github" href="http://github.com/DeepLearnPhysics" target="_blank"><i class="fa fa-github"></i></a></li>
      </ul>
    </div>

  </aside>
  <main>
    <nav>


      <a href="http://deeplearnphysics.org/Blog/index.html">Home</a>
      <a href="http://deeplearnphysics.org/Blog/categories.html">Category</a>
      <a href="http://deeplearnphysics.org/Blog/archives.html">Archives</a>
      <a href="http://deeplearnphysics.org/Blog/tags.html">Tags</a>
      <a href="http://deeplearnphysics.org/Blog/authors.html">Authors</a>


    </nav>

<article class="single">
  <header>
    <h1 id="pytorch-five-particle-example">PyTorch/LArCV Classification Example with Data Set (v0.1.0)</h1>
    <p>
          Posted on mar. 09 janvier 2018 in <a href="http://deeplearnphysics.org/Blog/category/tutorial.html">tutorial</a>

            by

              <a href="http://deeplearnphysics.org/Blog/author/taritree.html">Taritree</a>    </p>
  </header>

  <!-- script is a local library -->
  <link rel="stylesheet" type="text/css" href="http://deeplearnphysics.org/Blog/theme/stylesheet/kazunotebook.css">

  <div>
    <style type="text/css">/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}


.rendered_html pre,



.rendered_html tr,
.rendered_html th,

.rendered_html td,


.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,

div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
</style>
<style type="text/css">.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style>
<style type="text/css">
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }
</style>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<meta content="https://avatars0.githubusercontent.com/u/21003710?s=200&amp;v=4" name="twitter:image" />
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="PyTorch-Classification-Example">PyTorch Classification Example<a class="anchor-link" href="#PyTorch-Classification-Example">¶</a></h1><p>In this notebook, we're going to use ResNet-18 implemented in pyTorch to classify the 5-particle example training data.</p>
<p>This tutorial is meant to walk through some of the necessary steps to load images stored in LArCV files and train a network.  For more details on how to use pytorch, refer to the official pytorch tutorials.</p>
<p>This notebook will try to be self-contained in terms of code. 
However, you can find the code separated into different files in the following repositories</p>
<ul>
<li><a href="https://github.com/DeepLearnPhysics/larcvdataset">LArCVDataset</a>: concrete instance of pytorch Dataset class written for LArCV2 IO</li>
<li><a href="https://github.com/DeepLearnPhysics/pytorch-resnet-example">pytorch-classification-example</a>: many of the files and scripts found in this tutorial</li>
<li><a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py">pytorch resnet implementation</a>: is where we get our implementation with some slight modifications</li>
<li><a href="https://github.com/pytorch/examples/blob/master/imagenet/main.py">pytorch ImageNet training example</a>: is where we get the methods for training, again, with some slight modifications</li>
</ul>
<p>You will also need the training data. Go to the <a href="http://deeplearnphysics.org/DataChallenge/">open data page</a> and download the either the 5k or 50k training/validation samples.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [1]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># Import our modules</span>

<span class="c1"># python</span>
<span class="kn">import</span> <span class="nn">os</span><span class="o">,</span><span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">traceback</span>

<span class="c1"># numpy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># torch</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.parallel</span>
<span class="kn">import</span> <span class="nn">torch.backends.cudnn</span> <span class="kn">as</span> <span class="nn">cudnn</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="kn">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.optim</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span>
<span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="kn">as</span> <span class="nn">transforms</span>
<span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="kn">as</span> <span class="nn">datasets</span>
<span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="kn">as</span> <span class="nn">models</span>

<span class="c1"># ROOT/LArCV</span>
<span class="kn">import</span> <span class="nn">ROOT</span>
<span class="kn">from</span> <span class="nn">larcv</span> <span class="kn">import</span> <span class="n">larcv</span>

<span class="o">%</span><span class="k">matplotlib</span> notebook
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Welcome to JupyROOT 6.12/04
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Set-the-GPU-to-use">Set the GPU to use<a class="anchor-link" href="#Set-the-GPU-to-use">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [2]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span> <span class="mi">1</span> <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[2]:</div>
<div class="output_text output_subarea output_execute_result">
<pre><torch.cuda.device at 0x7f6d5c4d7ed0></pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Setup-Data-IO">Setup Data IO<a class="anchor-link" href="#Setup-Data-IO">¶</a></h1><h2 id="Location-of-data-on-your-local-machine">Location of data on your local machine<a class="anchor-link" href="#Location-of-data-on-your-local-machine">¶</a></h2><p>Set the path to the data files in this block.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [3]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">path_to_train_data</span><span class="o">=</span><span class="s2">"/home/taritree/working/dlphysics/testset/train_50k.root"</span>
<span class="n">path_to_test_data</span><span class="o">=</span><span class="s2">"/home/taritree/working/dlphysics/testset/test_40k.root"</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path_to_train_data</span><span class="p">):</span>
    <span class="k">print</span> <span class="s2">"Could not find the training data file."</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path_to_test_data</span><span class="p">):</span>
    <span class="k">print</span> <span class="s2">"Could not find the validation data file."</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Define-LArCVDataset">Define LArCVDataset<a class="anchor-link" href="#Define-LArCVDataset">¶</a></h2><p>First, we define a class that will load our data. There are many ways to do this. We create a concrete instance of pytorch's <code>Dataset</code> class, which can be used in the <code>DataLoader</code> class (which we do not use).</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [4]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># from: https://github.com/deeplearnphysics/larcvdataset</span>

<span class="n">larcv</span><span class="o">.</span><span class="n">PSet</span> <span class="c1"># touch this to force libBase to load, which has CreatePSetFromFile</span>
<span class="kn">from</span> <span class="nn">larcv.dataloader2</span> <span class="kn">import</span> <span class="n">larcv_threadio</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span> <span class="nc">LArCVDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="sd">""" LArCV data set interface for PyTorch"""</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">,</span> <span class="n">fillername</span><span class="p">,</span> <span class="n">verbosity</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">loadallinmem</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">randomize_inmem_data</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_inmem_events</span><span class="o">=-</span><span class="mi">1</span> <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbosity</span> <span class="o">=</span> <span class="n">verbosity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batchsize</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">randomize_inmem_data</span> <span class="o">=</span> <span class="n">randomize_inmem_data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_inmem_events</span> <span class="o">=</span> <span class="n">max_inmem_events</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loadallinmem</span> <span class="o">=</span> <span class="n">loadallinmem</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>  

        <span class="c1"># we setup the larcv threadfiller class, which handles io from larcv files</span>
        <span class="c1"># this follows steps from larcv tutorials</span>
        
        <span class="c1"># setup cfg dictionary needed for larcv_threadio      </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filler_cfg</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filler_cfg</span><span class="p">[</span><span class="s2">"filler_name"</span><span class="p">]</span> <span class="o">=</span> <span class="n">fillername</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filler_cfg</span><span class="p">[</span><span class="s2">"verbosity"</span><span class="p">]</span>   <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbosity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filler_cfg</span><span class="p">[</span><span class="s2">"filler_cfg"</span><span class="p">]</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Could not find filler configuration file: </span><span class="si">%s</span><span class="s2">"</span><span class="o">%</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">))</span>

        <span class="c1"># we read the first line of the config file, which should have name of config parameter set</span>
        <span class="n">linepset</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">,</span><span class="s1">'r'</span><span class="p">)</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cfgname</span> <span class="o">=</span> <span class="n">linepset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">":"</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        
        <span class="c1"># we load the pset ourselves, as we want access to values in 'ProcessName' list</span>
        <span class="c1"># will use these as the names of the data products loaded. store in self.datalist</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pset</span> <span class="o">=</span> <span class="n">larcv</span><span class="o">.</span><span class="n">CreatePSetFromFile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">cfgname</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"larcv::PSet"</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfgname</span><span class="p">)</span>
        <span class="n">datastr_v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pset</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"std::vector<std::string>"</span><span class="p">)(</span><span class="s2">"ProcessName"</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">datalist</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">datastr_v</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">datalist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">datastr_v</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        
        <span class="c1"># finally, configure io</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">io</span> <span class="o">=</span> <span class="n">larcv_threadio</span><span class="p">()</span>        
        <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">configure</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filler_cfg</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loadallinmem</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_loadinmem</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">loadallinmem</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">fetch_n_entries</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alldata</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">datalist</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">loadallinmem</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datalist</span><span class="p">:</span>
                <span class="n">out</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">fetch_data</span><span class="p">(</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span><span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batchsize</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datalist</span><span class="p">:</span>
                <span class="n">out</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchsize</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">alldata</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="bp">self</span><span class="o">.</span><span class="n">alldata</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="p">)</span>
                <span class="k">for</span> <span class="n">n</span><span class="p">,</span><span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">indices</span><span class="p">):</span>
                    <span class="n">out</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="n">n</span><span class="p">,:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alldata</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="n">idx</span><span class="p">,:]</span>
        <span class="k">return</span> <span class="n">out</span>
        
    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">dumpcfg</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">_loadinmem</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""load data into memory"""</span>
        <span class="n">nevents</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">fetch_n_entries</span><span class="p">())</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_inmem_events</span><span class="o">></span><span class="mi">0</span> <span class="ow">and</span> <span class="n">nevents</span><span class="o">></span><span class="bp">self</span><span class="o">.</span><span class="n">max_inmem_events</span><span class="p">:</span>
            <span class="n">nevents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_inmem_events</span>

        <span class="k">print</span> <span class="s2">"Attempting to load all "</span><span class="p">,</span><span class="n">nevents</span><span class="p">,</span><span class="s2">" into memory. good luck"</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="c1"># start threadio</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># get one data element to get shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
        <span class="n">firstout</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datalist</span><span class="p">:</span>
            <span class="n">firstout</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">fetch_data</span><span class="p">(</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alldata</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datalist</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alldata</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="p">(</span><span class="n">nevents</span><span class="p">,</span><span class="n">firstout</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">firstout</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alldata</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">firstout</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="mi">0</span><span class="p">,:]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">nevents</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="k">1000</span>==0:
                <span class="k">print</span> <span class="s2">"loading event </span><span class="si">%d</span><span class="s2"> of </span><span class="si">%d</span><span class="s2">"</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">nevents</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datalist</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">fetch_data</span><span class="p">(</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">alldata</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>

        <span class="k">print</span> <span class="s2">"elapsed time to bring data into memory: "</span><span class="p">,</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">,</span><span class="s2">"sec"</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">batchsize</span><span class="p">):</span>
        <span class="sd">"""exposes larcv_threadio::start which is used to start the thread managers"""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batchsize</span> <span class="o">=</span> <span class="n">batchsize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">start_manager</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchsize</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">stop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">""" stops the thread managers"""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">stop_manager</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">dumpcfg</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""dump the configuration file to a string"""</span>
        <span class="k">print</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
        
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Write-configuration-files-for-the-LArCV-ThreadFiller-class">Write configuration files for the LArCV ThreadFiller class<a class="anchor-link" href="#Write-configuration-files-for-the-LArCV-ThreadFiller-class">¶</a></h2><p>We define the configurations in this block, then write to file. We will load the files later when we create LArCVDataset instances for both the training and test data.</p>
<p>A note: the configurations need to have a separate name. Also, the <code>ProcessNames</code> have to be different. This is because of the way the threads are managed.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [5]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">train_cfg</span><span class="o">=</span><span class="s2">"""ThreadProcessor: {</span>
<span class="s2">  Verbosity:3</span>
<span class="s2">  NumThreads: 3</span>
<span class="s2">  NumBatchStorage: 3</span>
<span class="s2">  RandomAccess: true</span>
<span class="s2">  InputFiles: ["</span><span class="si">%s</span><span class="s2">"]  </span>
<span class="s2">  ProcessName: ["image","label"]</span>
<span class="s2">  ProcessType: ["BatchFillerImage2D","BatchFillerPIDLabel"]</span>
<span class="s2">  ProcessList: {</span>
<span class="s2">    image: {</span>
<span class="s2">      Verbosity:3</span>
<span class="s2">      ImageProducer: "data"</span>
<span class="s2">      Channels: [2]</span>
<span class="s2">      EnableMirror: true</span>
<span class="s2">    }</span>
<span class="s2">    label: {</span>
<span class="s2">      Verbosity:3</span>
<span class="s2">      ParticleProducer: "mctruth"</span>
<span class="s2">      PdgClassList: [2212,11,211,13,22]</span>
<span class="s2">    }</span>
<span class="s2">  }</span>
<span class="s2">}</span>
<span class="s2">"""</span><span class="o">%</span><span class="p">(</span><span class="n">path_to_train_data</span><span class="p">)</span>

<span class="n">test_cfg</span><span class="o">=</span><span class="s2">"""ThreadProcessorTest: {</span>
<span class="s2">  Verbosity:3</span>
<span class="s2">  NumThreads: 2</span>
<span class="s2">  NumBatchStorage: 2</span>
<span class="s2">  RandomAccess: true</span>
<span class="s2">  InputFiles: ["</span><span class="si">%s</span><span class="s2">"]</span>
<span class="s2">  ProcessName: ["imagetest","labeltest"]</span>
<span class="s2">  ProcessType: ["BatchFillerImage2D","BatchFillerPIDLabel"]</span>
<span class="s2">  ProcessList: {</span>
<span class="s2">    imagetest: {</span>
<span class="s2">      Verbosity:3</span>
<span class="s2">      ImageProducer: "data"</span>
<span class="s2">      Channels: [2]</span>
<span class="s2">      EnableMirror: false</span>
<span class="s2">    }</span>
<span class="s2">    labeltest: {</span>
<span class="s2">      Verbosity:3</span>
<span class="s2">      ParticleProducer: "mctruth"</span>
<span class="s2">      PdgClassList: [2212,11,211,13,22]</span>
<span class="s2">    }</span>
<span class="s2">  }</span>
<span class="s2">}</span>
<span class="s2">"""</span><span class="o">%</span><span class="p">(</span><span class="n">path_to_test_data</span><span class="p">)</span>

<span class="n">train_cfg_out</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"train_dataloader.cfg"</span><span class="p">,</span><span class="s1">'w'</span><span class="p">)</span>
<span class="k">print</span> <span class="o">>></span> <span class="n">train_cfg_out</span><span class="p">,</span><span class="n">train_cfg</span>
<span class="n">train_cfg_out</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">test_cfg_out</span>  <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"valid_dataloader.cfg"</span><span class="p">,</span><span class="s1">'w'</span><span class="p">)</span>
<span class="k">print</span> <span class="o">>></span> <span class="n">test_cfg_out</span><span class="p">,</span><span class="n">test_cfg</span>
<span class="n">test_cfg_out</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Setup-Network">Setup Network<a class="anchor-link" href="#Setup-Network">¶</a></h1><h2 id="Define-network">Define network<a class="anchor-link" href="#Define-network">¶</a></h2><p>We use ResNet-18 as implemented in the torchvision module.  We reproduce it here and make a slight modification: we change the number of input channels from 3 to 1.  The original resnet expects an RGB image.  For our example, we only use the image from one plane from our hypothetical LAr TPC detector.</p>
<p>Original can be found <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py">here</a>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [6]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># define convolution without bias that we will use throughout the network</span>
<span class="k">def</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">"""3x3 convolution with padding"""</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                     <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>


<span class="c1"># implements one ResNet unit</span>
<span class="k">class</span> <span class="nc">BasicBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">expansion</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">downsample</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BasicBlock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">conv3x3</span><span class="p">(</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">planes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">conv3x3</span><span class="p">(</span><span class="n">planes</span><span class="p">,</span> <span class="n">planes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">planes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span> <span class="o">=</span> <span class="n">downsample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">+=</span> <span class="n">residual</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>
    
<span class="c1"># define the network. It provides options for </span>
<span class="k">class</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">input_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        inputs</span>
<span class="sd">        ------</span>
<span class="sd">        block: type of resnet unit</span>
<span class="sd">        layers: list of 4 ints. defines number of basic block units in each set of resnet units</span>
<span class="sd">        num_classes: output classes</span>
<span class="sd">        input_channels: number of channels in input images</span>
<span class="sd">        """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inplanes</span> <span class="o">=</span> <span class="mi">64</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ResNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                               <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># had to change stride of avgpool from original from 1 to 2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># I've added dropout to the network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout2d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c1">#print "block.expansion=",block.expansion                                                                                                                                                           </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span> <span class="o">*</span> <span class="n">block</span><span class="o">.</span><span class="n">expansion</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
                <span class="n">n</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="o">.</span><span class="n">out_channels</span>
                <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="n">n</span><span class="p">))</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
                <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplanes</span> <span class="o">!=</span> <span class="n">planes</span> <span class="o">*</span> <span class="n">block</span><span class="o">.</span><span class="n">expansion</span><span class="p">:</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span> <span class="o">*</span> <span class="n">block</span><span class="o">.</span><span class="n">expansion</span><span class="p">,</span>
                          <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">planes</span> <span class="o">*</span> <span class="n">block</span><span class="o">.</span><span class="n">expansion</span><span class="p">),</span>
            <span class="p">)</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inplanes</span> <span class="o">=</span> <span class="n">planes</span> <span class="o">*</span> <span class="n">block</span><span class="o">.</span><span class="n">expansion</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1">#print "avepool: ",x.data.shape                                                                                                                                                                     </span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1">#print "view: ",x.data.shape                                                                                                                                                                        </span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>


    
<span class="c1"># define a helper function for ResNet-18</span>
<span class="k">def</span> <span class="nf">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">"""Constructs a ResNet-18 model.                                                                                                                                                                        </span>
<span class="sd">                                                                                                                                                                                                            </span>
<span class="sd">    Args:                                                                                                                                                                                                   </span>
<span class="sd">        pretrained (bool): If True, returns a model pre-trained on ImageNet                                                                                                                                 </span>
<span class="sd">    """</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ResNet</span><span class="p">(</span><span class="n">BasicBlock</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">pretrained</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_zoo</span><span class="o">.</span><span class="n">load_url</span><span class="p">(</span><span class="n">model_urls</span><span class="p">[</span><span class="s1">'resnet18'</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Create-instance-of-network">Create instance of network<a class="anchor-link" href="#Create-instance-of-network">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [7]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">input_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[7]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>ResNet(
  (conv1): Conv2d (1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1))
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d (64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (downsample): Sequential(
        (0): Conv2d (64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d (128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (downsample): Sequential(
        (0): Conv2d (128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (downsample): Sequential(
        (0): Conv2d (256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=2, padding=0, ceil_mode=False, count_include_pad=True)
  (dropout): Dropout2d(p=0.5, inplace)
  (fc): Linear(in_features=512, out_features=5)
)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Define-loss-function">Define loss function<a class="anchor-link" href="#Define-loss-function">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [8]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Define-optimizer-and-set-training-parameters">Define optimizer and set training parameters<a class="anchor-link" href="#Define-optimizer-and-set-training-parameters">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [9]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">1.0e-3</span>
<span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">1.0e-3</span>
<span class="n">batchsize</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">batchsize_valid</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">epochs</span>      <span class="o">=</span> <span class="mi">100</span>
<span class="n">nbatches_per_iteration</span> <span class="o">=</span> <span class="mi">10000</span><span class="o">/</span><span class="n">batchsize</span>
<span class="n">nbatches_per_valid</span>     <span class="o">=</span> <span class="mi">1000</span><span class="o">/</span><span class="n">batchsize_valid</span>

<span class="c1"># We use SGD</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Define-training-and-validation-steps">Define training and validation steps<a class="anchor-link" href="#Define-training-and-validation-steps">¶</a></h1><p>We define functions and classes to help us perform training.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Define-an-object-that-will-help-us-track-averages">Define an object that will help us track averages<a class="anchor-link" href="#Define-an-object-that-will-help-us-track-averages">¶</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [10]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="k">class</span> <span class="nc">AverageMeter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""Computes and stores the average and current value"""</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avg</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val</span> <span class="o">=</span> <span class="n">val</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sum</span> <span class="o">+=</span> <span class="n">val</span> <span class="o">*</span> <span class="n">n</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">+=</span> <span class="n">n</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sum</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">count</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training-step">Training step<a class="anchor-link" href="#Training-step">¶</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [11]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">nbatches</span><span class="p">,</span> <span class="n">iteration</span><span class="p">,</span> <span class="n">print_freq</span><span class="p">):</span>
    <span class="n">batch_time</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>
    <span class="n">data_time</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>
    <span class="n">format_time</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>
    <span class="n">train_time</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>
    <span class="n">top1</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>

    <span class="c1"># switch to train mode                                                                                                                                                                                  </span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nbatches</span><span class="p">):</span>                                                                                                                                                   
        <span class="n">batchstart</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">train_loader</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="c1"># measure data loading time                                                                                                                                                                         </span>
        <span class="n">data_time</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">end</span><span class="p">)</span>

        <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"image"</span><span class="p">]</span>
        <span class="n">lbl</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"label"</span><span class="p">]</span>
        <span class="n">img_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span> <span class="p">)</span>
        <span class="n">lbl_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="p">(</span><span class="n">lbl</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span> <span class="p">)</span>
        <span class="c1"># batch loop                                                                                                                                                                                        </span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">imgtmp</span> <span class="o">=</span> <span class="n">img</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span> <span class="p">(</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">)</span> <span class="p">)</span>
            <span class="n">img_np</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">,:,:]</span> <span class="o">=</span> <span class="n">padandcropandflip</span><span class="p">(</span><span class="n">imgtmp</span><span class="p">)</span> <span class="c1"># data augmentation                                                                                                                                 </span>
            <span class="n">lbl_np</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">lbl</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="nb">input</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">img_np</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">lbl_np</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

        <span class="c1"># measure data formatting time                                                                                                                                                                      </span>
        <span class="n">format_time</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">end</span><span class="p">)</span>

        <span class="c1"># convert into torch variable</span>
        <span class="n">input_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">target_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>

        <span class="c1"># compute output                                                                                                                                                                                    </span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_var</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target_var</span><span class="p">)</span>

        <span class="c1"># measure accuracy and record loss                                                                                                                                                                  </span>
        <span class="n">prec1</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">topk</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">top1</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">prec1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        
        <span class="c1"># compute gradient and do SGD step                                                                                                                                                                  </span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">train_time</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">end</span><span class="p">)</span>

        <span class="c1"># measure elapsed time                                                                                                                                                                              </span>
        <span class="n">batch_time</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">batchstart</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">print_freq</span><span class="o">></span><span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="n">print_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="p">(</span><span class="n">iteration</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">nbatches</span><span class="p">,</span>
                      <span class="n">batch_time</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">batch_time</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                      <span class="n">data_time</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">data_time</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                      <span class="n">format_time</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">format_time</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                      <span class="n">train_time</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">train_time</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                      <span class="n">losses</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">losses</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                      <span class="n">top1</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">top1</span><span class="o">.</span><span class="n">avg</span><span class="p">)</span>
            <span class="k">print</span> <span class="s2">"Iteration: [</span><span class="si">%d</span><span class="s2">][</span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2">]</span><span class="se">\t</span><span class="s2">Time </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)</span><span class="se">\t</span><span class="s2">Data </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)</span><span class="se">\t</span><span class="s2">Format </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)</span><span class="se">\t</span><span class="s2">Train </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)</span><span class="se">\t</span><span class="s2">Loss </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)</span><span class="se">\t</span><span class="s2">Prec@1 </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)"</span><span class="o">%</span><span class="k">status</span>
            
    <span class="k">return</span> <span class="n">losses</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span><span class="n">top1</span><span class="o">.</span><span class="n">avg</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Validation-step">Validation step<a class="anchor-link" href="#Validation-step">¶</a></h3><p>Here we process the test data and accumilate the accuracy.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [12]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="n">val_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">nbatches</span><span class="p">,</span> <span class="n">print_freq</span><span class="p">):</span>
    <span class="n">batch_time</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>
    <span class="n">top1</span> <span class="o">=</span> <span class="n">AverageMeter</span><span class="p">()</span>

    <span class="c1"># switch to evaluate mode                                                                                                                                                                               </span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nbatches</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">val_loader</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"imagetest"</span><span class="p">]</span>
        <span class="n">lbl</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"labeltest"</span><span class="p">]</span>
        <span class="n">img_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span> <span class="p">)</span>
        <span class="n">lbl_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="p">(</span><span class="n">lbl</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span> <span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">img_np</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">,:,:]</span> <span class="o">=</span> <span class="n">img</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span> <span class="p">(</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">)</span> <span class="p">)</span>
            <span class="n">lbl_np</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">lbl</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="nb">input</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">img_np</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">lbl_np</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

        <span class="c1"># convert into torch variable</span>
        <span class="n">input_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">volatile</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">target_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">volatile</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c1"># compute output                                                                                                                                                                                    </span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_var</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target_var</span><span class="p">)</span>

        <span class="c1"># measure accuracy and record loss                                                                                                                                                                  </span>
        <span class="n">prec1</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">topk</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">top1</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">prec1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

        <span class="c1"># measure elapsed time                                                                                                                                                                              </span>
        <span class="n">batch_time</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">end</span><span class="p">)</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">print_freq</span><span class="o">></span><span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="n">print_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">nbatches</span><span class="p">,</span><span class="n">batch_time</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">batch_time</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span><span class="n">losses</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">losses</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span><span class="n">top1</span><span class="o">.</span><span class="n">val</span><span class="p">,</span><span class="n">top1</span><span class="o">.</span><span class="n">avg</span><span class="p">)</span>
            <span class="k">print</span> <span class="s2">"Test: [</span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2">]</span><span class="se">\t</span><span class="s2">Time </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)</span><span class="se">\t</span><span class="s2">Loss </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)</span><span class="se">\t</span><span class="s2">Prec@1 </span><span class="si">%.3f</span><span class="s2"> (</span><span class="si">%.3f</span><span class="s2">)"</span><span class="o">%</span><span class="k">status</span>
 
    <span class="c1">#print "Test:Result* Prec@1 %.3f\tLoss %.3f"%(top1.avg,losses.avg)</span>
    
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">top1</span><span class="o">.</span><span class="n">avg</span><span class="p">),</span><span class="nb">float</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">avg</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="utility-functions">utility functions<a class="anchor-link" href="#utility-functions">¶</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [13]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="k">def</span> <span class="nf">adjust_learning_rate</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="sd">"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""</span>
    <span class="c1">#lr = lr * (0.5 ** (epoch // 300))                                                                                                                                                                      </span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="c1">#lr = lr*0.992                                                                                                                                                                                          </span>
    <span class="c1">#print "adjust learning rate to ",lr                                                                                                                                                                    </span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s1">'lr'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

<span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">topk</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,)):</span>
    <span class="sd">"""Computes the precision@k for the specified values of k"""</span>
    <span class="n">maxk</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">topk</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">pred</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">maxk</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">pred</span><span class="p">))</span>

    <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">topk</span><span class="p">:</span>
        <span class="n">correct_k</span> <span class="o">=</span> <span class="n">correct</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">correct_k</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mf">100.0</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">res</span>

<span class="k">def</span> <span class="nf">dump_lr_schedule</span><span class="p">(</span> <span class="n">startlr</span><span class="p">,</span> <span class="n">numepochs</span> <span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">numepochs</span><span class="p">):</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">startlr</span><span class="o">*</span><span class="p">(</span><span class="mf">0.5</span><span class="o">**</span><span class="p">(</span><span class="n">epoch</span><span class="o">//</span><span class="mi">300</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">epoch</span><span class="o">%</span><span class="k">10</span>==0:
            <span class="k">print</span> <span class="s2">"Epoch [</span><span class="si">%d</span><span class="s2">] lr=</span><span class="si">%.3e</span><span class="s2">"</span><span class="o">%</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span><span class="n">lr</span><span class="p">)</span>
    <span class="k">print</span> <span class="s2">"Epoch [</span><span class="si">%d</span><span class="s2">] lr=</span><span class="si">%.3e</span><span class="s2">"</span><span class="o">%</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span><span class="n">lr</span><span class="p">)</span>
    <span class="k">return</span>

<span class="k">def</span> <span class="nf">padandcropandflip</span><span class="p">(</span><span class="n">npimg2d</span><span class="p">):</span>
    <span class="n">imgpad</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="p">(</span><span class="mi">264</span><span class="p">,</span><span class="mi">264</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span> <span class="p">)</span>
    <span class="n">imgpad</span><span class="p">[</span><span class="mi">4</span><span class="p">:</span><span class="mi">256</span><span class="o">+</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">:</span><span class="mi">256</span><span class="o">+</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">npimg2d</span><span class="p">[:,:]</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span><span class="o">></span><span class="mf">0.5</span><span class="p">:</span>
        <span class="n">imgpad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span> <span class="n">imgpad</span><span class="p">,</span> <span class="mi">0</span> <span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span><span class="o">></span><span class="mf">0.5</span><span class="p">:</span>
        <span class="n">imgpad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span> <span class="n">imgpad</span><span class="p">,</span> <span class="mi">1</span> <span class="p">)</span>
    <span class="n">randx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">randy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">imgpad</span><span class="p">[</span><span class="n">randx</span><span class="p">:</span><span class="n">randx</span><span class="o">+</span><span class="mi">256</span><span class="p">,</span><span class="n">randy</span><span class="p">:</span><span class="n">randy</span><span class="o">+</span><span class="mi">256</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">is_best</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s1">'checkpoint.pth.tar'</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">></span><span class="mi">0</span><span class="p">:</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="s2">"checkpoint.</span><span class="si">%d</span><span class="s2">th.tar"</span><span class="o">%</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_best</span><span class="p">:</span>
        <span class="n">shutil</span><span class="o">.</span><span class="n">copyfile</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">'model_best.pth.tar'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Load-the-datasets-and-start-data-loading-threads">Load the datasets and start data loading threads<a class="anchor-link" href="#Load-the-datasets-and-start-data-loading-threads">¶</a></h1><h3 id="Training-data">Training data<a class="anchor-link" href="#Training-data">¶</a></h3><p>For the training data, we ask that all the data is loaded into memory. Since we need to get many, many batches to train the network, reducing the time to get a batch of images will pay off in the long run.</p>
<p>However, we first pay an upfront cost: this step takes a LONG time.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [14]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1">#capevents = 500 # first two lines are for debug: capping events to keep this step short</span>
<span class="c1">#iotrain = LArCVDataset("train_dataloader.cfg", "ThreadProcessor", loadallinmem=True, max_inmem_events=capevents)</span>
<span class="n">iotrain</span> <span class="o">=</span> <span class="n">LArCVDataset</span><span class="p">(</span><span class="s2">"train_dataloader.cfg"</span><span class="p">,</span> <span class="s2">"ThreadProcessor"</span><span class="p">,</span> <span class="n">loadallinmem</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">iotrain</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="n">batchsize</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Attempting to load all  50000  into memory. good luck
loading event 1000 of 50000
loading event 2000 of 50000
loading event 3000 of 50000
loading event 4000 of 50000
loading event 5000 of 50000
loading event 6000 of 50000
loading event 7000 of 50000
loading event 8000 of 50000
loading event 9000 of 50000
loading event 10000 of 50000
loading event 11000 of 50000
loading event 12000 of 50000
loading event 13000 of 50000
loading event 14000 of 50000
loading event 15000 of 50000
loading event 16000 of 50000
loading event 17000 of 50000
loading event 18000 of 50000
loading event 19000 of 50000
loading event 20000 of 50000
loading event 21000 of 50000
loading event 22000 of 50000
loading event 23000 of 50000
loading event 24000 of 50000
loading event 25000 of 50000
loading event 26000 of 50000
loading event 27000 of 50000
loading event 28000 of 50000
loading event 29000 of 50000
loading event 30000 of 50000
loading event 31000 of 50000
loading event 32000 of 50000
loading event 33000 of 50000
loading event 34000 of 50000
loading event 35000 of 50000
loading event 36000 of 50000
loading event 37000 of 50000
loading event 38000 of 50000
loading event 39000 of 50000
loading event 40000 of 50000
loading event 41000 of 50000
loading event 42000 of 50000
loading event 43000 of 50000
loading event 44000 of 50000
loading event 45000 of 50000
loading event 46000 of 50000
loading event 47000 of 50000
loading event 48000 of 50000
loading event 49000 of 50000
elapsed time to bring data into memory:  2608.85350108 sec
ThreadProcessor : {
  InputFiles : ["/home/taritree/working/dlphysics/testset/train_50k.root"]
  NumBatchStorage : 3
  NumThreads : 3
  ProcessName : ["image","label"]
  ProcessType : ["BatchFillerImage2D","BatchFillerPIDLabel"]
  RandomAccess : true
  Verbosity : 3
  ProcessList : {
    image : {
      Channels : [2]
      EnableMirror : true
      ImageProducer : "data"
      Verbosity : 3
    }

    label : {
      ParticleProducer : "mctruth"
      PdgClassList : [2212,11,211,13,22]
      Verbosity : 3
    }

  }

}

<span class="ansi-yellow-intense-fg"> setting verbosity </span>3
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>Error in <tprotoclass::finddatamember>: data member with index 0 is not found in class thread
Error in <createrealdata>: Cannot find data member # 0 of class thread for parent larcv::ThreadProcessor!
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Validation-data">Validation data<a class="anchor-link" href="#Validation-data">¶</a></h3><p>For the validation data, we do not load data into memory all at once. We will use the validation only periodically, in between many training steps. During those training steps, the thread filler will load data into memory.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [15]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">iovalid</span> <span class="o">=</span> <span class="n">LArCVDataset</span><span class="p">(</span><span class="s2">"valid_dataloader.cfg"</span><span class="p">,</span> <span class="s2">"ThreadProcessorTest"</span><span class="p">)</span>
<span class="n">iovalid</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="n">batchsize_valid</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre><span class="ansi-yellow-intense-fg"> setting verbosity </span>3
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Training-Loop">Training Loop<a class="anchor-link" href="#Training-Loop">¶</a></h1><p>For each iteration of the training loop, we</p>
<ul>
<li>set the learning rate</li>
<li>perform a training iteration which involves forward and backward passes for the number of batches set in <code>nbatches_per_iteration</code> </li>
<li>run a validation iteration over <code>nbatches_per_valid</code></li>
<li>for both the training and validation iterations, we save the average loss and average accuracy over the batches. Values are stored in a numpy array</li>
<li>we update the plot the training versus validation loss</li>
<li>every 10 epochs (i.e. 50 iterations), we save the state of the model and the optimizer</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [16]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">best_prec1</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="c1"># define plots</span>
<span class="n">fig</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'epoch'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'loss'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">5e-2</span><span class="p">,</span><span class="mf">10.0</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">"log"</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'epoch'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'accuracy'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">100.0</span><span class="p">)</span>

<span class="c1"># iterations:</span>
<span class="c1"># there are 50k events in the training set</span>
<span class="c1"># we use 10k images per training iteration</span>
<span class="c1"># therefore, 1 epoch is 5 iterations</span>
<span class="n">start_iteration</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">start_epoch</span>
<span class="n">end_iteration</span>   <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">epochs</span>
<span class="n">num_iterations</span>  <span class="o">=</span> <span class="n">end_iteration</span> <span class="o">-</span> <span class="n">start_iteration</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start_epoch</span><span class="p">,</span><span class="n">epochs</span><span class="p">,</span><span class="n">num_iterations</span><span class="p">)</span>

<span class="c1"># numpy arrays for loss and accuracy</span>
<span class="n">y_train_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">)</span>
<span class="n">y_train_acc</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">)</span>
<span class="n">y_valid_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">)</span>
<span class="n">y_valid_acc</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">)</span>


<span class="k">for</span> <span class="n">iiter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">num_iterations</span><span class="p">):</span>
    
    <span class="n">iteration</span> <span class="o">=</span> <span class="n">start_iteration</span> <span class="o">+</span> <span class="n">iiter</span>
    <span class="n">epoch</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">iteration</span><span class="p">)</span><span class="o">/</span><span class="mf">5.0</span>
    
    <span class="c1"># set the learning rate</span>
    <span class="n">adjust_learning_rate</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">iteration</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
    <span class="n">iterout</span> <span class="o">=</span> <span class="s2">"Iteration [</span><span class="si">%d</span><span class="s2">]: "</span><span class="o">%</span><span class="p">(</span><span class="n">iteration</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">iterout</span> <span class="o">+=</span> <span class="s2">"lr=</span><span class="si">%.3e</span><span class="s2">"</span><span class="o">%</span><span class="p">(</span><span class="n">param_group</span><span class="p">[</span><span class="s1">'lr'</span><span class="p">])</span>
    <span class="k">print</span> <span class="n">iterout</span>

    <span class="c1"># train for one iteration                                                                                                                                                                               </span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">train_ave_loss</span><span class="p">,</span> <span class="n">train_ave_acc</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">iotrain</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> 
                                              <span class="n">nbatches_per_iteration</span><span class="p">,</span> <span class="n">iteration</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">,</span><span class="n">e</span><span class="p">:</span>
        <span class="k">print</span> <span class="s2">"Error in training routine!"</span>
        <span class="k">print</span> <span class="n">e</span><span class="o">.</span><span class="n">message</span>
        <span class="k">print</span> <span class="n">e</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">traceback</span><span class="o">.</span><span class="n">print_exc</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="k">break</span>
    <span class="k">print</span> <span class="s2">"Iteration [</span><span class="si">%d</span><span class="s2">] train aveloss=</span><span class="si">%.3f</span><span class="s2"> aveacc=</span><span class="si">%.3f</span><span class="s2">"</span><span class="o">%</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span>
                                                           <span class="n">train_ave_loss</span><span class="p">,</span>
                                                           <span class="n">train_ave_acc</span><span class="p">)</span>
    <span class="n">y_train_loss</span><span class="p">[</span><span class="n">iiter</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_ave_loss</span>
    <span class="n">y_train_acc</span><span class="p">[</span><span class="n">iiter</span><span class="p">]</span>  <span class="o">=</span> <span class="n">train_ave_acc</span>

    <span class="c1"># evaluate on validation set                                                                                                                                                                        </span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">prec1</span><span class="p">,</span><span class="n">valid_loss</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">iovalid</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">nbatches_per_valid</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">,</span><span class="n">e</span><span class="p">:</span>
        <span class="k">print</span> <span class="s2">"Error in validation routine!"</span>
        <span class="k">print</span> <span class="n">e</span><span class="o">.</span><span class="n">message</span>
        <span class="k">print</span> <span class="n">e</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">traceback</span><span class="o">.</span><span class="n">print_exc</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="k">break</span>
    <span class="k">print</span> <span class="s2">"Test[</span><span class="si">%d</span><span class="s2">]:Result* Prec@1 </span><span class="si">%.3f</span><span class="se">\t</span><span class="s2">Loss </span><span class="si">%.3f</span><span class="s2">"</span><span class="o">%</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span><span class="n">prec1</span><span class="p">,</span><span class="n">valid_loss</span><span class="p">)</span>
    <span class="n">y_valid_loss</span><span class="p">[</span><span class="n">iiter</span><span class="p">]</span> <span class="o">=</span> <span class="n">valid_loss</span>
    <span class="n">y_valid_acc</span><span class="p">[</span><span class="n">iiter</span><span class="p">]</span>  <span class="o">=</span> <span class="n">prec1</span>
        
    <span class="c1"># plot up to current iteration</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="n">iiter</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">y_train_loss</span><span class="p">[:</span><span class="n">iiter</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="s1">'b'</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="n">iiter</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">y_valid_loss</span><span class="p">[:</span><span class="n">iiter</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="s1">'r'</span><span class="p">)</span>
    
    <span class="c1"># plot up to current iteration</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="n">iiter</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">y_train_acc</span><span class="p">[:</span><span class="n">iiter</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="s1">'b'</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="n">iiter</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">y_valid_acc</span><span class="p">[:</span><span class="n">iiter</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="s1">'r'</span><span class="p">)</span>
    
    <span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
    
    <span class="c1"># remember best prec@1 and save checkpoint                                                                                                                                                          </span>
    <span class="n">is_best</span> <span class="o">=</span> <span class="n">prec1</span> <span class="o">></span> <span class="n">best_prec1</span>
    <span class="n">best_prec1</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">prec1</span><span class="p">,</span> <span class="n">best_prec1</span><span class="p">)</span>
    <span class="n">save_checkpoint</span><span class="p">({</span>
        <span class="s1">'epoch'</span><span class="p">:</span> <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s1">'state_dict'</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
        <span class="s1">'best_prec1'</span><span class="p">:</span> <span class="n">best_prec1</span><span class="p">,</span>
        <span class="s1">'optimizer'</span> <span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="p">},</span> <span class="n">is_best</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">iteration</span><span class="o">%</span><span class="k">50</span>==0:
        <span class="n">save_checkpoint</span><span class="p">({</span>
            <span class="s1">'epoch'</span><span class="p">:</span> <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s1">'state_dict'</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s1">'best_prec1'</span><span class="p">:</span> <span class="n">best_prec1</span><span class="p">,</span>
            <span class="s1">'optimizer'</span> <span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
        <span class="p">},</span> <span class="bp">False</span><span class="p">,</span> <span class="n">iteration</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div id="0195d379-bb31-430d-98d5-ea5e4113e2ab"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#0195d379-bb31-430d-98d5-ea5e4113e2ab');
/* Put everything inside the global mpl namespace */
window.mpl = {};

mpl.get_websocket_type = function() {
    if (typeof(WebSocket) !== 'undefined') {
        return WebSocket;
    } else if (typeof(MozWebSocket) !== 'undefined') {
        return MozWebSocket;
    } else {
        alert('Your browser does not have WebSocket support.' +
              'Please try Chrome, Safari or Firefox ≥ 6. ' +
              'Firefox 4 and 5 are also supported but you ' +
              'have to enable WebSockets in about:config.');
    };
}

mpl.figure = function(figure_id, websocket, ondownload, parent_element) {
    this.id = figure_id;

    this.ws = websocket;

    this.supports_binary = (this.ws.binaryType != undefined);

    if (!this.supports_binary) {
        var warnings = document.getElementById("mpl-warnings");
        if (warnings) {
            warnings.style.display = 'block';
            warnings.textContent = (
                "This browser does not support binary websocket messages. " +
                    "Performance may be slow.");
        }
    }

    this.imageObj = new Image();

    this.context = undefined;
    this.message = undefined;
    this.canvas = undefined;
    this.rubberband_canvas = undefined;
    this.rubberband_context = undefined;
    this.format_dropdown = undefined;

    this.image_mode = 'full';

    this.root = $('<div/>');
    this._root_extra_style(this.root)
    this.root.attr('style', 'display: inline-block');

    $(parent_element).append(this.root);

    this._init_header(this);
    this._init_canvas(this);
    this._init_toolbar(this);

    var fig = this;

    this.waiting = false;

    this.ws.onopen =  function () {
            fig.send_message("supports_binary", {value: fig.supports_binary});
            fig.send_message("send_image_mode", {});
            fig.send_message("refresh", {});
        }

    this.imageObj.onload = function() {
            if (fig.image_mode == 'full') {
                // Full images could contain transparency (where diff images
                // almost always do), so we need to clear the canvas so that
                // there is no ghosting.
                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);
            }
            fig.context.drawImage(fig.imageObj, 0, 0);
        };

    this.imageObj.onunload = function() {
        this.ws.close();
    }

    this.ws.onmessage = this._make_on_message_function(this);

    this.ondownload = ondownload;
}

mpl.figure.prototype._init_header = function() {
    var titlebar = $(
        '<div class="ui-dialog-titlebar ui-widget-header ui-corner-all ' +
        'ui-helper-clearfix"/>');
    var titletext = $(
        '<div class="ui-dialog-title" style="width: 100%; ' +
        'text-align: center; padding: 3px;"/>');
    titlebar.append(titletext)
    this.root.append(titlebar);
    this.header = titletext[0];
}



mpl.figure.prototype._canvas_extra_style = function(canvas_div) {

}


mpl.figure.prototype._root_extra_style = function(canvas_div) {

}

mpl.figure.prototype._init_canvas = function() {
    var fig = this;

    var canvas_div = $('<div/>');

    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');

    function canvas_keyboard_event(event) {
        return fig.key_event(event, event['data']);
    }

    canvas_div.keydown('key_press', canvas_keyboard_event);
    canvas_div.keyup('key_release', canvas_keyboard_event);
    this.canvas_div = canvas_div
    this._canvas_extra_style(canvas_div)
    this.root.append(canvas_div);

    var canvas = $('<canvas/>');
    canvas.addClass('mpl-canvas');
    canvas.attr('style', "left: 0; top: 0; z-index: 0; outline: 0")

    this.canvas = canvas[0];
    this.context = canvas[0].getContext("2d");

    var rubberband = $('<canvas/>');
    rubberband.attr('style', "position: absolute; left: 0; top: 0; z-index: 1;")

    var pass_mouse_events = true;

    canvas_div.resizable({
        start: function(event, ui) {
            pass_mouse_events = false;
        },
        resize: function(event, ui) {
            fig.request_resize(ui.size.width, ui.size.height);
        },
        stop: function(event, ui) {
            pass_mouse_events = true;
            fig.request_resize(ui.size.width, ui.size.height);
        },
    });

    function mouse_event_fn(event) {
        if (pass_mouse_events)
            return fig.mouse_event(event, event['data']);
    }

    rubberband.mousedown('button_press', mouse_event_fn);
    rubberband.mouseup('button_release', mouse_event_fn);
    // Throttle sequential mouse events to 1 every 20ms.
    rubberband.mousemove('motion_notify', mouse_event_fn);

    rubberband.mouseenter('figure_enter', mouse_event_fn);
    rubberband.mouseleave('figure_leave', mouse_event_fn);

    canvas_div.on("wheel", function (event) {
        event = event.originalEvent;
        event['data'] = 'scroll'
        if (event.deltaY < 0) {
            event.step = 1;
        } else {
            event.step = -1;
        }
        mouse_event_fn(event);
    });

    canvas_div.append(canvas);
    canvas_div.append(rubberband);

    this.rubberband = rubberband;
    this.rubberband_canvas = rubberband[0];
    this.rubberband_context = rubberband[0].getContext("2d");
    this.rubberband_context.strokeStyle = "#000000";

    this._resize_canvas = function(width, height) {
        // Keep the size of the canvas, canvas container, and rubber band
        // canvas in synch.
        canvas_div.css('width', width)
        canvas_div.css('height', height)

        canvas.attr('width', width);
        canvas.attr('height', height);

        rubberband.attr('width', width);
        rubberband.attr('height', height);
    }

    // Set the figure to an initial 600x600px, this will subsequently be updated
    // upon first draw.
    this._resize_canvas(600, 600);

    // Disable right mouse context menu.
    $(this.rubberband_canvas).bind("contextmenu",function(e){
        return false;
    });

    function set_focus () {
        canvas.focus();
        canvas_div.focus();
    }

    window.setTimeout(set_focus, 100);
}

mpl.figure.prototype._init_toolbar = function() {
    var fig = this;

    var nav_element = $('<div/>')
    nav_element.attr('style', 'width: 100%');
    this.root.append(nav_element);

    // Define a callback function for later on.
    function toolbar_event(event) {
        return fig.toolbar_button_onclick(event['data']);
    }
    function toolbar_mouse_event(event) {
        return fig.toolbar_button_onmouseover(event['data']);
    }

    for(var toolbar_ind in mpl.toolbar_items) {
        var name = mpl.toolbar_items[toolbar_ind][0];
        var tooltip = mpl.toolbar_items[toolbar_ind][1];
        var image = mpl.toolbar_items[toolbar_ind][2];
        var method_name = mpl.toolbar_items[toolbar_ind][3];

        if (!name) {
            // put a spacer in here.
            continue;
        }
        var button = $('<button/>');
        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +
                        'ui-button-icon-only');
        button.attr('role', 'button');
        button.attr('aria-disabled', 'false');
        button.click(method_name, toolbar_event);
        button.mouseover(tooltip, toolbar_mouse_event);

        var icon_img = $('<span/>');
        icon_img.addClass('ui-button-icon-primary ui-icon');
        icon_img.addClass(image);
        icon_img.addClass('ui-corner-all');

        var tooltip_span = $('<span/>');
        tooltip_span.addClass('ui-button-text');
        tooltip_span.html(tooltip);

        button.append(icon_img);
        button.append(tooltip_span);

        nav_element.append(button);
    }

    var fmt_picker_span = $('<span/>');

    var fmt_picker = $('<select/>');
    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');
    fmt_picker_span.append(fmt_picker);
    nav_element.append(fmt_picker_span);
    this.format_dropdown = fmt_picker[0];

    for (var ind in mpl.extensions) {
        var fmt = mpl.extensions[ind];
        var option = $(
            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);
        fmt_picker.append(option)
    }

    // Add hover states to the ui-buttons
    $( ".ui-button" ).hover(
        function() { $(this).addClass("ui-state-hover");},
        function() { $(this).removeClass("ui-state-hover");}
    );

    var status_bar = $('<span class="mpl-message"/>');
    nav_element.append(status_bar);
    this.message = status_bar[0];
}

mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {
    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,
    // which will in turn request a refresh of the image.
    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});
}

mpl.figure.prototype.send_message = function(type, properties) {
    properties['type'] = type;
    properties['figure_id'] = this.id;
    this.ws.send(JSON.stringify(properties));
}

mpl.figure.prototype.send_draw_message = function() {
    if (!this.waiting) {
        this.waiting = true;
        this.ws.send(JSON.stringify({type: "draw", figure_id: this.id}));
    }
}


mpl.figure.prototype.handle_save = function(fig, msg) {
    var format_dropdown = fig.format_dropdown;
    var format = format_dropdown.options[format_dropdown.selectedIndex].value;
    fig.ondownload(fig, format);
}


mpl.figure.prototype.handle_resize = function(fig, msg) {
    var size = msg['size'];
    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {
        fig._resize_canvas(size[0], size[1]);
        fig.send_message("refresh", {});
    };
}

mpl.figure.prototype.handle_rubberband = function(fig, msg) {
    var x0 = msg['x0'];
    var y0 = fig.canvas.height - msg['y0'];
    var x1 = msg['x1'];
    var y1 = fig.canvas.height - msg['y1'];
    x0 = Math.floor(x0) + 0.5;
    y0 = Math.floor(y0) + 0.5;
    x1 = Math.floor(x1) + 0.5;
    y1 = Math.floor(y1) + 0.5;
    var min_x = Math.min(x0, x1);
    var min_y = Math.min(y0, y1);
    var width = Math.abs(x1 - x0);
    var height = Math.abs(y1 - y0);

    fig.rubberband_context.clearRect(
        0, 0, fig.canvas.width, fig.canvas.height);

    fig.rubberband_context.strokeRect(min_x, min_y, width, height);
}

mpl.figure.prototype.handle_figure_label = function(fig, msg) {
    // Updates the figure title.
    fig.header.textContent = msg['label'];
}

mpl.figure.prototype.handle_cursor = function(fig, msg) {
    var cursor = msg['cursor'];
    switch(cursor)
    {
    case 0:
        cursor = 'pointer';
        break;
    case 1:
        cursor = 'default';
        break;
    case 2:
        cursor = 'crosshair';
        break;
    case 3:
        cursor = 'move';
        break;
    }
    fig.rubberband_canvas.style.cursor = cursor;
}

mpl.figure.prototype.handle_message = function(fig, msg) {
    fig.message.textContent = msg['message'];
}

mpl.figure.prototype.handle_draw = function(fig, msg) {
    // Request the server to send over a new figure.
    fig.send_draw_message();
}

mpl.figure.prototype.handle_image_mode = function(fig, msg) {
    fig.image_mode = msg['mode'];
}

mpl.figure.prototype.updated_canvas_event = function() {
    // Called whenever the canvas gets updated.
    this.send_message("ack", {});
}

// A function to construct a web socket function for onmessage handling.
// Called in the figure constructor.
mpl.figure.prototype._make_on_message_function = function(fig) {
    return function socket_on_message(evt) {
        if (evt.data instanceof Blob) {
            /* FIXME: We get "Resource interpreted as Image but
             * transferred with MIME type text/plain:" errors on
             * Chrome.  But how to set the MIME type?  It doesn't seem
             * to be part of the websocket stream */
            evt.data.type = "image/png";

            /* Free the memory for the previous frames */
            if (fig.imageObj.src) {
                (window.URL || window.webkitURL).revokeObjectURL(
                    fig.imageObj.src);
            }

            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(
                evt.data);
            fig.updated_canvas_event();
            fig.waiting = false;
            return;
        }
        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == "data:image/png;base64") {
            fig.imageObj.src = evt.data;
            fig.updated_canvas_event();
            fig.waiting = false;
            return;
        }

        var msg = JSON.parse(evt.data);
        var msg_type = msg['type'];

        // Call the  "handle_{type}" callback, which takes
        // the figure and JSON message as its only arguments.
        try {
            var callback = fig["handle_" + msg_type];
        } catch (e) {
            console.log("No handler for the '" + msg_type + "' message type: ", msg);
            return;
        }

        if (callback) {
            try {
                // console.log("Handling '" + msg_type + "' message: ", msg);
                callback(fig, msg);
            } catch (e) {
                console.log("Exception inside the 'handler_" + msg_type + "' callback:", e, e.stack, msg);
            }
        }
    };
}

// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas
mpl.findpos = function(e) {
    //this section is from http://www.quirksmode.org/js/events_properties.html
    var targ;
    if (!e)
        e = window.event;
    if (e.target)
        targ = e.target;
    else if (e.srcElement)
        targ = e.srcElement;
    if (targ.nodeType == 3) // defeat Safari bug
        targ = targ.parentNode;

    // jQuery normalizes the pageX and pageY
    // pageX,Y are the mouse positions relative to the document
    // offset() returns the position of the element relative to the document
    var x = e.pageX - $(targ).offset().left;
    var y = e.pageY - $(targ).offset().top;

    return {"x": x, "y": y};
};

/*
 * return a copy of an object with only non-object keys
 * we need this to avoid circular references
 * http://stackoverflow.com/a/24161582/3208463
 */
function simpleKeys (original) {
  return Object.keys(original).reduce(function (obj, key) {
    if (typeof original[key] !== 'object')
        obj[key] = original[key]
    return obj;
  }, {});
}

mpl.figure.prototype.mouse_event = function(event, name) {
    var canvas_pos = mpl.findpos(event)

    if (name === 'button_press')
    {
        this.canvas.focus();
        this.canvas_div.focus();
    }

    var x = canvas_pos.x;
    var y = canvas_pos.y;

    this.send_message(name, {x: x, y: y, button: event.button,
                             step: event.step,
                             guiEvent: simpleKeys(event)});

    /* This prevents the web browser from automatically changing to
     * the text insertion cursor when the button is pressed.  We want
     * to control all of the cursor setting manually through the
     * 'cursor' event from matplotlib */
    event.preventDefault();
    return false;
}

mpl.figure.prototype._key_event_extra = function(event, name) {
    // Handle any extra behaviour associated with a key event
}

mpl.figure.prototype.key_event = function(event, name) {

    // Prevent repeat events
    if (name == 'key_press')
    {
        if (event.which === this._key)
            return;
        else
            this._key = event.which;
    }
    if (name == 'key_release')
        this._key = null;

    var value = '';
    if (event.ctrlKey && event.which != 17)
        value += "ctrl+";
    if (event.altKey && event.which != 18)
        value += "alt+";
    if (event.shiftKey && event.which != 16)
        value += "shift+";

    value += 'k';
    value += event.which.toString();

    this._key_event_extra(event, name);

    this.send_message(name, {key: value,
                             guiEvent: simpleKeys(event)});
    return false;
}

mpl.figure.prototype.toolbar_button_onclick = function(name) {
    if (name == 'download') {
        this.handle_save(this, null);
    } else {
        this.send_message("toolbar_button", {name: name});
    }
};

mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {
    this.message.textContent = tooltip;
};
mpl.toolbar_items = [["Home", "Reset original view", "fa fa-home icon-home", "home"], ["Back", "Back to  previous view", "fa fa-arrow-left icon-arrow-left", "back"], ["Forward", "Forward to next view", "fa fa-arrow-right icon-arrow-right", "forward"], ["", "", "", ""], ["Pan", "Pan axes with left mouse, zoom with right", "fa fa-arrows icon-move", "pan"], ["Zoom", "Zoom to rectangle", "fa fa-square-o icon-check-empty", "zoom"], ["", "", "", ""], ["Download", "Download plot", "fa fa-floppy-o icon-save", "download"]];

mpl.extensions = ["eps", "jpeg", "pdf", "png", "ps", "raw", "svg", "tif"];

mpl.default_extension = "png";var comm_websocket_adapter = function(comm) {
    // Create a "websocket"-like object which calls the given IPython comm
    // object with the appropriate methods. Currently this is a non binary
    // socket, so there is still some room for performance tuning.
    var ws = {};

    ws.close = function() {
        comm.close()
    };
    ws.send = function(m) {
        //console.log('sending', m);
        comm.send(m);
    };
    // Register the callback with on_msg.
    comm.on_msg(function(msg) {
        //console.log('receiving', msg['content']['data'], msg);
        // Pass the mpl event to the overriden (by mpl) onmessage function.
        ws.onmessage(msg['content']['data'])
    });
    return ws;
}

mpl.mpl_figure_comm = function(comm, msg) {
    // This is the function which gets called when the mpl process
    // starts-up an IPython Comm through the "matplotlib" channel.

    var id = msg.content.data.id;
    // Get hold of the div created by the display call when the Comm
    // socket was opened in Python.
    var element = $("#" + id);
    var ws_proxy = comm_websocket_adapter(comm)

    function ondownload(figure, format) {
        window.open(figure.imageObj.src);
    }

    var fig = new mpl.figure(id, ws_proxy,
                           ondownload,
                           element.get(0));

    // Call onopen now - mpl needs it, as it is assuming we've passed it a real
    // web socket which is closed, not our websocket->open comm proxy.
    ws_proxy.onopen();

    fig.parent_element = element.get(0);
    fig.cell_info = mpl.find_output_cell("<div id='" + id + "'></div>");
    if (!fig.cell_info) {
        console.error("Failed to find cell for figure", id, fig);
        return;
    }

    var output_index = fig.cell_info[2]
    var cell = fig.cell_info[0];

};

mpl.figure.prototype.handle_close = function(fig, msg) {
    fig.root.unbind('remove')

    // Update the output cell to use the data from the current canvas.
    fig.push_to_output();
    var dataURL = fig.canvas.toDataURL();
    // Re-enable the keyboard manager in IPython - without this line, in FF,
    // the notebook keyboard shortcuts fail.
    IPython.keyboard_manager.enable()
    $(fig.parent_element).html('<img src="' + dataURL + '">');
    fig.close_ws(fig, msg);
}

mpl.figure.prototype.close_ws = function(fig, msg){
    fig.send_message('closing', msg);
    // fig.ws.close()
}

mpl.figure.prototype.push_to_output = function(remove_interactive) {
    // Turn the data on the canvas into data in the output cell.
    var dataURL = this.canvas.toDataURL();
    this.cell_info[1]['text/html'] = '<img src="' + dataURL + '">';
}

mpl.figure.prototype.updated_canvas_event = function() {
    // Tell IPython that the notebook contents must change.
    IPython.notebook.set_dirty(true);
    this.send_message("ack", {});
    var fig = this;
    // Wait a second, then push the new image to the DOM so
    // that it is saved nicely (might be nice to debounce this).
    setTimeout(function () { fig.push_to_output() }, 1000);
}

mpl.figure.prototype._init_toolbar = function() {
    var fig = this;

    var nav_element = $('<div/>')
    nav_element.attr('style', 'width: 100%');
    this.root.append(nav_element);

    // Define a callback function for later on.
    function toolbar_event(event) {
        return fig.toolbar_button_onclick(event['data']);
    }
    function toolbar_mouse_event(event) {
        return fig.toolbar_button_onmouseover(event['data']);
    }

    for(var toolbar_ind in mpl.toolbar_items){
        var name = mpl.toolbar_items[toolbar_ind][0];
        var tooltip = mpl.toolbar_items[toolbar_ind][1];
        var image = mpl.toolbar_items[toolbar_ind][2];
        var method_name = mpl.toolbar_items[toolbar_ind][3];

        if (!name) { continue; };

        var button = $('<button class="btn btn-default" href="#" title="' + name + '"><i class="fa ' + image + ' fa-lg"></i></button>');
        button.click(method_name, toolbar_event);
        button.mouseover(tooltip, toolbar_mouse_event);
        nav_element.append(button);
    }

    // Add the status bar.
    var status_bar = $('<span class="mpl-message" style="text-align:right; float: right;"/>');
    nav_element.append(status_bar);
    this.message = status_bar[0];

    // Add the close button to the window.
    var buttongrp = $('<div class="btn-group inline pull-right"></div>');
    var button = $('<button class="btn btn-mini btn-primary" href="#" title="Stop Interaction"><i class="fa fa-power-off icon-remove icon-large"></i></button>');
    button.click(function (evt) { fig.handle_close(fig, {}); } );
    button.mouseover('Stop Interaction', toolbar_mouse_event);
    buttongrp.append(button);
    var titlebar = this.root.find($('.ui-dialog-titlebar'));
    titlebar.prepend(buttongrp);
}

mpl.figure.prototype._root_extra_style = function(el){
    var fig = this
    el.on("remove", function(){
	fig.close_ws(fig, {});
    });
}

mpl.figure.prototype._canvas_extra_style = function(el){
    // this is important to make the div 'focusable
    el.attr('tabindex', 0)
    // reach out to IPython and tell the keyboard manager to turn it's self
    // off when our div gets focus

    // location in version 3
    if (IPython.notebook.keyboard_manager) {
        IPython.notebook.keyboard_manager.register_events(el);
    }
    else {
        // location in version 2
        IPython.keyboard_manager.register_events(el);
    }

}

mpl.figure.prototype._key_event_extra = function(event, name) {
    var manager = IPython.notebook.keyboard_manager;
    if (!manager)
        manager = IPython.keyboard_manager;

    // Check for shift+enter
    if (event.shiftKey && event.which == 13) {
        this.canvas_div.blur();
        event.shiftKey = false;
        // Send a "J" for go to next cell
        event.which = 74;
        event.keyCode = 74;
        manager.command_mode();
        manager.handle_keydown(event);
    }
}

mpl.figure.prototype.handle_save = function(fig, msg) {
    fig.ondownload(fig, null);
}


mpl.find_output_cell = function(html_output) {
    // Return the cell and output element which can be found *uniquely* in the notebook.
    // Note - this is a bit hacky, but it is done because the "notebook_saving.Notebook"
    // IPython event is triggered only after the cells have been serialised, which for
    // our purposes (turning an active figure into a static one), is too late.
    var cells = IPython.notebook.get_cells();
    var ncells = cells.length;
    for (var i=0; i<ncells; i++) {
        var cell = cells[i];
        if (cell.cell_type === 'code'){
            for (var j=0; j<cell.output_area.outputs.length; j++) {
                var data = cell.output_area.outputs[j];
                if (data.data) {
                    // IPython >= 3 moved mimebundle to data attribute of output
                    data = data.data;
                }
                if (data['text/html'] == html_output) {
                    return [cell, data, j];
                }
            }
        }
    }
}

// Register the function which deals with the matplotlib target/channel.
// The kernel may be null if the page has been refreshed.
if (IPython.notebook.kernel != null) {
    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);
}

</script>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_html rendered_html output_subarea ">
<img src="data:image/png;base64,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" />
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Iteration [0]: lr=1.000e-03
Iteration [0] train aveloss=1.516 aveacc=32.930
Test[0]:Result* Prec@1 43.700	Loss 1.348
Iteration [1]: lr=1.000e-03
Iteration [1] train aveloss=1.299 aveacc=44.890
Test[1]:Result* Prec@1 52.100	Loss 1.148
Iteration [2]: lr=1.000e-03
Iteration [2] train aveloss=1.055 aveacc=53.830
Test[2]:Result* Prec@1 59.500	Loss 0.893
Iteration [3]: lr=1.000e-03
Iteration [3] train aveloss=0.920 aveacc=58.850
Test[3]:Result* Prec@1 59.000	Loss 0.852
Iteration [4]: lr=1.000e-03
Iteration [4] train aveloss=0.866 aveacc=60.510
Test[4]:Result* Prec@1 61.400	Loss 0.859
Iteration [5]: lr=1.000e-03
Iteration [5] train aveloss=0.826 aveacc=62.280
Test[5]:Result* Prec@1 63.900	Loss 0.744
Iteration [6]: lr=1.000e-03
Iteration [6] train aveloss=0.817 aveacc=62.480
Test[6]:Result* Prec@1 64.200	Loss 0.728
Iteration [7]: lr=1.000e-03
Iteration [7] train aveloss=0.793 aveacc=63.020
Test[7]:Result* Prec@1 64.400	Loss 0.775
Iteration [8]: lr=1.000e-03
Iteration [8] train aveloss=0.767 aveacc=63.340
Test[8]:Result* Prec@1 66.000	Loss 0.696
Iteration [9]: lr=1.000e-03
Iteration [9] train aveloss=0.753 aveacc=63.620
Test[9]:Result* Prec@1 66.400	Loss 0.677
Iteration [10]: lr=1.000e-03
Iteration [10] train aveloss=0.730 aveacc=64.260
Test[10]:Result* Prec@1 63.200	Loss 0.734
Iteration [11]: lr=1.000e-03
Iteration [11] train aveloss=0.743 aveacc=64.010
Test[11]:Result* Prec@1 66.500	Loss 0.674
Iteration [12]: lr=1.000e-03
Iteration [12] train aveloss=0.728 aveacc=65.130
Test[12]:Result* Prec@1 66.300	Loss 0.704
Iteration [13]: lr=1.000e-03
Iteration [13] train aveloss=0.719 aveacc=64.820
Test[13]:Result* Prec@1 64.400	Loss 0.687
Iteration [14]: lr=1.000e-03
Iteration [14] train aveloss=0.740 aveacc=64.320
Test[14]:Result* Prec@1 66.300	Loss 0.663
Iteration [15]: lr=1.000e-03
Iteration [15] train aveloss=0.701 aveacc=65.680
Test[15]:Result* Prec@1 65.300	Loss 0.708
Iteration [16]: lr=1.000e-03
Iteration [16] train aveloss=0.697 aveacc=66.300
Test[16]:Result* Prec@1 65.000	Loss 0.731
Iteration [17]: lr=1.000e-03
Iteration [17] train aveloss=0.701 aveacc=64.950
Test[17]:Result* Prec@1 67.700	Loss 0.642
Iteration [18]: lr=1.000e-03
Iteration [18] train aveloss=0.691 aveacc=65.720
Test[18]:Result* Prec@1 69.500	Loss 0.622
Iteration [19]: lr=1.000e-03
Iteration [19] train aveloss=0.690 aveacc=66.900
Test[19]:Result* Prec@1 67.400	Loss 0.696
Iteration [20]: lr=1.000e-03
Iteration [20] train aveloss=0.692 aveacc=66.050
Test[20]:Result* Prec@1 68.900	Loss 0.644
Iteration [21]: lr=1.000e-03
Iteration [21] train aveloss=0.682 aveacc=66.110
Test[21]:Result* Prec@1 68.400	Loss 0.647
Iteration [22]: lr=1.000e-03
Iteration [22] train aveloss=0.691 aveacc=65.720
Test[22]:Result* Prec@1 67.300	Loss 0.666
Iteration [23]: lr=1.000e-03
Iteration [23] train aveloss=0.676 aveacc=67.310
Test[23]:Result* Prec@1 69.100	Loss 0.652
Iteration [24]: lr=1.000e-03
Iteration [24] train aveloss=0.663 aveacc=67.610
Test[24]:Result* Prec@1 67.000	Loss 0.643
Iteration [25]: lr=1.000e-03
Iteration [25] train aveloss=0.660 aveacc=67.830
Test[25]:Result* Prec@1 68.400	Loss 0.627
Iteration [26]: lr=1.000e-03
Iteration [26] train aveloss=0.663 aveacc=67.840
Test[26]:Result* Prec@1 70.900	Loss 0.615
Iteration [27]: lr=1.000e-03
Iteration [27] train aveloss=0.665 aveacc=67.270
Test[27]:Result* Prec@1 70.100	Loss 0.620
Iteration [28]: lr=1.000e-03
Iteration [28] train aveloss=0.650 aveacc=67.610
Test[28]:Result* Prec@1 67.400	Loss 0.644
Iteration [29]: lr=1.000e-03
Iteration [29] train aveloss=0.660 aveacc=68.390
Test[29]:Result* Prec@1 71.000	Loss 0.621
Iteration [30]: lr=1.000e-03
Iteration [30] train aveloss=0.653 aveacc=68.340
Test[30]:Result* Prec@1 68.300	Loss 0.637
Iteration [31]: lr=1.000e-03
Iteration [31] train aveloss=0.646 aveacc=69.090
Test[31]:Result* Prec@1 69.200	Loss 0.615
Iteration [32]: lr=1.000e-03
Iteration [32] train aveloss=0.637 aveacc=69.270
Test[32]:Result* Prec@1 73.100	Loss 0.587
Iteration [33]: lr=1.000e-03
Iteration [33] train aveloss=0.658 aveacc=68.120
Test[33]:Result* Prec@1 71.700	Loss 0.618
Iteration [34]: lr=1.000e-03
Iteration [34] train aveloss=0.625 aveacc=70.150
Test[34]:Result* Prec@1 71.400	Loss 0.599
Iteration [35]: lr=1.000e-03
Iteration [35] train aveloss=0.646 aveacc=69.010
Test[35]:Result* Prec@1 73.000	Loss 0.594
Iteration [36]: lr=1.000e-03
Iteration [36] train aveloss=0.638 aveacc=69.400
Test[36]:Result* Prec@1 71.100	Loss 0.613
Iteration [37]: lr=1.000e-03
Iteration [37] train aveloss=0.626 aveacc=70.460
Test[37]:Result* Prec@1 70.700	Loss 0.615
Iteration [38]: lr=1.000e-03
Iteration [38] train aveloss=0.615 aveacc=70.800
Test[38]:Result* Prec@1 69.200	Loss 0.638
Iteration [39]: lr=1.000e-03
Iteration [39] train aveloss=0.609 aveacc=71.630
Test[39]:Result* Prec@1 71.600	Loss 0.612
Iteration [40]: lr=1.000e-03
Iteration [40] train aveloss=0.609 aveacc=70.820
Test[40]:Result* Prec@1 69.900	Loss 0.652
Iteration [41]: lr=1.000e-03
Iteration [41] train aveloss=0.611 aveacc=71.200
Test[41]:Result* Prec@1 74.400	Loss 0.565
Iteration [42]: lr=1.000e-03
Iteration [42] train aveloss=0.614 aveacc=71.860
Test[42]:Result* Prec@1 73.000	Loss 0.589
Iteration [43]: lr=1.000e-03
Iteration [43] train aveloss=0.609 aveacc=71.900
Test[43]:Result* Prec@1 74.100	Loss 0.609
Iteration [44]: lr=1.000e-03
Iteration [44] train aveloss=0.597 aveacc=72.570
Test[44]:Result* Prec@1 71.500	Loss 0.635
Iteration [45]: lr=1.000e-03
Iteration [45] train aveloss=0.593 aveacc=73.240
Test[45]:Result* Prec@1 74.800	Loss 0.542
Iteration [46]: lr=1.000e-03
Iteration [46] train aveloss=0.589 aveacc=73.530
Test[46]:Result* Prec@1 72.600	Loss 0.611
Iteration [47]: lr=1.000e-03
Iteration [47] train aveloss=0.583 aveacc=73.500
Test[47]:Result* Prec@1 74.300	Loss 0.575
Iteration [48]: lr=1.000e-03
Iteration [48] train aveloss=0.580 aveacc=74.210
Test[48]:Result* Prec@1 79.000	Loss 0.534
Iteration [49]: lr=1.000e-03
Iteration [49] train aveloss=0.589 aveacc=73.800
Test[49]:Result* Prec@1 73.300	Loss 0.589
Iteration [50]: lr=1.000e-03
Iteration [50] train aveloss=0.566 aveacc=75.180
Test[50]:Result* Prec@1 78.900	Loss 0.517
Iteration [51]: lr=1.000e-03
Iteration [51] train aveloss=0.571 aveacc=74.870
Test[51]:Result* Prec@1 75.700	Loss 0.537
Iteration [52]: lr=1.000e-03
Iteration [52] train aveloss=0.555 aveacc=76.270
Test[52]:Result* Prec@1 76.400	Loss 0.574
Iteration [53]: lr=1.000e-03
Iteration [53] train aveloss=0.567 aveacc=75.590
Test[53]:Result* Prec@1 77.300	Loss 0.543
Iteration [54]: lr=1.000e-03
Iteration [54] train aveloss=0.535 aveacc=77.280
Test[54]:Result* Prec@1 71.100	Loss 0.664
Iteration [55]: lr=1.000e-03
Iteration [55] train aveloss=0.551 aveacc=76.490
Test[55]:Result* Prec@1 73.500	Loss 0.628
Iteration [56]: lr=1.000e-03
Iteration [56] train aveloss=0.537 aveacc=76.860
Test[56]:Result* Prec@1 73.500	Loss 0.710
Iteration [57]: lr=1.000e-03
Iteration [57] train aveloss=0.528 aveacc=77.840
Test[57]:Result* Prec@1 75.600	Loss 0.563
Iteration [58]: lr=1.000e-03
Iteration [58] train aveloss=0.532 aveacc=77.470
Test[58]:Result* Prec@1 76.400	Loss 0.525
Iteration [59]: lr=1.000e-03
Iteration [59] train aveloss=0.522 aveacc=78.630
Test[59]:Result* Prec@1 79.400	Loss 0.505
Iteration [60]: lr=1.000e-03
Iteration [60] train aveloss=0.540 aveacc=77.500
Test[60]:Result* Prec@1 79.900	Loss 0.495
Iteration [61]: lr=1.000e-03
Iteration [61] train aveloss=0.506 aveacc=78.630
Test[61]:Result* Prec@1 78.700	Loss 0.500
Iteration [62]: lr=1.000e-03
Iteration [62] train aveloss=0.496 aveacc=79.010
Test[62]:Result* Prec@1 75.100	Loss 0.575
Iteration [63]: lr=1.000e-03
Iteration [63] train aveloss=0.515 aveacc=78.500
Test[63]:Result* Prec@1 77.500	Loss 0.555
Iteration [64]: lr=1.000e-03
Iteration [64] train aveloss=0.508 aveacc=78.900
Test[64]:Result* Prec@1 78.600	Loss 0.508
Iteration [65]: lr=1.000e-03
Iteration [65] train aveloss=0.500 aveacc=79.450
Test[65]:Result* Prec@1 76.800	Loss 0.542
Iteration [66]: lr=1.000e-03
Iteration [66] train aveloss=0.499 aveacc=79.760
Test[66]:Result* Prec@1 79.900	Loss 0.485
Iteration [67]: lr=1.000e-03
Iteration [67] train aveloss=0.505 aveacc=79.570
Test[67]:Result* Prec@1 80.300	Loss 0.504
Iteration [68]: lr=1.000e-03
Iteration [68] train aveloss=0.484 aveacc=80.400
Test[68]:Result* Prec@1 80.800	Loss 0.462
Iteration [69]: lr=1.000e-03
Iteration [69] train aveloss=0.485 aveacc=80.140
Test[69]:Result* Prec@1 79.400	Loss 0.491
Iteration [70]: lr=1.000e-03
Iteration [70] train aveloss=0.483 aveacc=79.860
Test[70]:Result* Prec@1 77.800	Loss 0.496
Iteration [71]: lr=1.000e-03
Iteration [71] train aveloss=0.481 aveacc=80.830
Test[71]:Result* Prec@1 81.900	Loss 0.483
Iteration [72]: lr=1.000e-03
Iteration [72] train aveloss=0.469 aveacc=81.060
Test[72]:Result* Prec@1 80.500	Loss 0.478
Iteration [73]: lr=1.000e-03
Iteration [73] train aveloss=0.489 aveacc=80.470
Test[73]:Result* Prec@1 78.500	Loss 0.553
Iteration [74]: lr=1.000e-03
Iteration [74] train aveloss=0.474 aveacc=80.960
Test[74]:Result* Prec@1 79.200	Loss 0.560
Iteration [75]: lr=1.000e-03
Iteration [75] train aveloss=0.467 aveacc=80.810
Test[75]:Result* Prec@1 79.000	Loss 0.503
Iteration [76]: lr=1.000e-03
Iteration [76] train aveloss=0.482 aveacc=79.980
Test[76]:Result* Prec@1 79.800	Loss 0.478
Iteration [77]: lr=1.000e-03
Iteration [77] train aveloss=0.467 aveacc=81.060
Test[77]:Result* Prec@1 74.200	Loss 0.590
Iteration [78]: lr=1.000e-03
Iteration [78] train aveloss=0.458 aveacc=81.330
Test[78]:Result* Prec@1 79.500	Loss 0.521
Iteration [79]: lr=1.000e-03
Iteration [79] train aveloss=0.449 aveacc=81.510
Test[79]:Result* Prec@1 79.600	Loss 0.492
Iteration [80]: lr=1.000e-03
Iteration [80] train aveloss=0.459 aveacc=81.170
Test[80]:Result* Prec@1 81.700	Loss 0.460
Iteration [81]: lr=1.000e-03
Iteration [81] train aveloss=0.451 aveacc=81.900
Test[81]:Result* Prec@1 81.500	Loss 0.492
Iteration [82]: lr=1.000e-03
Iteration [82] train aveloss=0.451 aveacc=82.020
Test[82]:Result* Prec@1 84.300	Loss 0.433
Iteration [83]: lr=1.000e-03
Iteration [83] train aveloss=0.446 aveacc=82.040
Test[83]:Result* Prec@1 83.700	Loss 0.416
Iteration [84]: lr=1.000e-03
Iteration [84] train aveloss=0.449 aveacc=81.760
Test[84]:Result* Prec@1 77.900	Loss 0.540
Iteration [85]: lr=1.000e-03
Iteration [85] train aveloss=0.456 aveacc=81.580
Test[85]:Result* Prec@1 82.600	Loss 0.419
Iteration [86]: lr=1.000e-03
Iteration [86] train aveloss=0.453 aveacc=81.590
Test[86]:Result* Prec@1 82.000	Loss 0.508
Iteration [87]: lr=1.000e-03
Iteration [87] train aveloss=0.430 aveacc=82.460
Test[87]:Result* Prec@1 78.800	Loss 0.505
Iteration [88]: lr=1.000e-03
Iteration [88] train aveloss=0.449 aveacc=82.210
Test[88]:Result* Prec@1 80.500	Loss 0.490
Iteration [89]: lr=1.000e-03
Iteration [89] train aveloss=0.442 aveacc=82.410
Test[89]:Result* Prec@1 79.100	Loss 0.494
Iteration [90]: lr=1.000e-03
Iteration [90] train aveloss=0.425 aveacc=82.650
Test[90]:Result* Prec@1 84.200	Loss 0.421
Iteration [91]: lr=1.000e-03
Iteration [91] train aveloss=0.426 aveacc=82.930
Test[91]:Result* Prec@1 79.600	Loss 0.504
Iteration [92]: lr=1.000e-03
Iteration [92] train aveloss=0.439 aveacc=82.460
Test[92]:Result* Prec@1 81.800	Loss 0.492
Iteration [93]: lr=1.000e-03
Iteration [93] train aveloss=0.444 aveacc=82.220
Test[93]:Result* Prec@1 82.800	Loss 0.415
Iteration [94]: lr=1.000e-03
Iteration [94] train aveloss=0.439 aveacc=83.040
Test[94]:Result* Prec@1 82.500	Loss 0.433
Iteration [95]: lr=1.000e-03
Iteration [95] train aveloss=0.429 aveacc=82.790
Test[95]:Result* Prec@1 78.300	Loss 0.541
Iteration [96]: lr=1.000e-03
Iteration [96] train aveloss=0.421 aveacc=83.000
Test[96]:Result* Prec@1 79.000	Loss 0.522
Iteration [97]: lr=1.000e-03
Iteration [97] train aveloss=0.420 aveacc=83.300
Test[97]:Result* Prec@1 81.300	Loss 0.469
Iteration [98]: lr=1.000e-03
Iteration [98] train aveloss=0.431 aveacc=82.560
Test[98]:Result* Prec@1 84.700	Loss 0.377
Iteration [99]: lr=1.000e-03
Iteration [99] train aveloss=0.437 aveacc=82.600
Test[99]:Result* Prec@1 81.000	Loss 0.476
Iteration [100]: lr=1.000e-03
Iteration [100] train aveloss=0.420 aveacc=82.920
Test[100]:Result* Prec@1 83.100	Loss 0.425
Iteration [101]: lr=1.000e-03
Iteration [101] train aveloss=0.431 aveacc=82.850
Test[101]:Result* Prec@1 82.900	Loss 0.429
Iteration [102]: lr=1.000e-03
Iteration [102] train aveloss=0.428 aveacc=82.920
Test[102]:Result* Prec@1 80.800	Loss 0.451
Iteration [103]: lr=1.000e-03
Iteration [103] train aveloss=0.412 aveacc=83.820
Test[103]:Result* Prec@1 82.300	Loss 0.472
Iteration [104]: lr=1.000e-03
Iteration [104] train aveloss=0.410 aveacc=83.400
Test[104]:Result* Prec@1 83.600	Loss 0.426
Iteration [105]: lr=1.000e-03
Iteration [105] train aveloss=0.420 aveacc=83.780
Test[105]:Result* Prec@1 84.300	Loss 0.405
Iteration [106]: lr=1.000e-03
Iteration [106] train aveloss=0.393 aveacc=84.330
Test[106]:Result* Prec@1 83.800	Loss 0.415
Iteration [107]: lr=1.000e-03
Iteration [107] train aveloss=0.407 aveacc=83.810
Test[107]:Result* Prec@1 82.500	Loss 0.430
Iteration [108]: lr=1.000e-03
Iteration [108] train aveloss=0.405 aveacc=83.960
Test[108]:Result* Prec@1 79.000	Loss 0.559
Iteration [109]: lr=1.000e-03
Iteration [109] train aveloss=0.395 aveacc=84.550
Test[109]:Result* Prec@1 83.300	Loss 0.399
Iteration [110]: lr=1.000e-03
Iteration [110] train aveloss=0.390 aveacc=84.820
Test[110]:Result* Prec@1 82.700	Loss 0.394
Iteration [111]: lr=1.000e-03
Iteration [111] train aveloss=0.397 aveacc=84.160
Test[111]:Result* Prec@1 82.700	Loss 0.448
Iteration [112]: lr=1.000e-03
Iteration [112] train aveloss=0.411 aveacc=83.760
Test[112]:Result* Prec@1 84.700	Loss 0.393
Iteration [113]: lr=1.000e-03
Iteration [113] train aveloss=0.397 aveacc=84.430
Test[113]:Result* Prec@1 85.900	Loss 0.390
Iteration [114]: lr=1.000e-03
Iteration [114] train aveloss=0.401 aveacc=84.370
Test[114]:Result* Prec@1 85.700	Loss 0.376
Iteration [115]: lr=1.000e-03
Iteration [115] train aveloss=0.388 aveacc=84.850
Test[115]:Result* Prec@1 81.600	Loss 0.425
Iteration [116]: lr=1.000e-03
Iteration [116] train aveloss=0.405 aveacc=84.180
Test[116]:Result* Prec@1 81.700	Loss 0.519
Iteration [117]: lr=1.000e-03
Iteration [117] train aveloss=0.398 aveacc=84.390
Test[117]:Result* Prec@1 81.300	Loss 0.473
Iteration [118]: lr=1.000e-03
Iteration [118] train aveloss=0.392 aveacc=84.540
Test[118]:Result* Prec@1 81.400	Loss 0.472
Iteration [119]: lr=1.000e-03
Iteration [119] train aveloss=0.392 aveacc=84.450
Test[119]:Result* Prec@1 79.100	Loss 0.536
Iteration [120]: lr=1.000e-03
Iteration [120] train aveloss=0.384 aveacc=85.390
Test[120]:Result* Prec@1 82.900	Loss 0.419
Iteration [121]: lr=1.000e-03
Iteration [121] train aveloss=0.388 aveacc=84.250
Test[121]:Result* Prec@1 83.500	Loss 0.434
Iteration [122]: lr=1.000e-03
Iteration [122] train aveloss=0.379 aveacc=85.390
Test[122]:Result* Prec@1 86.100	Loss 0.381
Iteration [123]: lr=1.000e-03
Iteration [123] train aveloss=0.391 aveacc=84.620
Test[123]:Result* Prec@1 85.900	Loss 0.357
Iteration [124]: lr=1.000e-03
Iteration [124] train aveloss=0.385 aveacc=84.860
Test[124]:Result* Prec@1 81.200	Loss 0.433
Iteration [125]: lr=1.000e-03
Iteration [125] train aveloss=0.379 aveacc=85.180
Test[125]:Result* Prec@1 83.600	Loss 0.385
Iteration [126]: lr=1.000e-03
Iteration [126] train aveloss=0.382 aveacc=85.020
Test[126]:Result* Prec@1 80.000	Loss 0.505
Iteration [127]: lr=1.000e-03
Iteration [127] train aveloss=0.384 aveacc=84.720
Test[127]:Result* Prec@1 83.700	Loss 0.394
Iteration [128]: lr=1.000e-03
Iteration [128] train aveloss=0.385 aveacc=85.200
Test[128]:Result* Prec@1 79.000	Loss 0.648
Iteration [129]: lr=1.000e-03
Iteration [129] train aveloss=0.387 aveacc=84.860
Test[129]:Result* Prec@1 84.300	Loss 0.402
Iteration [130]: lr=1.000e-03
Iteration [130] train aveloss=0.392 aveacc=84.590
Test[130]:Result* Prec@1 82.500	Loss 0.431
Iteration [131]: lr=1.000e-03
Iteration [131] train aveloss=0.374 aveacc=85.110
Test[131]:Result* Prec@1 83.900	Loss 0.408
Iteration [132]: lr=1.000e-03
Iteration [132] train aveloss=0.383 aveacc=84.760
Test[132]:Result* Prec@1 82.100	Loss 0.518
Iteration [133]: lr=1.000e-03
Iteration [133] train aveloss=0.374 aveacc=85.180
Test[133]:Result* Prec@1 83.700	Loss 0.427
Iteration [134]: lr=1.000e-03
Iteration [134] train aveloss=0.380 aveacc=85.020
Test[134]:Result* Prec@1 83.000	Loss 0.426
Iteration [135]: lr=1.000e-03
Iteration [135] train aveloss=0.377 aveacc=85.150
Test[135]:Result* Prec@1 83.300	Loss 0.420
Iteration [136]: lr=1.000e-03
Iteration [136] train aveloss=0.379 aveacc=85.260
Test[136]:Result* Prec@1 82.100	Loss 0.473
Iteration [137]: lr=1.000e-03
Iteration [137] train aveloss=0.363 aveacc=85.690
Test[137]:Result* Prec@1 82.300	Loss 0.439
Iteration [138]: lr=1.000e-03
Iteration [138] train aveloss=0.370 aveacc=85.760
Test[138]:Result* Prec@1 85.100	Loss 0.373
Iteration [139]: lr=1.000e-03
Iteration [139] train aveloss=0.375 aveacc=85.210
Test[139]:Result* Prec@1 81.600	Loss 0.473
Iteration [140]: lr=1.000e-03
Iteration [140] train aveloss=0.380 aveacc=84.790
Test[140]:Result* Prec@1 84.800	Loss 0.415
Iteration [141]: lr=1.000e-03
Iteration [141] train aveloss=0.364 aveacc=85.520
Test[141]:Result* Prec@1 84.100	Loss 0.449
Iteration [142]: lr=1.000e-03
Iteration [142] train aveloss=0.367 aveacc=85.750
Test[142]:Result* Prec@1 82.400	Loss 0.437
Iteration [143]: lr=1.000e-03
Iteration [143] train aveloss=0.363 aveacc=85.610
Test[143]:Result* Prec@1 81.900	Loss 0.435
Iteration [144]: lr=1.000e-03
Iteration [144] train aveloss=0.348 aveacc=86.510
Test[144]:Result* Prec@1 82.900	Loss 0.441
Iteration [145]: lr=1.000e-03
Iteration [145] train aveloss=0.366 aveacc=85.860
Test[145]:Result* Prec@1 84.800	Loss 0.407
Iteration [146]: lr=1.000e-03
Iteration [146] train aveloss=0.367 aveacc=85.360
Test[146]:Result* Prec@1 83.500	Loss 0.429
Iteration [147]: lr=1.000e-03
Iteration [147] train aveloss=0.363 aveacc=85.760
Test[147]:Result* Prec@1 84.300	Loss 0.385
Iteration [148]: lr=1.000e-03
Iteration [148] train aveloss=0.363 aveacc=85.450
Test[148]:Result* Prec@1 86.800	Loss 0.358
Iteration [149]: lr=1.000e-03
Iteration [149] train aveloss=0.363 aveacc=85.670
Test[149]:Result* Prec@1 84.900	Loss 0.435
Iteration [150]: lr=1.000e-03
Iteration [150] train aveloss=0.369 aveacc=85.820
Test[150]:Result* Prec@1 81.700	Loss 0.449
Iteration [151]: lr=1.000e-03
Iteration [151] train aveloss=0.364 aveacc=85.790
Test[151]:Result* Prec@1 85.100	Loss 0.377
Iteration [152]: lr=1.000e-03
Iteration [152] train aveloss=0.365 aveacc=85.140
Test[152]:Result* Prec@1 83.500	Loss 0.429
Iteration [153]: lr=1.000e-03
Iteration [153] train aveloss=0.341 aveacc=86.880
Test[153]:Result* Prec@1 86.800	Loss 0.348
Iteration [154]: lr=1.000e-03
Iteration [154] train aveloss=0.353 aveacc=86.210
Test[154]:Result* Prec@1 87.200	Loss 0.330
Iteration [155]: lr=1.000e-03
Iteration [155] train aveloss=0.351 aveacc=86.310
Test[155]:Result* Prec@1 80.200	Loss 0.519
Iteration [156]: lr=1.000e-03
Iteration [156] train aveloss=0.354 aveacc=86.370
Test[156]:Result* Prec@1 85.300	Loss 0.372
Iteration [157]: lr=1.000e-03
Iteration [157] train aveloss=0.368 aveacc=85.830
Test[157]:Result* Prec@1 83.900	Loss 0.391
Iteration [158]: lr=1.000e-03
Iteration [158] train aveloss=0.359 aveacc=85.880
Test[158]:Result* Prec@1 83.200	Loss 0.475
Iteration [159]: lr=1.000e-03
Iteration [159] train aveloss=0.363 aveacc=85.900
Test[159]:Result* Prec@1 84.100	Loss 0.429
Iteration [160]: lr=1.000e-03
Iteration [160] train aveloss=0.356 aveacc=86.250
Test[160]:Result* Prec@1 80.400	Loss 0.694
Iteration [161]: lr=1.000e-03
Iteration [161] train aveloss=0.371 aveacc=85.300
Test[161]:Result* Prec@1 85.900	Loss 0.389
Iteration [162]: lr=1.000e-03
Iteration [162] train aveloss=0.344 aveacc=86.790
Test[162]:Result* Prec@1 85.700	Loss 0.369
Iteration [163]: lr=1.000e-03
Iteration [163] train aveloss=0.342 aveacc=86.270
Test[163]:Result* Prec@1 82.700	Loss 0.415
Iteration [164]: lr=1.000e-03
Iteration [164] train aveloss=0.348 aveacc=86.390
Test[164]:Result* Prec@1 83.900	Loss 0.382
Iteration [165]: lr=1.000e-03
Iteration [165] train aveloss=0.346 aveacc=86.130
Test[165]:Result* Prec@1 86.000	Loss 0.357
Iteration [166]: lr=1.000e-03
Iteration [166] train aveloss=0.348 aveacc=86.210
Test[166]:Result* Prec@1 85.500	Loss 0.394
Iteration [167]: lr=1.000e-03
Iteration [167] train aveloss=0.354 aveacc=86.350
Test[167]:Result* Prec@1 88.100	Loss 0.335
Iteration [168]: lr=1.000e-03
Iteration [168] train aveloss=0.339 aveacc=86.560
Test[168]:Result* Prec@1 86.800	Loss 0.344
Iteration [169]: lr=1.000e-03
Iteration [169] train aveloss=0.356 aveacc=86.110
Test[169]:Result* Prec@1 84.900	Loss 0.376
Iteration [170]: lr=1.000e-03
Iteration [170] train aveloss=0.356 aveacc=86.210
Test[170]:Result* Prec@1 83.200	Loss 0.440
Iteration [171]: lr=1.000e-03
Iteration [171] train aveloss=0.344 aveacc=86.920
Test[171]:Result* Prec@1 83.300	Loss 0.420
Iteration [172]: lr=1.000e-03
Iteration [172] train aveloss=0.344 aveacc=86.710
Test[172]:Result* Prec@1 84.400	Loss 0.395
Iteration [173]: lr=1.000e-03
Iteration [173] train aveloss=0.354 aveacc=85.980
Test[173]:Result* Prec@1 85.100	Loss 0.382
Iteration [174]: lr=1.000e-03
Iteration [174] train aveloss=0.341 aveacc=86.710
Test[174]:Result* Prec@1 85.900	Loss 0.388
Iteration [175]: lr=1.000e-03
Iteration [175] train aveloss=0.352 aveacc=86.500
Test[175]:Result* Prec@1 82.900	Loss 0.405
Iteration [176]: lr=1.000e-03
Iteration [176] train aveloss=0.337 aveacc=87.220
Test[176]:Result* Prec@1 83.200	Loss 0.420
Iteration [177]: lr=1.000e-03
Iteration [177] train aveloss=0.351 aveacc=86.510
Test[177]:Result* Prec@1 78.300	Loss 0.534
Iteration [178]: lr=1.000e-03
Iteration [178] train aveloss=0.348 aveacc=86.560
Test[178]:Result* Prec@1 86.200	Loss 0.363
Iteration [179]: lr=1.000e-03
Iteration [179] train aveloss=0.341 aveacc=86.580
Test[179]:Result* Prec@1 84.800	Loss 0.380
Iteration [180]: lr=1.000e-03
Iteration [180] train aveloss=0.345 aveacc=86.220
Test[180]:Result* Prec@1 83.300	Loss 0.440
Iteration [181]: lr=1.000e-03
Iteration [181] train aveloss=0.330 aveacc=87.280
Test[181]:Result* Prec@1 87.000	Loss 0.343
Iteration [182]: lr=1.000e-03
Iteration [182] train aveloss=0.341 aveacc=86.740
Test[182]:Result* Prec@1 81.400	Loss 0.462
Iteration [183]: lr=1.000e-03
Iteration [183] train aveloss=0.344 aveacc=86.400
Test[183]:Result* Prec@1 84.000	Loss 0.415
Iteration [184]: lr=1.000e-03
Iteration [184] train aveloss=0.348 aveacc=86.210
Test[184]:Result* Prec@1 74.500	Loss 1.096
Iteration [185]: lr=1.000e-03
Iteration [185] train aveloss=0.340 aveacc=86.820
Test[185]:Result* Prec@1 85.400	Loss 0.405
Iteration [186]: lr=1.000e-03
Iteration [186] train aveloss=0.330 aveacc=87.560
Test[186]:Result* Prec@1 84.600	Loss 0.407
Iteration [187]: lr=1.000e-03
Iteration [187] train aveloss=0.337 aveacc=86.630
Test[187]:Result* Prec@1 66.800	Loss 1.879
Iteration [188]: lr=1.000e-03
Iteration [188] train aveloss=0.332 aveacc=86.890
Test[188]:Result* Prec@1 84.900	Loss 0.405
Iteration [189]: lr=1.000e-03
Iteration [189] train aveloss=0.330 aveacc=87.070
Test[189]:Result* Prec@1 84.900	Loss 0.385
Iteration [190]: lr=1.000e-03
Iteration [190] train aveloss=0.334 aveacc=87.260
Test[190]:Result* Prec@1 83.100	Loss 0.473
Iteration [191]: lr=1.000e-03
Iteration [191] train aveloss=0.331 aveacc=86.980
Test[191]:Result* Prec@1 83.900	Loss 0.406
Iteration [192]: lr=1.000e-03
Iteration [192] train aveloss=0.336 aveacc=86.530
Test[192]:Result* Prec@1 81.900	Loss 0.477
Iteration [193]: lr=1.000e-03
Iteration [193] train aveloss=0.325 aveacc=87.080
Test[193]:Result* Prec@1 87.400	Loss 0.319
Iteration [194]: lr=1.000e-03
Iteration [194] train aveloss=0.327 aveacc=87.270
Test[194]:Result* Prec@1 81.600	Loss 0.471
Iteration [195]: lr=1.000e-03
Iteration [195] train aveloss=0.338 aveacc=87.060
Test[195]:Result* Prec@1 84.000	Loss 0.406
Iteration [196]: lr=1.000e-03
Iteration [196] train aveloss=0.333 aveacc=87.300
Test[196]:Result* Prec@1 85.000	Loss 0.368
Iteration [197]: lr=1.000e-03
Iteration [197] train aveloss=0.331 aveacc=86.860
Test[197]:Result* Prec@1 84.900	Loss 0.382
Iteration [198]: lr=1.000e-03
Iteration [198] train aveloss=0.342 aveacc=86.580
Test[198]:Result* Prec@1 84.300	Loss 0.398
Iteration [199]: lr=1.000e-03
Iteration [199] train aveloss=0.325 aveacc=87.350
Test[199]:Result* Prec@1 83.800	Loss 0.406
Iteration [200]: lr=1.000e-03
Iteration [200] train aveloss=0.333 aveacc=87.050
Test[200]:Result* Prec@1 77.900	Loss 0.713
Iteration [201]: lr=1.000e-03
Iteration [201] train aveloss=0.324 aveacc=87.390
Test[201]:Result* Prec@1 73.700	Loss 1.004
Iteration [202]: lr=1.000e-03
Iteration [202] train aveloss=0.333 aveacc=87.070
Test[202]:Result* Prec@1 86.300	Loss 0.353
Iteration [203]: lr=1.000e-03
Iteration [203] train aveloss=0.323 aveacc=87.410
Test[203]:Result* Prec@1 55.000	Loss 3.883
Iteration [204]: lr=1.000e-03
Iteration [204] train aveloss=0.339 aveacc=86.930
Test[204]:Result* Prec@1 83.000	Loss 0.428
Iteration [205]: lr=1.000e-03
Iteration [205] train aveloss=0.331 aveacc=86.920
Test[205]:Result* Prec@1 82.100	Loss 0.537
Iteration [206]: lr=1.000e-03
Iteration [206] train aveloss=0.325 aveacc=87.090
Test[206]:Result* Prec@1 85.600	Loss 0.385
Iteration [207]: lr=1.000e-03
Iteration [207] train aveloss=0.327 aveacc=87.390
Test[207]:Result* Prec@1 86.700	Loss 0.358
Iteration [208]: lr=1.000e-03
Iteration [208] train aveloss=0.316 aveacc=87.590
Test[208]:Result* Prec@1 84.500	Loss 0.408
Iteration [209]: lr=1.000e-03
Iteration [209] train aveloss=0.327 aveacc=87.450
Test[209]:Result* Prec@1 77.300	Loss 0.575
Iteration [210]: lr=1.000e-03
Iteration [210] train aveloss=0.329 aveacc=87.360
Test[210]:Result* Prec@1 85.800	Loss 0.370
Iteration [211]: lr=1.000e-03
Iteration [211] train aveloss=0.324 aveacc=87.650
Test[211]:Result* Prec@1 83.300	Loss 0.410
Iteration [212]: lr=1.000e-03
Iteration [212] train aveloss=0.330 aveacc=87.130
Test[212]:Result* Prec@1 85.900	Loss 0.375
Iteration [213]: lr=1.000e-03
Iteration [213] train aveloss=0.324 aveacc=87.480
Test[213]:Result* Prec@1 84.500	Loss 0.396
Iteration [214]: lr=1.000e-03
Iteration [214] train aveloss=0.314 aveacc=87.740
Test[214]:Result* Prec@1 83.700	Loss 0.407
Iteration [215]: lr=1.000e-03
Iteration [215] train aveloss=0.325 aveacc=86.890
Test[215]:Result* Prec@1 80.500	Loss 0.489
Iteration [216]: lr=1.000e-03
Iteration [216] train aveloss=0.323 aveacc=87.430
Test[216]:Result* Prec@1 82.600	Loss 0.442
Iteration [217]: lr=1.000e-03
Iteration [217] train aveloss=0.326 aveacc=87.070
Test[217]:Result* Prec@1 82.900	Loss 0.415
Iteration [218]: lr=1.000e-03
Iteration [218] train aveloss=0.319 aveacc=87.480
Test[218]:Result* Prec@1 86.600	Loss 0.359
Iteration [219]: lr=1.000e-03
Iteration [219] train aveloss=0.328 aveacc=87.150
Test[219]:Result* Prec@1 85.000	Loss 0.384
Iteration [220]: lr=1.000e-03
Iteration [220] train aveloss=0.325 aveacc=87.240
Test[220]:Result* Prec@1 85.500	Loss 0.357
Iteration [221]: lr=1.000e-03
Iteration [221] train aveloss=0.321 aveacc=87.600
Test[221]:Result* Prec@1 87.100	Loss 0.334
Iteration [222]: lr=1.000e-03
Iteration [222] train aveloss=0.325 aveacc=87.050
Test[222]:Result* Prec@1 83.400	Loss 0.423
Iteration [223]: lr=1.000e-03
Iteration [223] train aveloss=0.324 aveacc=87.280
Test[223]:Result* Prec@1 85.300	Loss 0.392
Iteration [224]: lr=1.000e-03
Iteration [224] train aveloss=0.313 aveacc=88.110
Test[224]:Result* Prec@1 84.500	Loss 0.390
Iteration [225]: lr=1.000e-03
Iteration [225] train aveloss=0.313 aveacc=87.870
Test[225]:Result* Prec@1 85.100	Loss 0.386
Iteration [226]: lr=1.000e-03
Iteration [226] train aveloss=0.327 aveacc=87.450
Test[226]:Result* Prec@1 84.700	Loss 0.392
Iteration [227]: lr=1.000e-03
Iteration [227] train aveloss=0.308 aveacc=87.970
Test[227]:Result* Prec@1 84.400	Loss 0.457
Iteration [228]: lr=1.000e-03
Iteration [228] train aveloss=0.321 aveacc=87.500
Test[228]:Result* Prec@1 85.000	Loss 0.362
Iteration [229]: lr=1.000e-03
Iteration [229] train aveloss=0.316 aveacc=87.760
Test[229]:Result* Prec@1 74.300	Loss 1.261
Iteration [230]: lr=1.000e-03
Iteration [230] train aveloss=0.320 aveacc=87.690
Test[230]:Result* Prec@1 78.900	Loss 0.705
Iteration [231]: lr=1.000e-03
Iteration [231] train aveloss=0.318 aveacc=87.850
Test[231]:Result* Prec@1 86.400	Loss 0.353
Iteration [232]: lr=1.000e-03
Iteration [232] train aveloss=0.315 aveacc=87.740
Test[232]:Result* Prec@1 83.800	Loss 0.437
Iteration [233]: lr=1.000e-03
Iteration [233] train aveloss=0.311 aveacc=88.120
Test[233]:Result* Prec@1 85.500	Loss 0.377
Iteration [234]: lr=1.000e-03
Iteration [234] train aveloss=0.302 aveacc=88.410
Test[234]:Result* Prec@1 87.600	Loss 0.303
Iteration [235]: lr=1.000e-03
Iteration [235] train aveloss=0.302 aveacc=88.390
Test[235]:Result* Prec@1 73.600	Loss 0.932
Iteration [236]: lr=1.000e-03
Iteration [236] train aveloss=0.320 aveacc=87.420
Test[236]:Result* Prec@1 86.100	Loss 0.339
Iteration [237]: lr=1.000e-03
Iteration [237] train aveloss=0.315 aveacc=87.790
Test[237]:Result* Prec@1 76.000	Loss 1.096
Iteration [238]: lr=1.000e-03
Iteration [238] train aveloss=0.311 aveacc=88.040
Test[238]:Result* Prec@1 85.800	Loss 0.372
Iteration [239]: lr=1.000e-03
Iteration [239] train aveloss=0.310 aveacc=88.250
Test[239]:Result* Prec@1 84.600	Loss 0.397
Iteration [240]: lr=1.000e-03
Iteration [240] train aveloss=0.300 aveacc=88.220
Test[240]:Result* Prec@1 55.500	Loss 5.103
Iteration [241]: lr=1.000e-03
Iteration [241] train aveloss=0.302 aveacc=88.120
Test[241]:Result* Prec@1 84.600	Loss 0.412
Iteration [242]: lr=1.000e-03
Iteration [242] train aveloss=0.309 aveacc=88.050
Test[242]:Result* Prec@1 85.600	Loss 0.381
Iteration [243]: lr=1.000e-03
Iteration [243] train aveloss=0.308 aveacc=87.990
Test[243]:Result* Prec@1 86.300	Loss 0.348
Iteration [244]: lr=1.000e-03
Iteration [244] train aveloss=0.293 aveacc=89.000
Test[244]:Result* Prec@1 85.900	Loss 0.353
Iteration [245]: lr=1.000e-03
Iteration [245] train aveloss=0.305 aveacc=88.300
Test[245]:Result* Prec@1 81.900	Loss 0.423
Iteration [246]: lr=1.000e-03
Iteration [246] train aveloss=0.308 aveacc=87.840
Test[246]:Result* Prec@1 84.300	Loss 0.389
Iteration [247]: lr=1.000e-03
Iteration [247] train aveloss=0.298 aveacc=88.380
Test[247]:Result* Prec@1 87.000	Loss 0.354
Iteration [248]: lr=1.000e-03
Iteration [248] train aveloss=0.310 aveacc=87.910
Test[248]:Result* Prec@1 75.900	Loss 0.634
Iteration [249]: lr=1.000e-03
Iteration [249] train aveloss=0.300 aveacc=88.000
Test[249]:Result* Prec@1 85.800	Loss 0.352
Iteration [250]: lr=1.000e-03
Iteration [250] train aveloss=0.290 aveacc=88.740
Test[250]:Result* Prec@1 87.900	Loss 0.308
Iteration [251]: lr=1.000e-03
Iteration [251] train aveloss=0.308 aveacc=87.670
Test[251]:Result* Prec@1 85.400	Loss 0.372
Iteration [252]: lr=1.000e-03
Iteration [252] train aveloss=0.294 aveacc=88.860
Test[252]:Result* Prec@1 86.600	Loss 0.349
Iteration [253]: lr=1.000e-03
Iteration [253] train aveloss=0.304 aveacc=88.530
Test[253]:Result* Prec@1 85.300	Loss 0.379
Iteration [254]: lr=1.000e-03
Iteration [254] train aveloss=0.303 aveacc=88.550
Test[254]:Result* Prec@1 84.400	Loss 0.395
Iteration [255]: lr=1.000e-03
Iteration [255] train aveloss=0.292 aveacc=88.940
Test[255]:Result* Prec@1 74.900	Loss 1.040
Iteration [256]: lr=1.000e-03
Iteration [256] train aveloss=0.308 aveacc=87.830
Test[256]:Result* Prec@1 83.100	Loss 0.453
Iteration [257]: lr=1.000e-03
Iteration [257] train aveloss=0.302 aveacc=88.090
Test[257]:Result* Prec@1 81.100	Loss 0.470
Iteration [258]: lr=1.000e-03
Iteration [258] train aveloss=0.296 aveacc=88.390
Test[258]:Result* Prec@1 53.100	Loss 4.688
Iteration [259]: lr=1.000e-03
Iteration [259] train aveloss=0.301 aveacc=88.310
Test[259]:Result* Prec@1 56.600	Loss 3.432
Iteration [260]: lr=1.000e-03
Iteration [260] train aveloss=0.298 aveacc=88.300
Test[260]:Result* Prec@1 84.500	Loss 0.391
Iteration [261]: lr=1.000e-03
Iteration [261] train aveloss=0.295 aveacc=88.420
Test[261]:Result* Prec@1 86.400	Loss 0.375
Iteration [262]: lr=1.000e-03
Iteration [262] train aveloss=0.302 aveacc=88.340
Test[262]:Result* Prec@1 69.500	Loss 1.245
Iteration [263]: lr=1.000e-03
Iteration [263] train aveloss=0.302 aveacc=88.160
Test[263]:Result* Prec@1 82.500	Loss 0.427
Iteration [264]: lr=1.000e-03
Iteration [264] train aveloss=0.293 aveacc=88.540
Test[264]:Result* Prec@1 85.000	Loss 0.413
Iteration [265]: lr=1.000e-03
Iteration [265] train aveloss=0.297 aveacc=88.450
Test[265]:Result* Prec@1 79.000	Loss 0.546
Iteration [266]: lr=1.000e-03
Iteration [266] train aveloss=0.282 aveacc=88.970
Test[266]:Result* Prec@1 75.900	Loss 0.950
Iteration [267]: lr=1.000e-03
Iteration [267] train aveloss=0.286 aveacc=89.120
Test[267]:Result* Prec@1 86.700	Loss 0.343
Iteration [268]: lr=1.000e-03
Iteration [268] train aveloss=0.302 aveacc=88.540
Test[268]:Result* Prec@1 85.800	Loss 0.361
Iteration [269]: lr=1.000e-03
Iteration [269] train aveloss=0.294 aveacc=88.670
Test[269]:Result* Prec@1 86.600	Loss 0.358
Iteration [270]: lr=1.000e-03
Iteration [270] train aveloss=0.286 aveacc=89.440
Test[270]:Result* Prec@1 85.000	Loss 0.371
Iteration [271]: lr=1.000e-03
Iteration [271] train aveloss=0.297 aveacc=88.160
Test[271]:Result* Prec@1 76.900	Loss 0.972
Iteration [272]: lr=1.000e-03
Iteration [272] train aveloss=0.293 aveacc=88.710
Test[272]:Result* Prec@1 74.500	Loss 0.798
Iteration [273]: lr=1.000e-03
Iteration [273] train aveloss=0.295 aveacc=88.530
Test[273]:Result* Prec@1 86.700	Loss 0.328
Iteration [274]: lr=1.000e-03
Iteration [274] train aveloss=0.283 aveacc=89.170
Test[274]:Result* Prec@1 68.300	Loss 0.912
Iteration [275]: lr=1.000e-03
Iteration [275] train aveloss=0.286 aveacc=89.270
Test[275]:Result* Prec@1 83.100	Loss 0.528
Iteration [276]: lr=1.000e-03
Iteration [276] train aveloss=0.291 aveacc=89.060
Test[276]:Result* Prec@1 84.800	Loss 0.382
Iteration [277]: lr=1.000e-03
Iteration [277] train aveloss=0.280 aveacc=89.430
Test[277]:Result* Prec@1 64.500	Loss 2.230
Iteration [278]: lr=1.000e-03
Iteration [278] train aveloss=0.288 aveacc=88.960
Test[278]:Result* Prec@1 84.000	Loss 0.446
Iteration [279]: lr=1.000e-03
Iteration [279] train aveloss=0.280 aveacc=89.120
Test[279]:Result* Prec@1 64.600	Loss 2.299
Iteration [280]: lr=1.000e-03
Iteration [280] train aveloss=0.296 aveacc=88.440
Test[280]:Result* Prec@1 83.800	Loss 0.502
Iteration [281]: lr=1.000e-03
Iteration [281] train aveloss=0.289 aveacc=88.680
Test[281]:Result* Prec@1 73.000	Loss 1.374
Iteration [282]: lr=1.000e-03
Iteration [282] train aveloss=0.283 aveacc=89.000
Test[282]:Result* Prec@1 86.500	Loss 0.387
Iteration [283]: lr=1.000e-03
Iteration [283] train aveloss=0.294 aveacc=88.650
Test[283]:Result* Prec@1 83.500	Loss 0.376
Iteration [284]: lr=1.000e-03
Iteration [284] train aveloss=0.291 aveacc=88.580
Test[284]:Result* Prec@1 76.700	Loss 0.831
Iteration [285]: lr=1.000e-03
Iteration [285] train aveloss=0.289 aveacc=88.800
Test[285]:Result* Prec@1 87.700	Loss 0.302
Iteration [286]: lr=1.000e-03
Iteration [286] train aveloss=0.279 aveacc=89.100
Test[286]:Result* Prec@1 84.500	Loss 0.412
Iteration [287]: lr=1.000e-03
Iteration [287] train aveloss=0.292 aveacc=89.040
Test[287]:Result* Prec@1 47.300	Loss 7.311
Iteration [288]: lr=1.000e-03
Iteration [288] train aveloss=0.282 aveacc=89.290
Test[288]:Result* Prec@1 77.900	Loss 0.607
Iteration [289]: lr=1.000e-03
Iteration [289] train aveloss=0.290 aveacc=88.780
Test[289]:Result* Prec@1 78.400	Loss 0.541
Iteration [290]: lr=1.000e-03
Iteration [290] train aveloss=0.277 aveacc=89.000
Test[290]:Result* Prec@1 56.500	Loss 4.201
Iteration [291]: lr=1.000e-03
Iteration [291] train aveloss=0.283 aveacc=89.270
Test[291]:Result* Prec@1 85.000	Loss 0.381
Iteration [292]: lr=1.000e-03
Iteration [292] train aveloss=0.271 aveacc=89.700
Test[292]:Result* Prec@1 80.500	Loss 0.526
Iteration [293]: lr=1.000e-03
Iteration [293] train aveloss=0.289 aveacc=88.730
Test[293]:Result* Prec@1 78.400	Loss 0.546
Iteration [294]: lr=1.000e-03
Iteration [294] train aveloss=0.277 aveacc=89.260
Test[294]:Result* Prec@1 46.300	Loss 8.920
Iteration [295]: lr=1.000e-03
Iteration [295] train aveloss=0.263 aveacc=90.100
Test[295]:Result* Prec@1 81.600	Loss 0.473
Iteration [296]: lr=1.000e-03
Iteration [296] train aveloss=0.271 aveacc=89.600
Test[296]:Result* Prec@1 74.000	Loss 0.728
Iteration [297]: lr=1.000e-03
Iteration [297] train aveloss=0.275 aveacc=89.680
Test[297]:Result* Prec@1 41.000	Loss 12.024
Iteration [298]: lr=1.000e-03
Iteration [298] train aveloss=0.272 aveacc=89.660
Test[298]:Result* Prec@1 85.200	Loss 0.382
Iteration [299]: lr=1.000e-03
Iteration [299] train aveloss=0.285 aveacc=88.990
Test[299]:Result* Prec@1 49.300	Loss 6.753
Iteration [300]: lr=1.000e-03
Iteration [300] train aveloss=0.286 aveacc=88.620
Test[300]:Result* Prec@1 44.900	Loss 9.932
Iteration [301]: lr=1.000e-03
Iteration [301] train aveloss=0.283 aveacc=89.190
Test[301]:Result* Prec@1 86.900	Loss 0.343
Iteration [302]: lr=1.000e-03
Iteration [302] train aveloss=0.280 aveacc=89.430
Test[302]:Result* Prec@1 81.000	Loss 0.606
Iteration [303]: lr=1.000e-03
Iteration [303] train aveloss=0.280 aveacc=88.870
Test[303]:Result* Prec@1 84.000	Loss 0.421
Iteration [304]: lr=1.000e-03
Iteration [304] train aveloss=0.275 aveacc=89.290
Test[304]:Result* Prec@1 85.000	Loss 0.380
Iteration [305]: lr=1.000e-03
Iteration [305] train aveloss=0.276 aveacc=89.390
Test[305]:Result* Prec@1 87.400	Loss 0.344
Iteration [306]: lr=1.000e-03
Iteration [306] train aveloss=0.279 aveacc=89.060
Test[306]:Result* Prec@1 85.100	Loss 0.410
Iteration [307]: lr=1.000e-03
Iteration [307] train aveloss=0.271 aveacc=89.720
Test[307]:Result* Prec@1 87.300	Loss 0.341
Iteration [308]: lr=1.000e-03
Iteration [308] train aveloss=0.284 aveacc=89.240
Test[308]:Result* Prec@1 77.000	Loss 0.698
Iteration [309]: lr=1.000e-03
Iteration [309] train aveloss=0.270 aveacc=89.510
Test[309]:Result* Prec@1 80.600	Loss 0.467
Iteration [310]: lr=1.000e-03
Iteration [310] train aveloss=0.276 aveacc=89.600
Test[310]:Result* Prec@1 82.200	Loss 0.458
Iteration [311]: lr=1.000e-03
Iteration [311] train aveloss=0.268 aveacc=89.520
Test[311]:Result* Prec@1 88.300	Loss 0.340
Iteration [312]: lr=1.000e-03
Iteration [312] train aveloss=0.275 aveacc=89.500
Test[312]:Result* Prec@1 85.400	Loss 0.396
Iteration [313]: lr=1.000e-03
Iteration [313] train aveloss=0.271 aveacc=89.660
Test[313]:Result* Prec@1 87.900	Loss 0.328
Iteration [314]: lr=1.000e-03
Iteration [314] train aveloss=0.268 aveacc=89.830
Test[314]:Result* Prec@1 70.400	Loss 1.560
Iteration [315]: lr=1.000e-03
Iteration [315] train aveloss=0.272 aveacc=89.460
Test[315]:Result* Prec@1 67.400	Loss 1.614
Iteration [316]: lr=1.000e-03
Iteration [316] train aveloss=0.270 aveacc=89.660
Test[316]:Result* Prec@1 85.600	Loss 0.365
Iteration [317]: lr=1.000e-03
Iteration [317] train aveloss=0.270 aveacc=89.580
Test[317]:Result* Prec@1 86.200	Loss 0.365
Iteration [318]: lr=1.000e-03
Iteration [318] train aveloss=0.275 aveacc=89.720
Test[318]:Result* Prec@1 80.800	Loss 0.488
Iteration [319]: lr=1.000e-03
Iteration [319] train aveloss=0.263 aveacc=90.160
Test[319]:Result* Prec@1 84.000	Loss 0.440
Iteration [320]: lr=1.000e-03
Iteration [320] train aveloss=0.282 aveacc=89.070
Test[320]:Result* Prec@1 85.600	Loss 0.388
Iteration [321]: lr=1.000e-03
Iteration [321] train aveloss=0.277 aveacc=89.040
Test[321]:Result* Prec@1 84.500	Loss 0.427
Iteration [322]: lr=1.000e-03
Iteration [322] train aveloss=0.270 aveacc=89.270
Test[322]:Result* Prec@1 47.900	Loss 7.543
Iteration [323]: lr=1.000e-03
Iteration [323] train aveloss=0.271 aveacc=89.780
Test[323]:Result* Prec@1 85.200	Loss 0.396
Iteration [324]: lr=1.000e-03
Iteration [324] train aveloss=0.274 aveacc=89.570
Test[324]:Result* Prec@1 83.800	Loss 0.399
Iteration [325]: lr=1.000e-03
Iteration [325] train aveloss=0.266 aveacc=89.800
Test[325]:Result* Prec@1 85.200	Loss 0.368
Iteration [326]: lr=1.000e-03
Iteration [326] train aveloss=0.267 aveacc=90.040
Test[326]:Result* Prec@1 45.000	Loss 12.132
Iteration [327]: lr=1.000e-03
Iteration [327] train aveloss=0.267 aveacc=89.820
Test[327]:Result* Prec@1 87.400	Loss 0.309
Iteration [328]: lr=1.000e-03
Iteration [328] train aveloss=0.258 aveacc=90.070
Test[328]:Result* Prec@1 69.200	Loss 1.440
Iteration [329]: lr=1.000e-03
Iteration [329] train aveloss=0.264 aveacc=89.560
Test[329]:Result* Prec@1 84.100	Loss 0.459
Iteration [330]: lr=1.000e-03
Iteration [330] train aveloss=0.258 aveacc=90.190
Test[330]:Result* Prec@1 78.600	Loss 0.528
Iteration [331]: lr=1.000e-03
Iteration [331] train aveloss=0.273 aveacc=89.460
Test[331]:Result* Prec@1 85.600	Loss 0.387
Iteration [332]: lr=1.000e-03
Iteration [332] train aveloss=0.253 aveacc=90.140
Test[332]:Result* Prec@1 86.300	Loss 0.401
Iteration [333]: lr=1.000e-03
Iteration [333] train aveloss=0.267 aveacc=89.950
Test[333]:Result* Prec@1 84.700	Loss 0.354
Iteration [334]: lr=1.000e-03
Iteration [334] train aveloss=0.269 aveacc=89.370
Test[334]:Result* Prec@1 65.500	Loss 2.042
Iteration [335]: lr=1.000e-03
Iteration [335] train aveloss=0.257 aveacc=90.200
Test[335]:Result* Prec@1 85.100	Loss 0.378
Iteration [336]: lr=1.000e-03
Iteration [336] train aveloss=0.260 aveacc=90.250
Test[336]:Result* Prec@1 77.200	Loss 0.653
Iteration [337]: lr=1.000e-03
Iteration [337] train aveloss=0.255 aveacc=90.210
Test[337]:Result* Prec@1 79.300	Loss 0.590
Iteration [338]: lr=1.000e-03
Iteration [338] train aveloss=0.266 aveacc=89.850
Test[338]:Result* Prec@1 76.700	Loss 0.713
Iteration [339]: lr=1.000e-03
Iteration [339] train aveloss=0.250 aveacc=90.460
Test[339]:Result* Prec@1 83.800	Loss 0.430
Iteration [340]: lr=1.000e-03
Iteration [340] train aveloss=0.265 aveacc=90.020
Test[340]:Result* Prec@1 67.600	Loss 1.007
Iteration [341]: lr=1.000e-03
Iteration [341] train aveloss=0.258 aveacc=90.240
Test[341]:Result* Prec@1 81.300	Loss 0.624
Iteration [342]: lr=1.000e-03
Iteration [342] train aveloss=0.267 aveacc=90.330
Test[342]:Result* Prec@1 39.600	Loss 12.274
Iteration [343]: lr=1.000e-03
Iteration [343] train aveloss=0.260 aveacc=90.190
Test[343]:Result* Prec@1 85.300	Loss 0.400
Iteration [344]: lr=1.000e-03
Iteration [344] train aveloss=0.264 aveacc=89.770
Test[344]:Result* Prec@1 85.800	Loss 0.370
Iteration [345]: lr=1.000e-03
Iteration [345] train aveloss=0.252 aveacc=90.390
Test[345]:Result* Prec@1 83.100	Loss 0.621
Iteration [346]: lr=1.000e-03
Iteration [346] train aveloss=0.257 aveacc=90.360
Test[346]:Result* Prec@1 42.200	Loss 13.378
Iteration [347]: lr=1.000e-03
Iteration [347] train aveloss=0.266 aveacc=89.900
Test[347]:Result* Prec@1 67.200	Loss 1.983
Iteration [348]: lr=1.000e-03
Iteration [348] train aveloss=0.251 aveacc=90.540
Test[348]:Result* Prec@1 83.000	Loss 0.425
Iteration [349]: lr=1.000e-03
Iteration [349] train aveloss=0.254 aveacc=90.110
Test[349]:Result* Prec@1 68.900	Loss 1.944
Iteration [350]: lr=1.000e-03
Iteration [350] train aveloss=0.251 aveacc=90.320
Test[350]:Result* Prec@1 85.500	Loss 0.420
Iteration [351]: lr=1.000e-03
Iteration [351] train aveloss=0.256 aveacc=90.280
Test[351]:Result* Prec@1 87.900	Loss 0.346
Iteration [352]: lr=1.000e-03
Iteration [352] train aveloss=0.243 aveacc=90.860
Test[352]:Result* Prec@1 85.100	Loss 0.390
Iteration [353]: lr=1.000e-03
Iteration [353] train aveloss=0.253 aveacc=90.460
Test[353]:Result* Prec@1 84.700	Loss 0.419
Iteration [354]: lr=1.000e-03
Iteration [354] train aveloss=0.259 aveacc=89.830
Test[354]:Result* Prec@1 85.600	Loss 0.377
Iteration [355]: lr=1.000e-03
Iteration [355] train aveloss=0.244 aveacc=90.710
Test[355]:Result* Prec@1 70.600	Loss 1.331
Iteration [356]: lr=1.000e-03
Iteration [356] train aveloss=0.271 aveacc=89.490
Test[356]:Result* Prec@1 80.500	Loss 0.581
Iteration [357]: lr=1.000e-03
Iteration [357] train aveloss=0.258 aveacc=90.050
Test[357]:Result* Prec@1 71.100	Loss 0.947
Iteration [358]: lr=1.000e-03
Iteration [358] train aveloss=0.244 aveacc=90.610
Test[358]:Result* Prec@1 84.800	Loss 0.421
Iteration [359]: lr=1.000e-03
Iteration [359] train aveloss=0.265 aveacc=89.810
Test[359]:Result* Prec@1 85.000	Loss 0.399
Iteration [360]: lr=1.000e-03
Iteration [360] train aveloss=0.244 aveacc=91.020
Test[360]:Result* Prec@1 85.000	Loss 0.393
Iteration [361]: lr=1.000e-03
Iteration [361] train aveloss=0.252 aveacc=90.400
Test[361]:Result* Prec@1 57.000	Loss 4.272
Iteration [362]: lr=1.000e-03
Iteration [362] train aveloss=0.253 aveacc=90.440
Test[362]:Result* Prec@1 77.400	Loss 0.670
Iteration [363]: lr=1.000e-03
Iteration [363] train aveloss=0.255 aveacc=90.420
Test[363]:Result* Prec@1 84.100	Loss 0.461
Iteration [364]: lr=1.000e-03
Iteration [364] train aveloss=0.252 aveacc=90.240
Test[364]:Result* Prec@1 84.300	Loss 0.500
Iteration [365]: lr=1.000e-03
Iteration [365] train aveloss=0.247 aveacc=90.990
Test[365]:Result* Prec@1 72.000	Loss 1.744
Iteration [366]: lr=1.000e-03
Iteration [366] train aveloss=0.253 aveacc=90.700
Test[366]:Result* Prec@1 75.800	Loss 0.753
Iteration [367]: lr=1.000e-03
Iteration [367] train aveloss=0.246 aveacc=90.620
Test[367]:Result* Prec@1 85.500	Loss 0.446
Iteration [368]: lr=1.000e-03
Iteration [368] train aveloss=0.243 aveacc=90.750
Test[368]:Result* Prec@1 83.900	Loss 0.426
Iteration [369]: lr=1.000e-03
Iteration [369] train aveloss=0.252 aveacc=90.190
Test[369]:Result* Prec@1 84.700	Loss 0.409
Iteration [370]: lr=1.000e-03
Iteration [370] train aveloss=0.253 aveacc=90.250
Test[370]:Result* Prec@1 86.100	Loss 0.365
Iteration [371]: lr=1.000e-03
Iteration [371] train aveloss=0.251 aveacc=90.770
Test[371]:Result* Prec@1 69.300	Loss 1.722
Iteration [372]: lr=1.000e-03
Iteration [372] train aveloss=0.248 aveacc=90.890
Test[372]:Result* Prec@1 74.500	Loss 0.806
Iteration [373]: lr=1.000e-03
Iteration [373] train aveloss=0.243 aveacc=90.830
Test[373]:Result* Prec@1 85.700	Loss 0.377
Iteration [374]: lr=1.000e-03
Iteration [374] train aveloss=0.249 aveacc=90.640
Test[374]:Result* Prec@1 79.700	Loss 0.535
Iteration [375]: lr=1.000e-03
Iteration [375] train aveloss=0.240 aveacc=91.060
Test[375]:Result* Prec@1 86.000	Loss 0.379
Iteration [376]: lr=1.000e-03
Iteration [376] train aveloss=0.245 aveacc=90.740
Test[376]:Result* Prec@1 84.500	Loss 0.446
Iteration [377]: lr=1.000e-03
Iteration [377] train aveloss=0.241 aveacc=91.050
Test[377]:Result* Prec@1 66.000	Loss 2.398
Iteration [378]: lr=1.000e-03
Iteration [378] train aveloss=0.241 aveacc=90.880
Test[378]:Result* Prec@1 86.200	Loss 0.377
Iteration [379]: lr=1.000e-03
Iteration [379] train aveloss=0.244 aveacc=90.690
Test[379]:Result* Prec@1 84.000	Loss 0.458
Iteration [380]: lr=1.000e-03
Iteration [380] train aveloss=0.244 aveacc=90.990
Test[380]:Result* Prec@1 86.100	Loss 0.386
Iteration [381]: lr=1.000e-03
Iteration [381] train aveloss=0.236 aveacc=91.040
Test[381]:Result* Prec@1 83.100	Loss 0.466
Iteration [382]: lr=1.000e-03
Iteration [382] train aveloss=0.235 aveacc=91.120
Test[382]:Result* Prec@1 51.800	Loss 5.644
Iteration [383]: lr=1.000e-03
Iteration [383] train aveloss=0.245 aveacc=90.900
Test[383]:Result* Prec@1 83.900	Loss 0.432
Iteration [384]: lr=1.000e-03
Iteration [384] train aveloss=0.245 aveacc=90.610
Test[384]:Result* Prec@1 42.500	Loss 11.027
Iteration [385]: lr=1.000e-03
Iteration [385] train aveloss=0.233 aveacc=91.350
Test[385]:Result* Prec@1 84.900	Loss 0.404
Iteration [386]: lr=1.000e-03
Iteration [386] train aveloss=0.237 aveacc=90.870
Test[386]:Result* Prec@1 85.400	Loss 0.392
Iteration [387]: lr=1.000e-03
Iteration [387] train aveloss=0.233 aveacc=91.090
Test[387]:Result* Prec@1 84.400	Loss 0.454
Iteration [388]: lr=1.000e-03
Iteration [388] train aveloss=0.246 aveacc=90.680
Test[388]:Result* Prec@1 58.300	Loss 4.107
Iteration [389]: lr=1.000e-03
Iteration [389] train aveloss=0.237 aveacc=90.970
Test[389]:Result* Prec@1 69.500	Loss 2.124
Iteration [390]: lr=1.000e-03
Iteration [390] train aveloss=0.227 aveacc=91.610
Test[390]:Result* Prec@1 84.000	Loss 0.395
Iteration [391]: lr=1.000e-03
Iteration [391] train aveloss=0.236 aveacc=91.010
Test[391]:Result* Prec@1 81.500	Loss 0.511
Iteration [392]: lr=1.000e-03
Iteration [392] train aveloss=0.239 aveacc=90.860
Test[392]:Result* Prec@1 84.700	Loss 0.423
Iteration [393]: lr=1.000e-03
Iteration [393] train aveloss=0.232 aveacc=91.130
Test[393]:Result* Prec@1 77.000	Loss 0.688
Iteration [394]: lr=1.000e-03
Iteration [394] train aveloss=0.239 aveacc=90.840
Test[394]:Result* Prec@1 80.100	Loss 0.810
Iteration [395]: lr=1.000e-03
Iteration [395] train aveloss=0.232 aveacc=90.880
Test[395]:Result* Prec@1 84.800	Loss 0.411
Iteration [396]: lr=1.000e-03
Iteration [396] train aveloss=0.236 aveacc=91.230
Test[396]:Result* Prec@1 48.400	Loss 10.558
Iteration [397]: lr=1.000e-03
Iteration [397] train aveloss=0.236 aveacc=91.310
Test[397]:Result* Prec@1 86.300	Loss 0.362
Iteration [398]: lr=1.000e-03
Iteration [398] train aveloss=0.238 aveacc=91.130
Test[398]:Result* Prec@1 82.400	Loss 0.463
Iteration [399]: lr=1.000e-03
Iteration [399] train aveloss=0.234 aveacc=90.800
Test[399]:Result* Prec@1 85.900	Loss 0.381
Iteration [400]: lr=1.000e-03
Iteration [400] train aveloss=0.234 aveacc=91.120
Test[400]:Result* Prec@1 85.700	Loss 0.417
Iteration [401]: lr=1.000e-03
Iteration [401] train aveloss=0.245 aveacc=90.920
Test[401]:Result* Prec@1 85.400	Loss 0.391
Iteration [402]: lr=1.000e-03
Iteration [402] train aveloss=0.228 aveacc=91.480
Test[402]:Result* Prec@1 85.200	Loss 0.409
Iteration [403]: lr=1.000e-03
Iteration [403] train aveloss=0.228 aveacc=91.100
Test[403]:Result* Prec@1 45.900	Loss 10.156
Iteration [404]: lr=1.000e-03
Iteration [404] train aveloss=0.241 aveacc=90.550
Test[404]:Result* Prec@1 85.000	Loss 0.383
Iteration [405]: lr=1.000e-03
Iteration [405] train aveloss=0.232 aveacc=91.150
Test[405]:Result* Prec@1 71.500	Loss 1.012
Iteration [406]: lr=1.000e-03
Iteration [406] train aveloss=0.233 aveacc=91.470
Test[406]:Result* Prec@1 86.600	Loss 0.332
Iteration [407]: lr=1.000e-03
Iteration [407] train aveloss=0.228 aveacc=91.140
Test[407]:Result* Prec@1 79.200	Loss 0.763
Iteration [408]: lr=1.000e-03
Iteration [408] train aveloss=0.230 aveacc=91.440
Test[408]:Result* Prec@1 86.300	Loss 0.436
Iteration [409]: lr=1.000e-03
Iteration [409] train aveloss=0.222 aveacc=91.620
Test[409]:Result* Prec@1 85.100	Loss 0.396
Iteration [410]: lr=1.000e-03
Iteration [410] train aveloss=0.237 aveacc=91.270
Test[410]:Result* Prec@1 86.300	Loss 0.379
Iteration [411]: lr=1.000e-03
Iteration [411] train aveloss=0.229 aveacc=91.200
Test[411]:Result* Prec@1 86.900	Loss 0.384
Iteration [412]: lr=1.000e-03
Iteration [412] train aveloss=0.234 aveacc=91.290
Test[412]:Result* Prec@1 78.300	Loss 0.989
Iteration [413]: lr=1.000e-03
Iteration [413] train aveloss=0.224 aveacc=91.780
Test[413]:Result* Prec@1 55.900	Loss 4.943
Iteration [414]: lr=1.000e-03
Iteration [414] train aveloss=0.232 aveacc=91.320
Test[414]:Result* Prec@1 83.800	Loss 0.425
Iteration [415]: lr=1.000e-03
Iteration [415] train aveloss=0.223 aveacc=91.750
Test[415]:Result* Prec@1 55.200	Loss 4.850
Iteration [416]: lr=1.000e-03
Iteration [416] train aveloss=0.222 aveacc=91.490
Test[416]:Result* Prec@1 79.700	Loss 0.532
Iteration [417]: lr=1.000e-03
Iteration [417] train aveloss=0.232 aveacc=91.300
Test[417]:Result* Prec@1 81.700	Loss 0.457
Iteration [418]: lr=1.000e-03
Iteration [418] train aveloss=0.216 aveacc=92.120
Test[418]:Result* Prec@1 39.100	Loss 13.251
Iteration [419]: lr=1.000e-03
Iteration [419] train aveloss=0.220 aveacc=91.890
Test[419]:Result* Prec@1 84.400	Loss 0.448
Iteration [420]: lr=1.000e-03
Iteration [420] train aveloss=0.234 aveacc=91.410
Test[420]:Result* Prec@1 57.600	Loss 4.367
Iteration [421]: lr=1.000e-03
Iteration [421] train aveloss=0.229 aveacc=91.030
Test[421]:Result* Prec@1 87.000	Loss 0.374
Iteration [422]: lr=1.000e-03
Iteration [422] train aveloss=0.230 aveacc=91.540
Test[422]:Result* Prec@1 80.700	Loss 0.545
Iteration [423]: lr=1.000e-03
Iteration [423] train aveloss=0.224 aveacc=91.530
Test[423]:Result* Prec@1 71.100	Loss 1.655
Iteration [424]: lr=1.000e-03
Iteration [424] train aveloss=0.221 aveacc=91.740
Test[424]:Result* Prec@1 81.400	Loss 0.518
Iteration [425]: lr=1.000e-03
Iteration [425] train aveloss=0.236 aveacc=91.120
Test[425]:Result* Prec@1 85.500	Loss 0.370
Iteration [426]: lr=1.000e-03
Iteration [426] train aveloss=0.218 aveacc=91.760
Test[426]:Result* Prec@1 85.200	Loss 0.402
Iteration [427]: lr=1.000e-03
Iteration [427] train aveloss=0.209 aveacc=92.250
Test[427]:Result* Prec@1 41.000	Loss 12.466
Iteration [428]: lr=1.000e-03
Iteration [428] train aveloss=0.222 aveacc=91.660
Test[428]:Result* Prec@1 85.100	Loss 0.412
Iteration [429]: lr=1.000e-03
Iteration [429] train aveloss=0.229 aveacc=91.350
Test[429]:Result* Prec@1 87.000	Loss 0.393
Iteration [430]: lr=1.000e-03
Iteration [430] train aveloss=0.208 aveacc=92.540
Test[430]:Result* Prec@1 49.600	Loss 5.950
Iteration [431]: lr=1.000e-03
Iteration [431] train aveloss=0.224 aveacc=91.750
Test[431]:Result* Prec@1 82.600	Loss 0.431
Iteration [432]: lr=1.000e-03
Iteration [432] train aveloss=0.219 aveacc=91.810
Test[432]:Result* Prec@1 83.600	Loss 0.510
Iteration [433]: lr=1.000e-03
Iteration [433] train aveloss=0.234 aveacc=91.210
Test[433]:Result* Prec@1 82.700	Loss 0.545
Iteration [434]: lr=1.000e-03
Iteration [434] train aveloss=0.207 aveacc=92.480
Test[434]:Result* Prec@1 86.100	Loss 0.393
Iteration [435]: lr=1.000e-03
Iteration [435] train aveloss=0.227 aveacc=91.750
Test[435]:Result* Prec@1 85.200	Loss 0.396
Iteration [436]: lr=1.000e-03
Iteration [436] train aveloss=0.219 aveacc=91.450
Test[436]:Result* Prec@1 85.900	Loss 0.374
Iteration [437]: lr=1.000e-03
Iteration [437] train aveloss=0.214 aveacc=91.760
Test[437]:Result* Prec@1 43.100	Loss 10.707
Iteration [438]: lr=1.000e-03
Iteration [438] train aveloss=0.215 aveacc=92.060
Test[438]:Result* Prec@1 65.200	Loss 2.112
Iteration [439]: lr=1.000e-03
Iteration [439] train aveloss=0.214 aveacc=91.820
Test[439]:Result* Prec@1 84.500	Loss 0.434
Iteration [440]: lr=1.000e-03
Iteration [440] train aveloss=0.215 aveacc=91.900
Test[440]:Result* Prec@1 83.100	Loss 0.484
Iteration [441]: lr=1.000e-03
Iteration [441] train aveloss=0.212 aveacc=92.190
Test[441]:Result* Prec@1 82.900	Loss 0.476
Iteration [442]: lr=1.000e-03
Iteration [442] train aveloss=0.205 aveacc=92.450
Test[442]:Result* Prec@1 86.900	Loss 0.373
Iteration [443]: lr=1.000e-03
Iteration [443] train aveloss=0.206 aveacc=92.620
Test[443]:Result* Prec@1 80.000	Loss 0.588
Iteration [444]: lr=1.000e-03
Iteration [444] train aveloss=0.209 aveacc=91.950
Test[444]:Result* Prec@1 84.500	Loss 0.444
Iteration [445]: lr=1.000e-03
Iteration [445] train aveloss=0.216 aveacc=92.030
Test[445]:Result* Prec@1 76.400	Loss 1.028
Iteration [446]: lr=1.000e-03
Iteration [446] train aveloss=0.207 aveacc=92.520
Test[446]:Result* Prec@1 47.700	Loss 10.043
Iteration [447]: lr=1.000e-03
Iteration [447] train aveloss=0.209 aveacc=92.460
Test[447]:Result* Prec@1 46.900	Loss 9.025
Iteration [448]: lr=1.000e-03
Iteration [448] train aveloss=0.217 aveacc=91.730
Test[448]:Result* Prec@1 82.000	Loss 0.468
Iteration [449]: lr=1.000e-03
Iteration [449] train aveloss=0.209 aveacc=92.410
Test[449]:Result* Prec@1 85.700	Loss 0.376
Iteration [450]: lr=1.000e-03
Iteration [450] train aveloss=0.209 aveacc=92.170
Test[450]:Result* Prec@1 83.300	Loss 0.431
Iteration [451]: lr=1.000e-03
Iteration [451] train aveloss=0.220 aveacc=91.720
Test[451]:Result* Prec@1 85.200	Loss 0.448
Iteration [452]: lr=1.000e-03
Iteration [452] train aveloss=0.197 aveacc=92.630
Test[452]:Result* Prec@1 83.500	Loss 0.458
Iteration [453]: lr=1.000e-03
Iteration [453] train aveloss=0.193 aveacc=92.840
Test[453]:Result* Prec@1 70.300	Loss 1.682
Iteration [454]: lr=1.000e-03
Iteration [454] train aveloss=0.214 aveacc=91.930
Test[454]:Result* Prec@1 45.700	Loss 9.104
Iteration [455]: lr=1.000e-03
Iteration [455] train aveloss=0.204 aveacc=92.560
Test[455]:Result* Prec@1 87.200	Loss 0.379
Iteration [456]: lr=1.000e-03
Iteration [456] train aveloss=0.214 aveacc=92.200
Test[456]:Result* Prec@1 72.100	Loss 1.003
Iteration [457]: lr=1.000e-03
Iteration [457] train aveloss=0.201 aveacc=92.690
Test[457]:Result* Prec@1 69.600	Loss 1.114
Iteration [458]: lr=1.000e-03
Iteration [458] train aveloss=0.197 aveacc=92.790
Test[458]:Result* Prec@1 87.000	Loss 0.354
Iteration [459]: lr=1.000e-03
Iteration [459] train aveloss=0.205 aveacc=92.490
Test[459]:Result* Prec@1 81.300	Loss 0.487
Iteration [460]: lr=1.000e-03
Iteration [460] train aveloss=0.205 aveacc=92.200
Test[460]:Result* Prec@1 85.400	Loss 0.409
Iteration [461]: lr=1.000e-03
Iteration [461] train aveloss=0.200 aveacc=92.710
Test[461]:Result* Prec@1 79.800	Loss 0.505
Iteration [462]: lr=1.000e-03
Iteration [462] train aveloss=0.213 aveacc=92.260
Test[462]:Result* Prec@1 52.400	Loss 5.328
Iteration [463]: lr=1.000e-03
Iteration [463] train aveloss=0.213 aveacc=92.030
Test[463]:Result* Prec@1 84.800	Loss 0.421
Iteration [464]: lr=1.000e-03
Iteration [464] train aveloss=0.213 aveacc=91.970
Test[464]:Result* Prec@1 84.500	Loss 0.429
Iteration [465]: lr=1.000e-03
Iteration [465] train aveloss=0.208 aveacc=92.220
Test[465]:Result* Prec@1 85.800	Loss 0.381
Iteration [466]: lr=1.000e-03
Iteration [466] train aveloss=0.198 aveacc=92.640
Test[466]:Result* Prec@1 70.900	Loss 1.514
Iteration [467]: lr=1.000e-03
Iteration [467] train aveloss=0.198 aveacc=92.640
Test[467]:Result* Prec@1 44.200	Loss 12.786
Iteration [468]: lr=1.000e-03
Iteration [468] train aveloss=0.195 aveacc=92.660
Test[468]:Result* Prec@1 84.500	Loss 0.436
Iteration [469]: lr=1.000e-03
Iteration [469] train aveloss=0.201 aveacc=92.700
Test[469]:Result* Prec@1 83.900	Loss 0.471
Iteration [470]: lr=1.000e-03
Iteration [470] train aveloss=0.199 aveacc=92.430
Test[470]:Result* Prec@1 83.400	Loss 0.432
Iteration [471]: lr=1.000e-03
Iteration [471] train aveloss=0.209 aveacc=92.250
Test[471]:Result* Prec@1 85.400	Loss 0.405
Iteration [472]: lr=1.000e-03
Iteration [472] train aveloss=0.205 aveacc=92.520
Test[472]:Result* Prec@1 83.900	Loss 0.419
Iteration [473]: lr=1.000e-03
Iteration [473] train aveloss=0.206 aveacc=92.410
Test[473]:Result* Prec@1 87.500	Loss 0.336
Iteration [474]: lr=1.000e-03
Iteration [474] train aveloss=0.197 aveacc=92.720
Test[474]:Result* Prec@1 79.000	Loss 0.543
Iteration [475]: lr=1.000e-03
Iteration [475] train aveloss=0.196 aveacc=92.730
Test[475]:Result* Prec@1 83.600	Loss 0.453
Iteration [476]: lr=1.000e-03
Iteration [476] train aveloss=0.190 aveacc=93.030
Test[476]:Result* Prec@1 78.200	Loss 0.770
Iteration [477]: lr=1.000e-03
Iteration [477] train aveloss=0.198 aveacc=92.640
Test[477]:Result* Prec@1 82.900	Loss 0.561
Iteration [478]: lr=1.000e-03
Iteration [478] train aveloss=0.200 aveacc=92.470
Test[478]:Result* Prec@1 83.000	Loss 0.441
Iteration [479]: lr=1.000e-03
Iteration [479] train aveloss=0.198 aveacc=92.960
Test[479]:Result* Prec@1 85.800	Loss 0.409
Iteration [480]: lr=1.000e-03
Iteration [480] train aveloss=0.194 aveacc=92.870
Test[480]:Result* Prec@1 85.900	Loss 0.427
Iteration [481]: lr=1.000e-03
Iteration [481] train aveloss=0.191 aveacc=92.820
Test[481]:Result* Prec@1 43.800	Loss 8.062
Iteration [482]: lr=1.000e-03
Iteration [482] train aveloss=0.195 aveacc=92.520
Test[482]:Result* Prec@1 78.300	Loss 0.796
Iteration [483]: lr=1.000e-03
Iteration [483] train aveloss=0.200 aveacc=92.540
Test[483]:Result* Prec@1 85.300	Loss 0.443
Iteration [484]: lr=1.000e-03
Iteration [484] train aveloss=0.195 aveacc=92.730
Test[484]:Result* Prec@1 61.700	Loss 2.328
Iteration [485]: lr=1.000e-03
Iteration [485] train aveloss=0.192 aveacc=92.920
Test[485]:Result* Prec@1 86.700	Loss 0.373
Iteration [486]: lr=1.000e-03
Iteration [486] train aveloss=0.191 aveacc=92.870
Test[486]:Result* Prec@1 52.300	Loss 6.176
Iteration [487]: lr=1.000e-03
Iteration [487] train aveloss=0.190 aveacc=93.200
Test[487]:Result* Prec@1 73.800	Loss 1.325
Iteration [488]: lr=1.000e-03
Iteration [488] train aveloss=0.191 aveacc=93.010
Test[488]:Result* Prec@1 83.400	Loss 0.487
Iteration [489]: lr=1.000e-03
Iteration [489] train aveloss=0.190 aveacc=93.090
Test[489]:Result* Prec@1 82.500	Loss 0.449
Iteration [490]: lr=1.000e-03
Iteration [490] train aveloss=0.186 aveacc=93.040
Test[490]:Result* Prec@1 59.600	Loss 3.171
Iteration [491]: lr=1.000e-03
Iteration [491] train aveloss=0.195 aveacc=93.180
Test[491]:Result* Prec@1 84.500	Loss 0.458
Iteration [492]: lr=1.000e-03
Iteration [492] train aveloss=0.185 aveacc=93.220
Test[492]:Result* Prec@1 78.800	Loss 0.722
Iteration [493]: lr=1.000e-03
Iteration [493] train aveloss=0.188 aveacc=93.010
Test[493]:Result* Prec@1 85.900	Loss 0.410
Iteration [494]: lr=1.000e-03
Iteration [494] train aveloss=0.200 aveacc=92.750
Test[494]:Result* Prec@1 82.200	Loss 0.599
Iteration [495]: lr=1.000e-03
Iteration [495] train aveloss=0.183 aveacc=93.160
Test[495]:Result* Prec@1 79.800	Loss 0.614
Iteration [496]: lr=1.000e-03
Iteration [496] train aveloss=0.186 aveacc=93.320
Test[496]:Result* Prec@1 81.000	Loss 0.532
Iteration [497]: lr=1.000e-03
Iteration [497] train aveloss=0.182 aveacc=93.360
Test[497]:Result* Prec@1 73.600	Loss 0.868
Iteration [498]: lr=1.000e-03
Iteration [498] train aveloss=0.193 aveacc=92.830
Test[498]:Result* Prec@1 85.300	Loss 0.429
Iteration [499]: lr=1.000e-03
Iteration [499] train aveloss=0.186 aveacc=93.140
Test[499]:Result* Prec@1 83.200	Loss 0.550
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [17]:</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># once the training is over. stop the fillers</span>
<span class="n">iotrain</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
<span class="n">iovalid</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Observations-from-training">Observations from training<a class="anchor-link" href="#Observations-from-training">¶</a></h2><p>For the first 30 epochs (150 iterations), the training is going well. The average loss for the training data (blue) and validation data (red) are dropping steadily and doing so together.</p>
<p>After epoch 30, the training loss keeps lowering.  However, the training and validation losses are separating. The validation loss stops improving and becomes very variable.    These are both hallmarks of overtraining.</p>
<p>Looking at the standard output, the accuracy of the validation gets stuck at about 85%.  This is expected with our training data. The 15% of events involves images where the labels are inaccurate. For example, a proton interacts with a nucleus producing a bunch of photons. Or a muon decays early into an electron.  Refer to the blog post about version 0.1.0 of the open training data. This means we probably hit the accuracy limit.</p>
<p>This is why we saved a checkpoint every 50 iterations.  You'll find <code>checkpoint.Xth.tar</code> files in the folder where this notebook is located. We can use the model saved at epoch 30 (i.e. <code>checkpoint.150th.tar</code>).  In a subsequent post, we'll look at the performance of that model.</p>
</div>
</div>
</div>

<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

  </div>

  <div class="tag-cloud">
    <p>
      <a href="http://deeplearnphysics.org/Blog/tag/resnet.html">resnet</a>
      <a href="http://deeplearnphysics.org/Blog/tag/pytorch.html">pytorch</a>
      <a href="http://deeplearnphysics.org/Blog/tag/classification.html">classification</a>
      <a href="http://deeplearnphysics.org/Blog/tag/example.html">example</a>
    </p>
  </div>



</article>

    <footer>
<p>
  &copy; DeepLearnPhysics 2017 - This work is licensed under a <a rel="license" href="https://opensource.org/licenses/MIT">MIT License</a>
</p>
<p>
</p>    </footer>
  </main>





<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " DeepLearnPhysics Blog ",
  "url" : "http://deeplearnphysics.org/Blog",
  "image": "profile.png",
  "description": "description!"
}
</script>
</body>
</html>