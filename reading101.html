
<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="http://deeplearnphysics.org/Blog/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="http://deeplearnphysics.org/Blog/theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="http://deeplearnphysics.org/Blog/theme/font-awesome/css/font-awesome.min.css">




  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="index, follow" />

    <!-- Chrome, Firefox OS and Opera -->
    <meta name="theme-color" content="#333">
    <!-- Windows Phone -->
    <meta name="msapplication-navbutton-color" content="#333">
    <!-- iOS Safari -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

<meta name="author" content="Kazuhiro Terao" />
<meta name="description" content="Useful readings for (new) group members." />
<meta name="keywords" content="paper">
<meta property="og:site_name" content="DeepLearnPhysics Blog"/>
<meta property="og:title" content="Getting started: paper readings"/>
<meta property="og:description" content="Useful readings for (new) group members."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="http://deeplearnphysics.org/Blog/reading101.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2017-12-19 00:00:00-06:00"/>
<meta property="article:modified_time" content="2017-12-19 00:00:00-06:00"/>
<meta property="article:author" content="http://deeplearnphysics.org/Blog/author/kazuhiro-terao.html">
<meta property="article:section" content="tutorial"/>
<meta property="article:tag" content="paper"/>
<meta property="og:image" content="profile.png">


<!-- Default meta cards for twitter -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@dlphysics">
<meta name="twitter:creator" content="@dlphysics">
<meta name="twitter:title" content="Getting started: paper readings">
<meta name="twitter:description" content="<p>Useful readings for (new) group members.</p>">
<meta name="twitter:image" content="http://deeplearnphysics.org/Blog/theme/img/profile.png" />


  <title>DeepLearnPhysics Blog &ndash; Getting started: paper readings</title>
</head>
<body>
  <aside>
    <div>
      <a href="http://deeplearnphysics.org/Blog">
        <img src="http://deeplearnphysics.org/Blog/theme/img/profile.png" alt="Blog" title="Blog">
      </a>
      <h1><a href="http://deeplearnphysics.org/Blog">Blog</a></h1>

<p>DeepLearnPhysics Group</p>

      <ul class="social">
        <li><a class="sc-home" href="http://deeplearnphysics.org" target="_blank"><i class="fa fa-home"></i></a></li>
        <li><a class="sc-twitter" href="https://twitter.com/dlphysics" target="_blank"><i class="fa fa-twitter"></i></a></li>
        <li><a class="sc-github" href="http://github.com/DeepLearnPhysics" target="_blank"><i class="fa fa-github"></i></a></li>
      </ul>
    </div>

  </aside>
  <main>
    <nav>


      <a href="http://deeplearnphysics.org/Blog/index.html">Home</a>
      <a href="http://deeplearnphysics.org/Blog/categories.html">Category</a>
      <a href="http://deeplearnphysics.org/Blog/archives.html">Archives</a>
      <a href="http://deeplearnphysics.org/Blog/tags.html">Tags</a>
      <a href="http://deeplearnphysics.org/Blog/authors.html">Authors</a>


    </nav>

<article class="single">
  <header>
    <h1 id="reading101">Getting started: paper readings</h1>
    <p>
          Posted on Tue 19 December 2017 in <a href="http://deeplearnphysics.org/Blog/category/tutorial.html">tutorial</a>

            by

              <a href="http://deeplearnphysics.org/Blog/author/kazuhiro-terao.html">Kazuhiro Terao</a>    </p>
  </header>

  <!-- script is a local library -->
  <link rel="stylesheet" type="text/css" href="http://deeplearnphysics.org/Blog/theme/stylesheet/kazunotebook.css">

  <div>
    <p>This is a message I compiled once for myself but also share with my students etc..
You can easily find a similar compilation of papers on people's github: just google it :) 
But here's just one of those for our group's reference.</p>
<p>I list them in an order of history, hoping this allows you to skip some toward the beginning.
I put “<strong>recommended</strong>” next to the paper i think it’s good/important to read.</p>
<h2><strong>Modern CNN</strong></h2>
<ul>
<li>
<p>2012 <a href="https://arxiv.org/pdf/1207.0580.pdf">Drop-out</a> (<strong>recommended</strong>)</p>
<ul>
<li>A big jump in training technique to avoid over-fitting and improve final accuracy, key technique for AlexNet</li>
</ul>
</li>
<li>
<p>2012 <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a> (<strong>recommended</strong>)</p>
<ul>
<li>Legendary debut of CNN, first implementation on GPU by Hinton (prof. U. of Toronto), Alex (now Google), and Ilya (now OpenAI), dramatic performance improvement on ILSVRC, annual competition of image recognition from the last year in both accuracy and speed of processing (previous year based on fisher vector machine). First time CNN was applied on 224x224x3 tensor image.</li>
</ul>
</li>
<li>
<p>2014 <a href="https://arxiv.org/abs/1409.1556">VGG</a></p>
<ul>
<li>First systematic approach to understand the effect of network depth using a homogeneous network architectures (all 3x3 kernel convolutions + 2x2 pooling layers)</li>
</ul>
</li>
<li>
<p>2014 <a href="https://arxiv.org/abs/1409.4842">GoogLeNet</a></p>
<ul>
<li>First “Inception” network idea</li>
</ul>
</li>
<li>
<p>2015 <a href="https://arxiv.org/abs/1502.03167">Batch Normalization</a> (<strong>recommended</strong>)</p>
<ul>
<li>Another big jump in training techniques to make dependency on initial weights smaller, making training dramatically easier (avoids over-fitting)</li>
</ul>
</li>
<li>
<p>2015 <a href="https://arxiv.org/abs/1512.03385">ResNet</a> (<strong>recommended</strong>)</p>
<ul>
<li>First “ResNet” idea, surpassed human average accuracy on ILSVRC data set. Technique allowed to train 1000 layer deep network, jaw-dropping for researchers who have been competing to make network deeper and deeper since GoogLeNet and VGG. This technique development was a huge deal, and ResNet is current default choice for constructing a deep network architecture today.</li>
</ul>
</li>
<li>
<p>2015 <a href="https://arxiv.org/abs/1506.01497">Faster-RCNN</a> (<strong>recommended</strong>)</p>
<ul>
<li>First real-time object detection by neural network (I think it was 20~30 Hz, which is &gt; 60Hz today with advanced version). Elegant technique to piggy-back detection network on top of any image recognition network. Region Proposal Network (RPN), part of Faster-RCNN development originally for an object detection (in this paper), is today cooked further and used by the best semantic segmentation network today.</li>
</ul>
</li>
<li>
<p>2015 <a href="https://arxiv.org/pdf/1511.06434.pdf">DC-GAN</a> (<strong>recommended</strong>)</p>
<ul>
<li>First generative-adversarial-network which looks like the machine has learned a concept of real world image. Super popular for the network accurately generating images of a bedroom and bathroom (toilet).</li>
</ul>
</li>
<li>
<p>2016 <a href="https://arxiv.org/abs/1605.06211">FCN</a> (<strong>recommended</strong>)</p>
<ul>
<li>First solid implementation of CNN for semantic segmentation. &gt; 1500 citations! Still used today as a standard candle of accuracy. Not the best accuracy in the field but extremely fast learning, simple architecture.</li>
</ul>
</li>
<li>
<p>2016 <a href="https://arxiv.org/abs/1605.06409">R-FCN</a></p>
<ul>
<li>Detection network improved by combining FCN with Faster-RCNN to improve the detection (first find pixels, then draw boudning box, makes sense!). First segmentation=&gt;detection=&gt;classification work flow.  … note “Kaming He” :)</li>
</ul>
</li>
<li>
<p>2016 <a href="https://arxiv.org/abs/1602.07261">Inception-V4, Inception-ResNet</a> (<strong>recommended</strong>)</p>
<ul>
<li>Hey let’s improve image classification even more… here’s huge network by Google, the latest Inception module, studied by combining with ResNet.</li>
</ul>
</li>
<li>
<p>2016 <a href="https://arxiv.org/abs/1605.07146">Wide-ResNet</a> (<strong>recommended</strong>)</p>
<ul>
<li>Empirical study to answer the question of What-is-the-“depth”-in-ResNet? The group found that ResNet actually performs better by making it “wider” rather than “deeper”.</li>
</ul>
</li>
<li>
<p>2016 <a href="https://arxiv.org/abs/1611.10080">Wider-or-Deeper ResNet?</a> (<strong>recommended</strong>)</p>
<ul>
<li>Analytical explanation and analysis of the observation made in the previous paper. Very well written. Demonstrated the importance of the width to image classification and semantic segmentation</li>
</ul>
</li>
<li>
<p>2016 <a href="https://arxiv.org/abs/1603.08678">Instance-sensitive Fully Convolutional Network (IS-FCN)</a> … (<strong>recommended</strong>)</p>
<ul>
<li>Extension of R-FCN, improved design architecture to win the ILSVRC semantic segmentation competition 2016 … note “Kaming He” :)</li>
</ul>
</li>
<li>
<p>2016 <a href="https://arxiv.org/abs/1611.05431">Aggregated Residual Transformations: ResNeXt</a> (<strong>recommended</strong>)</p>
<ul>
<li>Introduces a new dimension, named "cardinality" (the size of the set of transformations), claimed as yet another effective direction to improve the accuracy besides "width and depth".</li>
</ul>
</li>
<li>
<p>2017 <a href="https://arxiv.org/abs/1703.06870">Mask R-CNN</a> (<strong>recommended</strong>)</p>
<ul>
<li>Kaming He’s latest work that already beated IS-FCN, our current target to implement for instance-aware semantic segmentation for particle clustering.</li>
</ul>
</li>
<li>
<p>2017 <a href="https://arxiv.org/abs/1702.08591">Shattered Gradient Problem</a></p>
<ul>
<li>If resnets are the answer, what is the question?</li>
</ul>
</li>
<li>
<p>2017 <a href="https://arxiv.org/abs/1709.01507">Squeeze-and-Excitation Networks</a> (<strong>recommended</strong>)</p>
<ul>
<li>First detailed study for enhancing the "channels" of the tensor to encode image features instead of spatial dimensions (width/height) of images.</li>
</ul>
</li>
</ul>
  </div>

  <div class="tag-cloud">
    <p>
      <a href="http://deeplearnphysics.org/Blog/tag/paper.html">paper</a>
    </p>
  </div>



</article>

    <footer>
<p>
  &copy; DeepLearnPhysics 2017 - This work is licensed under a <a rel="license" href="https://opensource.org/licenses/MIT">MIT License</a>
</p>
<p>
</p>    </footer>
  </main>





<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " DeepLearnPhysics Blog ",
  "url" : "http://deeplearnphysics.org/Blog",
  "image": "profile.png",
  "description": "description!"
}
</script>
</body>
</html>